{"cells":[{"cell_type":"markdown","id":"bb427423-0eab-4440-b27a-05a5ee8d9e9b","metadata":{"id":"bb427423-0eab-4440-b27a-05a5ee8d9e9b"},"source":["# To investigate model-stitching analysis on VGG19\n","Stitch randomly initialised model into VGG19 unbias receiver - based on exp2j which self-stitched models. In this case, use the bias (correlated) dataset to see how that affects the ability to stitch in the random model"]},{"cell_type":"code","source":["from google.colab import files\n","from google.colab import drive\n","\n","drive.mount('/content/drive')\n","!pip install torchbearer\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NWaxtglmOqyp","executionInfo":{"status":"ok","timestamp":1747732347337,"user_tz":-60,"elapsed":98824,"user":{"displayName":"Damian Smith","userId":"13736799635312870126"}},"outputId":"316ae1e4-e2df-4f7e-e321-15cbc0a7dbbe"},"id":"NWaxtglmOqyp","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Collecting torchbearer\n","  Downloading torchbearer-0.5.5-py3-none-any.whl.metadata (29 kB)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from torchbearer) (2.6.0+cu124)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchbearer) (2.0.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torchbearer) (4.67.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchbearer) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchbearer) (4.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchbearer) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchbearer) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchbearer) (2025.3.2)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.0.0->torchbearer)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.0.0->torchbearer)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.0.0->torchbearer)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.0.0->torchbearer)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.0.0->torchbearer)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.0.0->torchbearer)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.0.0->torchbearer)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.0.0->torchbearer)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.0.0->torchbearer)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchbearer) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchbearer) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchbearer) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.0.0->torchbearer)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchbearer) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchbearer) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->torchbearer) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->torchbearer) (3.0.2)\n","Downloading torchbearer-0.5.5-py3-none-any.whl (139 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.4/139.4 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m122.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m95.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m112.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchbearer\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torchbearer-0.5.5\n"]}]},{"cell_type":"code","execution_count":2,"id":"51fc60f8-f6f6-469c-8705-c9015bd43951","metadata":{"id":"51fc60f8-f6f6-469c-8705-c9015bd43951","executionInfo":{"status":"ok","timestamp":1747732366798,"user_tz":-60,"elapsed":19457,"user":{"displayName":"Damian Smith","userId":"13736799635312870126"}}},"outputs":[],"source":["# Packages\n","%matplotlib inline\n","\n","drive_path = '/content/drive/MyDrive/Colab Notebooks/icml'\n","import os\n","os.chdir(drive_path)\n","\n","import argparse\n","import gc\n","import os.path\n","\n","import pandas as pd\n","from torch.linalg import LinAlgError\n","\n","import matplotlib.pyplot as plt\n","import torchvision\n","import torch\n","from torch import optim\n","\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from torchvision.datasets import MNIST\n","import torchvision.transforms as transforms\n","import datetime\n","\n","import random\n","import numpy as np\n","\n","import sys\n","import os\n","# add the path to find colour_mnist\n","#sys.path.append(os.path.abspath('../ReferenceCode'))\n","import colour_mnist\n","from stitch_utils import train_model, StitchedVGG19, get_layer_output_shape\n","from stitch_utils import generate_activations, SyntheticDataset\n","import stitch_utils\n","\n","# add the path to find the rank analysis code\n","# https://github.com/DHLSmith/jons-tunnel-effect/tree/NeurIPSPaper\n","\n","#sys.path.append(os.path.abspath('../../jons-tunnel-effect/'))\n","from utils.modelfitting import evaluate_model, set_seed\n","from extract_weight_rank import install_hooks, perform_analysis\n","\n","import torchvision\n","from torchvision.datasets import MNIST\n","\n","def logtofile(log_text, verbose=True):\n","    if verbose:\n","        print(log_text)\n","    with open(save_log_as, \"a\") as f:\n","        print(log_text, file=f)"]},{"cell_type":"code","execution_count":3,"id":"6761870b-2996-4763-8d90-76529ec5822e","metadata":{"id":"6761870b-2996-4763-8d90-76529ec5822e","executionInfo":{"status":"ok","timestamp":1747732366829,"user_tz":-60,"elapsed":27,"user":{"displayName":"Damian Smith","userId":"13736799635312870126"}}},"outputs":[],"source":["# Set Parameters\n","\n","# fix random seed for reproducibility\n","seed = 107\n","torch.manual_seed(seed)\n","torch.backends.cudnn.benchmark = False\n","torch.backends.cudnn.deterministic = True\n","random.seed(seed)\n","torch.cuda.manual_seed(seed)\n","np.random.seed(seed)\n","\n","train_all = False\n","save_stitch_delta = False\n","measure_rank = False\n","\n","results_root = \"results_2j_b\"\n","\n","# randinit model\n","gen_randinit_model = False\n","randinit_model_to_load = '2025-03-26_18-57-22_SEED107_EPOCHS50_BGN0.1_exp2h_VGG19_randinit.weights'\n","\n","# MIX is 1/3 bgonly, 1/3 mnist only, 1/3 biased data\n","train_mix_mnist_model = train_all\n","mix_mnist_model_to_load = '../exp1_ms_with_random_dataset/results_1g/2025-01-07_10-01-18_SEED107_EPOCHS50_BGN0.1_exp1g_VGG19_mix_mnist.weights'\n","\n","# BW is greyscale mnist with no colour added (i.e. original mnist)\n","train_bw_mnist_model = train_all  # when False, automatically loads a trained model\n","bw_mnist_model_to_load = '../exp1_ms_with_random_dataset/results_1g/2025-01-07_10-01-18_SEED107_EPOCHS50_BGN0.1_exp1g_VGG19_bw_mnist.weights'\n","\n","# BG_ONLY contains no mnist data, just a coloured background\n","train_bg_only_colour_mnist_model = train_all # when False, automatically loads a trained model\n","bg_only_colour_mnist_model_to_load     =  '../exp1_ms_with_random_dataset/results_1g/2025-01-07_10-01-18_SEED107_EPOCHS50_BGN0.1_exp1g_VGG19_bg_only_colour_mnist.weights'\n","#bg_only_std_colour_mnist_model_to_load =  '../exp1_ms_with_random_dataset/results_1g/2024-08-10_22-57-13_SEED104_EPOCHS50_BGN0.1_exp1g_VGG19_bg_only_colour_mnist.weights'\n","\n","# BG_UNBIASED is digits with randomly selected colour background. Targets represent the colour\n","train_bg_unbiased_colour_mnist_model = train_all  # when False, automatically loads a trained model\n","bg_unbiased_colour_mnist_model_to_load     = '../exp1_ms_with_random_dataset/results_1g/2025-01-07_10-01-18_SEED107_EPOCHS50_BGN0.1_exp1g_VGG19_bg_unbiased_colour_mnist.weights'\n","#bg_unbiased_std_colour_mnist_model_to_load = '../exp1_ms_with_random_dataset/results_1g/2024-08-11_11-32-44_SEED104_EPOCHS50_BGN0.1_exp1g_VGG19_bg_unbiased_colour_mnist.weights'\n","\n","# BIASED is digits with consistent per-class colour background.\n","train_biased_colour_mnist_model = train_all  # when False, automatically loads a trained model\n","biased_colour_mnist_model_to_load = '../exp1_ms_with_random_dataset/results_1g/2025-01-07_10-01-18_SEED107_EPOCHS50_BGN0.1_exp1g_VGG19_biased_colour_mnist.weights'\n","\n","# UNBIASED is digits with randoly selected colour background. Targets are digit values\n","train_unbiased_colour_mnist_model = train_all  # when False, automatically loads a trained model\n","unbiased_colour_mnist_model_to_load = '2025-01-07_10-01-18_SEED107_EPOCHS50_BGN0.1_exp1g_VGG19_unbiased_colour_mnist.weights'\n","\n","original_train_epochs = 50\n","bg_noise = 0.1\n","synthetic_dataset_noise = 0.1\n","\n","stitch_train_epochs = 50\n","#stitch_train_loss_tol = 40\n","\n","device = 'cuda'\n"]},{"cell_type":"code","execution_count":4,"id":"20ee6e98-a6f2-4647-b378-5f7b1af48581","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"20ee6e98-a6f2-4647-b378-5f7b1af48581","executionInfo":{"status":"ok","timestamp":1747732366859,"user_tz":-60,"elapsed":27,"user":{"displayName":"Damian Smith","userId":"13736799635312870126"}},"outputId":"2ef2077a-3e8b-46ed-ca0c-f4a3e2139cc1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Executed at 2025-05-20_09-12-45\n","logging to ./results_2j_b/2025-05-20_09-12-45_SEED107_EPOCHS50_BGN0.1_exp2h_VGG19_log.txt\n","seed=107\n","bg_noise=0.1\n","synthetic_dataset_noise=0.1\n","gen_randinit_model=False\n","randinit_model_to_load='2025-03-26_18-57-22_SEED107_EPOCHS50_BGN0.1_exp2h_VGG19_randinit.weights'\n","train_mix_mnist_model=False\n","mix_mnist_model_to_load='../exp1_ms_with_random_dataset/results_1g/2025-01-07_10-01-18_SEED107_EPOCHS50_BGN0.1_exp1g_VGG19_mix_mnist.weights'\n","train_bw_mnist_model=False\n","bw_mnist_model_to_load='../exp1_ms_with_random_dataset/results_1g/2025-01-07_10-01-18_SEED107_EPOCHS50_BGN0.1_exp1g_VGG19_bw_mnist.weights'\n","train_bg_only_colour_mnist_model=False\n","bg_only_colour_mnist_model_to_load='../exp1_ms_with_random_dataset/results_1g/2025-01-07_10-01-18_SEED107_EPOCHS50_BGN0.1_exp1g_VGG19_bg_only_colour_mnist.weights'\n","train_bg_unbiased_colour_mnist_model=False\n","bg_unbiased_colour_mnist_model_to_load='../exp1_ms_with_random_dataset/results_1g/2025-01-07_10-01-18_SEED107_EPOCHS50_BGN0.1_exp1g_VGG19_bg_unbiased_colour_mnist.weights'\n","train_biased_colour_mnist_model=False\n","biased_colour_mnist_model_to_load='../exp1_ms_with_random_dataset/results_1g/2025-01-07_10-01-18_SEED107_EPOCHS50_BGN0.1_exp1g_VGG19_biased_colour_mnist.weights'\n","train_unbiased_colour_mnist_model=False\n","unbiased_colour_mnist_model_to_load='2025-01-07_10-01-18_SEED107_EPOCHS50_BGN0.1_exp1g_VGG19_unbiased_colour_mnist.weights'\n","stitch_train_epochs=50\n","================================================\n"]}],"source":["# Generate filenames and log the setup details\n","formatted_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n","\n","filename_prefix = f\"./{results_root}/{formatted_time}_SEED{seed}_EPOCHS{original_train_epochs}_BGN{bg_noise}_exp2h_VGG19\"\n","save_mix_mnist_model_as = f\"{filename_prefix}_mix_mnist.weights\"\n","save_bw_mnist_model_as = f\"{filename_prefix}_bw_mnist.weights\"\n","save_bg_only_colour_mnist_model_as = f\"{filename_prefix}_bg_only_colour_mnist.weights\"\n","save_bg_unbiased_colour_mnist_model_as = f\"{filename_prefix}_bg_unbiased_colour_mnist.weights\"\n","save_biased_colour_mnist_model_as = f\"{filename_prefix}_biased_colour_mnist.weights\"\n","save_unbiased_colour_mnist_model_as = f\"{filename_prefix}_unbiased_colour_mnist.weights\"\n","save_randinit_model_as = f\"{filename_prefix}_randinit.weights\"\n","\n","save_log_as = f\"{filename_prefix}_log.txt\"\n","\n","colour_mnist_shape = (3,28,28)\n","vgg19_input_shape = (3,32,32) # monochrome\n","\n","logtofile(f\"Executed at {formatted_time}\")\n","logtofile(f\"logging to {save_log_as}\")\n","logtofile(f\"{seed=}\")\n","logtofile(f\"{bg_noise=}\")\n","logtofile(f\"{synthetic_dataset_noise=}\")\n","\n","logtofile(f\"{gen_randinit_model=}\")\n","if gen_randinit_model:\n","    logtofile(f\"{save_randinit_model_as=}\")\n","else:\n","    logtofile(f\"{randinit_model_to_load=}\")\n","\n","logtofile(f\"{train_mix_mnist_model=}\")\n","if train_mix_mnist_model:\n","    logtofile(f\"{save_mix_mnist_model_as=}\")\n","    logtofile(f\"{original_train_epochs=}\")\n","else:\n","    logtofile(f\"{mix_mnist_model_to_load=}\")\n","\n","logtofile(f\"{train_bw_mnist_model=}\")\n","if train_bw_mnist_model:\n","    logtofile(f\"{save_bw_mnist_model_as=}\")\n","    logtofile(f\"{original_train_epochs=}\")\n","else:\n","    logtofile(f\"{bw_mnist_model_to_load=}\")\n","\n","logtofile(f\"{train_bg_only_colour_mnist_model=}\")\n","if train_bg_only_colour_mnist_model:\n","    logtofile(f\"{save_bg_only_colour_mnist_model_as=}\")\n","    logtofile(f\"{original_train_epochs=}\")\n","else:\n","    logtofile(f\"{bg_only_colour_mnist_model_to_load=}\")\n","\n","logtofile(f\"{train_bg_unbiased_colour_mnist_model=}\")\n","if train_bg_unbiased_colour_mnist_model:\n","    logtofile(f\"{save_bg_unbiased_colour_mnist_model_as=}\")\n","    logtofile(f\"{original_train_epochs=}\")\n","else:\n","    logtofile(f\"{bg_unbiased_colour_mnist_model_to_load=}\")\n","\n","logtofile(f\"{train_biased_colour_mnist_model=}\")\n","if train_biased_colour_mnist_model:\n","    logtofile(f\"{save_biased_colour_mnist_model_as=}\")\n","    logtofile(f\"{original_train_epochs=}\")\n","else:\n","    logtofile(f\"{biased_colour_mnist_model_to_load=}\")\n","\n","logtofile(f\"{train_unbiased_colour_mnist_model=}\")\n","if train_unbiased_colour_mnist_model:\n","    logtofile(f\"{save_unbiased_colour_mnist_model_as=}\")\n","    logtofile(f\"{original_train_epochs=}\")\n","else:\n","    logtofile(f\"{unbiased_colour_mnist_model_to_load=}\")\n","\n","logtofile(f\"{stitch_train_epochs=}\")\n","logtofile(f\"================================================\")"]},{"cell_type":"markdown","id":"6cc621f6-55e8-4191-8288-dc2493cd6bff","metadata":{"id":"6cc621f6-55e8-4191-8288-dc2493cd6bff"},"source":["mnist and cifar-10 both use 10-classes, with 60_000 train samples and 10_000 test samples."]},{"cell_type":"code","execution_count":5,"id":"d34a54d2-c8fa-4f51-8809-7a40b4fefc6c","metadata":{"id":"d34a54d2-c8fa-4f51-8809-7a40b4fefc6c","executionInfo":{"status":"ok","timestamp":1747732379354,"user_tz":-60,"elapsed":12494,"user":{"displayName":"Damian Smith","userId":"13736799635312870126"}}},"outputs":[],"source":["# Set up dataloaders\n","transform_resize=transforms.Resize(vgg19_input_shape[2])\n","transform_bw = transforms.Compose([\n","    transforms.Grayscale(num_output_channels=3),  # Convert to 3 channels\n","    transform_resize,\n","    transforms.ToTensor(),  # convert to tensor. We always do this one\n","    transforms.Normalize((0.1307,) * 3, (0.3081,) * 3)\n","])\n","\n","\n","mnist_train = MNIST(\"./MNIST\", train=True, download=True, transform=transform_bw)\n","mnist_test = MNIST(\"./MNIST\", train=False, download=True, transform=transform_bw)\n","\n","bw_train_dataloader = DataLoader(mnist_train, batch_size=64, shuffle=True, drop_last=True)\n","bw_test_dataloader  = DataLoader(mnist_test,  batch_size=64, shuffle=True, drop_last=False)\n","\n","# mix dataloader\n","mix_train_dataloader = colour_mnist.get_mixed_mnist_dataloader(root=\"./MNIST\", batch_size=64, train=True, bg_noise_level=bg_noise, standard_getitem=True, dl_transform=transform_resize)\n","mix_test_dataloader = colour_mnist.get_mixed_mnist_dataloader(root=\"./MNIST\", batch_size=64,  train=False, bg_noise_level=bg_noise, standard_getitem=True, dl_transform=transform_resize)\n","\n","# bg_only means no digits - we will use colour as label\n","bg_only_train_dataloader = colour_mnist.get_biased_mnist_dataloader(root=\"./MNIST\", batch_size=64, data_label_correlation=1.0, train=True, bg_noise_level=bg_noise, bg_only=True, standard_getitem=True, dl_transform=transform_resize)\n","bg_only_test_dataloader = colour_mnist.get_biased_mnist_dataloader(root=\"./MNIST\", batch_size=64, data_label_correlation=1.0, train=False, bg_noise_level=bg_noise, bg_only=True, standard_getitem=True, dl_transform=transform_resize)\n","\n","# unbiased means each digit has correct label and random colour - but bg means we will use colour as label (i.e. the bias_target will be the target)\n","bg_unbiased_train_dataloader = colour_mnist.get_biased_mnist_dataloader(root=\"./MNIST\", batch_size=64, data_label_correlation=0.1, train=True, bg_noise_level=bg_noise, bias_targets_as_targets=True, dl_transform=transform_resize)\n","bg_unbiased_test_dataloader = colour_mnist.get_biased_mnist_dataloader(root=\"./MNIST\", batch_size=64, data_label_correlation=0.1, train=False, bg_noise_level=bg_noise, bias_targets_as_targets=True, dl_transform=transform_resize)\n","\n","# biased means each digit has correct label and consistent colour - Expect network to learn the colours only\n","biased_train_dataloader = colour_mnist.get_biased_mnist_dataloader(root=\"./MNIST\", batch_size=64, data_label_correlation=1.0, train=True, bg_noise_level=bg_noise, standard_getitem=True, dl_transform=transform_resize)\n","biased_test_dataloader = colour_mnist.get_biased_mnist_dataloader(root=\"./MNIST\", batch_size=64, data_label_correlation=1.0, train=False, bg_noise_level=bg_noise, standard_getitem=True, dl_transform=transform_resize)\n","\n","# unbiased means each digit has correct label and random colour - Expect network to disregard colours?\n","unbiased_train_dataloader = colour_mnist.get_biased_mnist_dataloader(root=\"./MNIST\", batch_size=64, data_label_correlation=0.1, train=True, bg_noise_level=bg_noise, standard_getitem=True, dl_transform=transform_resize)\n","unbiased_test_dataloader = colour_mnist.get_biased_mnist_dataloader(root=\"./MNIST\", batch_size=64, data_label_correlation=0.1, train=False, bg_noise_level=bg_noise, standard_getitem=True, dl_transform=transform_resize)"]},{"cell_type":"markdown","id":"24842d82-5f62-4d1b-acc5-d1997a08b0b9","metadata":{"id":"24842d82-5f62-4d1b-acc5-d1997a08b0b9"},"source":["## Set up VGG19 models and train it on versions of MNIST"]},{"cell_type":"code","execution_count":6,"id":"cf635cfd-a9ad-4e37-98a0-80d0db2a3b9f","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cf635cfd-a9ad-4e37-98a0-80d0db2a3b9f","executionInfo":{"status":"ok","timestamp":1747732415576,"user_tz":-60,"elapsed":36216,"user":{"displayName":"Damian Smith","userId":"13736799635312870126"}},"outputId":"36800135-218b-410c-dcbc-ec255f664155"},"outputs":[{"output_type":"stream","name":"stdout","text":["Processing for key='randinit'\n","val['loadfrom']='2025-03-26_18-57-22_SEED107_EPOCHS50_BGN0.1_exp2h_VGG19_randinit.weights'\n","Processing for key='unbias'\n","val['loadfrom']='2025-01-07_10-01-18_SEED107_EPOCHS50_BGN0.1_exp1g_VGG19_unbiased_colour_mnist.weights'\n"]}],"source":["\n","process_structure = dict()\n","\n","\n","process_structure[\"randinit\"]    = dict()\n","process_structure[\"unbias\"]    = dict()\n","\n","# \"randinit\"\n","process_structure[\"randinit\"][\"model\"] = torchvision.models.vgg19(weights=None, num_classes=10).to(device) # Untrained model\n","process_structure[\"randinit\"][\"train\"] = gen_randinit_model\n","process_structure[\"randinit\"][\"train_loader\"] = biased_train_dataloader\n","process_structure[\"randinit\"][\"test_loader\"] = biased_test_dataloader  # by default, test it against the unbias data set\n","process_structure[\"randinit\"][\"saveas\"] = save_randinit_model_as\n","process_structure[\"randinit\"][\"loadfrom\"] =  randinit_model_to_load\n","\n","# \"unbiased_colour_mnist\"\n","process_structure[\"unbias\"][\"model\"] = torchvision.models.vgg19(weights=None, num_classes=10).to(device) # Untrained model\n","process_structure[\"unbias\"][\"train\"] = train_unbiased_colour_mnist_model\n","process_structure[\"unbias\"][\"train_loader\"] = biased_train_dataloader\n","process_structure[\"unbias\"][\"test_loader\"] = biased_test_dataloader\n","process_structure[\"unbias\"][\"saveas\"] = save_unbiased_colour_mnist_model_as\n","process_structure[\"unbias\"][\"loadfrom\"] =  unbiased_colour_mnist_model_to_load\n","\n","\n","for key, val in process_structure.items():\n","    print(f\"Processing for {key=}\")\n","    if key == \"randinit\":\n","        if gen_randinit_model:  # create new model but don't train it\n","            logtofile(f\"model has already been initialised: save it as {val['saveas']}\")\n","            torch.save(val[\"model\"].state_dict(), val[\"saveas\"])\n","        else:\n","            logtofile(f\"{val['loadfrom']=}\")\n","            val[\"model\"].load_state_dict(torch.load(val[\"loadfrom\"], map_location=torch.device(device)))\n","    else:\n","        if val[\"train\"]:\n","            train_model(model=val[\"model\"], train_loader=val[\"train_loader\"],\n","                        epochs=original_train_epochs, saveas=val[\"saveas\"],\n","                        description=key, device=device, logtofile=logtofile,\n","                        milestones=[100,150],\n","                        lr=0.01, #0.1 didn't work for some datasets\n","                       )\n","        else:\n","            logtofile(f\"{val['loadfrom']=}\")\n","            val[\"model\"].load_state_dict(torch.load(val[\"loadfrom\"], map_location=torch.device(device)))\n","    val[\"model\"].eval()\n"]},{"cell_type":"markdown","id":"b169c5c7-7929-48e1-82eb-6f047aa4e5f2","metadata":{"id":"b169c5c7-7929-48e1-82eb-6f047aa4e5f2"},"source":["## Measure the Accuracy, Record the Confusion Matrix\n"]},{"cell_type":"code","execution_count":7,"id":"4fbb925a-269d-4027-9500-6cdce4de9d70","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4fbb925a-269d-4027-9500-6cdce4de9d70","executionInfo":{"status":"ok","timestamp":1747732420336,"user_tz":-60,"elapsed":4757,"user":{"displayName":"Damian Smith","userId":"13736799635312870126"}},"outputId":"8b43959a-5f88-4e4a-a8e9-a49724428013"},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy Calculation for VGG19 with model key='randinit' against dataset biased_test_dataloader\n","Test the Trained VGG19\n","Test Accuracy: 10.32 %\n","Confusion Matrix\n","tensor([[   0,    0,  980,    0,    0,    0,    0,    0,    0,    0],\n","        [   0,    0, 1135,    0,    0,    0,    0,    0,    0,    0],\n","        [   0,    0, 1032,    0,    0,    0,    0,    0,    0,    0],\n","        [   0,    0, 1010,    0,    0,    0,    0,    0,    0,    0],\n","        [   0,    0,  982,    0,    0,    0,    0,    0,    0,    0],\n","        [   0,    0,  892,    0,    0,    0,    0,    0,    0,    0],\n","        [   0,    0,  958,    0,    0,    0,    0,    0,    0,    0],\n","        [   0,    0, 1028,    0,    0,    0,    0,    0,    0,    0],\n","        [   0,    0,  974,    0,    0,    0,    0,    0,    0,    0],\n","        [   0,    0, 1009,    0,    0,    0,    0,    0,    0,    0]],\n","       dtype=torch.int32)\n","tensor(10000)\n","Already evaluated for ./results_2j_b_rank/randinit-bias-107_2025-03-26_18-57-22_SEED107_EPOCHS50_BGN0.1_exp2h_VGG19_randinit-test.csv\n","Accuracy Calculation for VGG19 with model key='unbias' against dataset biased_test_dataloader\n","Test the Trained VGG19\n","Test Accuracy: 98.41 %\n","Confusion Matrix\n","tensor([[ 974,    0,    0,    0,    0,    1,    0,    1,    3,    1],\n","        [   1, 1126,    2,    1,    1,    0,    1,    0,    3,    0],\n","        [   2,    0, 1019,    0,    0,    0,    1,    5,    5,    0],\n","        [   0,    0,    3, 1000,    0,    2,    0,    1,    3,    1],\n","        [   0,    0,    1,    0,  953,    0,    4,    2,    1,   21],\n","        [   2,    1,    0,    7,    0,  874,    2,    1,    4,    1],\n","        [   5,    3,    0,    1,    3,    3,  940,    0,    3,    0],\n","        [   2,    2,    3,    1,    0,    0,    0, 1013,    1,    6],\n","        [   2,    0,    3,    5,    0,    3,    0,    2,  955,    4],\n","        [   3,    3,    0,    6,    3,    1,    0,    5,    1,  987]],\n","       dtype=torch.int32)\n","tensor(10000)\n","Already evaluated for ./results_2j_b_rank/unbias-bias-107_2025-01-07_10-01-18_SEED107_EPOCHS50_BGN0.1_exp1g_VGG19_unbiased_colour_mnist-test.csv\n","original_accuracy={'randinit': 10.32, 'unbias': 98.41}\n"]}],"source":["\n","\n","original_accuracy = dict()\n","TDL = biased_test_dataloader\n","for key, val in process_structure.items():\n","    logtofile(f\"Accuracy Calculation for VGG19 with model {key=} against dataset biased_test_dataloader\")\n","    model = val[\"model\"]\n","    model.eval() # ALWAYS DO THIS BEFORE YOU EVALUATE MODELS\n","\n","    # Compute the model accuracy on the test set\n","    correct = 0\n","    total = 0\n","\n","    # assuming 10 classes\n","    # rows represent actual class, columns are predicted\n","    confusion_matrix = torch.zeros(10,10, dtype=torch.int)\n","\n","    for data in TDL:\n","        inputs, labels = data\n","\n","        inputs = inputs.to(device)\n","        labels = labels.to(device)\n","        predictions = torch.argmax(model(inputs),1)\n","\n","        matches = predictions == labels\n","        correct += matches.sum().item()\n","        total += len(labels)\n","        for idx, l in enumerate(labels):\n","            confusion_matrix[l, predictions[idx]] = 1 + confusion_matrix[l, predictions[idx]]\n","\n","    logtofile(\"Test the Trained VGG19\")\n","    acc = ((100.0 * correct) / total)\n","    logtofile('Test Accuracy: %2.2f %%' % acc)\n","    original_accuracy[key] = acc\n","    logtofile('Confusion Matrix')\n","    logtofile(confusion_matrix)\n","    logtofile(confusion_matrix.sum())\n","\n","    if not measure_rank:\n","\n","        dl = TDL\n","\n","        if val[\"train\"]:\n","            filename = val[\"saveas\"]\n","        else:\n","            filename = val[\"loadfrom\"]\n","        assert os.path.exists(filename)\n","\n","        #model = torchvision.models.vgg19(weights=None, num_classes=10)  # Untrained model\n","\n","        #state = torch.load(filename, map_location=torch.device(\"cpu\"))\n","        #model.load_state_dict(state, assign=True)\n","        #model = model.to(device)\n","\n","        #model = RcvVGG19(model, -1, vgg19_input_shape, device).to(device)\n","        out_filename = filename.split('/')[-1].replace('.weights', '-test.csv')\n","\n","        outpath = f\"./{results_root}_rank/{key}-bias-{seed}_{out_filename}\"  # denote output name as <model_training_type>-dataset-<name>\n","\n","        if os.path.exists(f\"{outpath}\"):\n","            logtofile(f\"Already evaluated for {outpath}\")\n","            continue\n","        #logtofile(f\"Measure Rank for {key=}\")\n","        print(f\"output to {outpath}\")\n","\n","        params = {}\n","        params[\"model\"] = key\n","        params[\"dataset\"] = \"bias\"\n","        params[\"seed\"] = seed\n","        if val[\"train\"]: # as only one network used, record its filename as both send and receive files\n","            params[\"send_file\"] = val[\"saveas\"]\n","            params[\"rcv_file\"] = val[\"saveas\"]\n","        else:\n","            params[\"send_file\"] = val[\"loadfrom\"]\n","            params[\"rcv_file\"] = val[\"loadfrom\"]\n","        params[\"val_acc\"] = acc / 100\n","        params[\"name\"] = \"only\"\n","\n","        results = []\n","        results.append(params)\n","        df = pd.DataFrame.from_records(results)\n","        df.to_csv(f\"{outpath}\")\n","\n","        del  params, df\n","        gc.collect()\n","\n","logtofile(f\"{original_accuracy=}\")"]},{"cell_type":"markdown","id":"da373b34-fe35-4256-a9e0-1040f699d45d","metadata":{"id":"da373b34-fe35-4256-a9e0-1040f699d45d"},"source":["## Measure Rank with bias dataloader (test) before cutting and stitching"]},{"cell_type":"code","execution_count":8,"id":"c4f74591-2a3f-4521-8aec-32c324125a5b","metadata":{"id":"c4f74591-2a3f-4521-8aec-32c324125a5b","executionInfo":{"status":"ok","timestamp":1747732420370,"user_tz":-60,"elapsed":27,"user":{"displayName":"Damian Smith","userId":"13736799635312870126"}}},"outputs":[],"source":["# For the Whole Model - but we will pass it through the RcvResNet18 function to get matching feature names\n","if measure_rank:\n","    for key, val in process_structure.items():\n","\n","        dl = biased_test_dataloader\n","\n","        if val[\"train\"]:\n","            filename = val[\"saveas\"]\n","        else:\n","            filename = val[\"loadfrom\"]\n","        assert os.path.exists(filename)\n","\n","        model = torchvision.models.vgg19(weights=None, num_classes=10)  # Untrained model\n","\n","        state = torch.load(filename, map_location=torch.device(\"cpu\"))\n","        model.load_state_dict(state, assign=True)\n","        model = model.to(device)\n","\n","        model = RcvVGG19(model, -1, vgg19_input_shape, device).to(device)\n","        out_filename = filename.split('/')[-1].replace('.weights', '-test.csv')\n","\n","        outpath = f\"./{results_root}_rank/{key}-bias-{seed}_{out_filename}\"  # denote output name as <model_training_type>-dataset-<name>\n","\n","        if os.path.exists(f\"{outpath}\"):\n","            logtofile(f\"Already evaluated for {outpath}\")\n","            continue\n","        logtofile(f\"Measure Rank for {key=}\")\n","        print(f\"output to {outpath}\")\n","\n","        params = {}\n","        params[\"model\"] = key\n","        params[\"dataset\"] = \"bias\"\n","        params[\"seed\"] = seed\n","        if val[\"train\"]: # as only one network used, record its filename as both send and receive files\n","            params[\"send_file\"] = val[\"saveas\"]\n","            params[\"rcv_file\"] = val[\"saveas\"]\n","        else:\n","            params[\"send_file\"] = val[\"loadfrom\"]\n","            params[\"rcv_file\"] = val[\"loadfrom\"]\n","        with torch.no_grad():\n","            layers, features, handles = install_hooks(model)\n","\n","            metrics = evaluate_model(model, dl, 'acc', verbose=2, device=device)\n","            params.update(metrics)\n","            classes = None\n","            df = perform_analysis(features, classes, layers, params, n=8000)\n","            df.to_csv(f\"{outpath}\")\n","\n","        for h in handles:\n","            h.remove()\n","        del model, layers, features, metrics, params, df, handles\n","        gc.collect()"]},{"cell_type":"markdown","id":"de2928d3-9c5f-411f-a590-14fd04026ab6","metadata":{"id":"de2928d3-9c5f-411f-a590-14fd04026ab6"},"source":["# Stitch at a given layer\n"]},{"cell_type":"markdown","id":"3cbaf773-ed43-4d91-b0a0-a35fd468ac02","metadata":{"id":"3cbaf773-ed43-4d91-b0a0-a35fd468ac02"},"source":["## Train the stitch layer"]},{"cell_type":"code","execution_count":9,"id":"5a1c72bd-a3d0-471d-809c-06e6d532d313","metadata":{"id":"5a1c72bd-a3d0-471d-809c-06e6d532d313","executionInfo":{"status":"ok","timestamp":1747732420406,"user_tz":-60,"elapsed":32,"user":{"displayName":"Damian Smith","userId":"13736799635312870126"}}},"outputs":[],"source":["if False:\n","    layer_to_cut_after = 1\n","    device = \"cpu\"\n","    model = torchvision.models.vgg19(weights=None, num_classes=10).to(device)  # Untrained model\n","    #model.load_state_dict(torch.load(filename, map_location=torch.device(device)))  # uses either the load/save name depending whether it'\n","    cut_layer_output_size = get_layer_output_shape(model, layer_to_cut_after, vgg19_input_shape, device, type=\"VGG19\")\n","    model_cut = RcvVGG19(model, layer_to_cut_after, vgg19_input_shape, device).to(device)\n","    print(model_cut)"]},{"cell_type":"code","execution_count":10,"id":"2c29d808-b86e-4a0c-a3c5-127c952f3ab9","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2c29d808-b86e-4a0c-a3c5-127c952f3ab9","executionInfo":{"status":"ok","timestamp":1747733816736,"user_tz":-60,"elapsed":1396328,"user":{"displayName":"Damian Smith","userId":"13736799635312870126"}},"outputId":"ca2d7244-992c-4a5f-9f43-db95c55c9dec"},"outputs":[{"output_type":"stream","name":"stdout","text":["Already evaluated for ./results_2j_b_rank/randinit1unbias-bias-107_2025-03-26_18-57-22_SEED107_EPOCHS50_BGN0.1_exp2h_VGG19_randinit-test.csv\n","Already evaluated for ./results_2j_b_rank/randinit8unbias-bias-107_2025-03-26_18-57-22_SEED107_EPOCHS50_BGN0.1_exp2h_VGG19_randinit-test.csv\n","Already evaluated for ./results_2j_b_rank/randinit22unbias-bias-107_2025-03-26_18-57-22_SEED107_EPOCHS50_BGN0.1_exp2h_VGG19_randinit-test.csv\n","Already evaluated for ./results_2j_b_rank/randinit29unbias-bias-107_2025-03-26_18-57-22_SEED107_EPOCHS50_BGN0.1_exp2h_VGG19_randinit-test.csv\n","Already evaluated for ./results_2j_b_rank/randinit35unbias-bias-107_2025-03-26_18-57-22_SEED107_EPOCHS50_BGN0.1_exp2h_VGG19_randinit-test.csv\n","Already evaluated for ./results_2j_b_rank/unbias1unbias-bias-107_2025-01-07_10-01-18_SEED107_EPOCHS50_BGN0.1_exp1g_VGG19_unbiased_colour_mnist-test.csv\n","Evaluate ranks and output to ./results_2j_b_rank/unbias8unbias-bias-107_2025-01-07_10-01-18_SEED107_EPOCHS50_BGN0.1_exp1g_VGG19_unbiased_colour_mnist-test.csv\n","stitch from model unbias\n","get_layer_output_shape for type='VGG19'\n","The shape of the output from layer 8 of send_model is: torch.Size([1, 128, 16, 16])\n","Train the stitch after layer 8\n","Epoch 0, loss 238.88\n","Epoch 1, loss 89.88\n","Epoch 2, loss 79.39\n","Epoch 3, loss 76.07\n","Epoch 4, loss 73.39\n","Epoch 5, loss 72.48\n","Epoch 6, loss 71.84\n","Epoch 7, loss 71.97\n","Epoch 8, loss 72.05\n","Epoch 9, loss 72.00\n","Epoch 10, loss 72.32\n","Epoch 11, loss 72.23\n","Epoch 12, loss 72.73\n","Epoch 13, loss 72.74\n","Epoch 14, loss 72.90\n","Epoch 15, loss 72.90\n","Epoch 16, loss 73.88\n","Epoch 17, loss 73.87\n","Epoch 18, loss 74.70\n","Epoch 19, loss 74.77\n","Epoch 20, loss 75.19\n","Epoch 21, loss 75.33\n","Epoch 22, loss 75.58\n","Epoch 23, loss 75.66\n","Epoch 24, loss 75.83\n","Epoch 25, loss 75.93\n","Epoch 26, loss 76.02\n","Epoch 27, loss 75.83\n","Epoch 28, loss 76.11\n","Epoch 29, loss 75.52\n","Epoch 30, loss 75.77\n","Epoch 31, loss 76.20\n","Epoch 32, loss 75.69\n","Epoch 33, loss 75.91\n","Epoch 34, loss 76.08\n","Epoch 35, loss 76.03\n","Epoch 36, loss 75.63\n","Epoch 37, loss 75.43\n","Epoch 38, loss 75.73\n","Epoch 39, loss 75.50\n","Epoch 40, loss 75.43\n","Epoch 41, loss 75.64\n","Epoch 42, loss 75.45\n","Epoch 43, loss 75.74\n","Epoch 44, loss 75.76\n","Epoch 45, loss 75.81\n","Epoch 46, loss 75.52\n","Epoch 47, loss 75.68\n","Epoch 48, loss 75.70\n","Epoch 49, loss 75.43\n","**** Finished Training ****\n","Number of weight / bias in stitch layer is 128\n","Change in stitch weights: 6.654252529144287\n","Largest abs weight change: 0.1571284830570221\n","Number of weights changing > 0.1 of that: 13440\n","Change in stitch bias: 0.5780701041221619\n","Largest abs bias change: 0.08697467297315598\n","Number of bias changing > 0.1 of that: 116\n","Test the trained stitch\n","Test Accuracy: 99.18 %\n","Confusion Matrix\n","tensor([[ 976,    0,    0,    0,    0,    0,    1,    1,    2,    0],\n","        [   0, 1131,    2,    0,    0,    0,    0,    0,    2,    0],\n","        [   3,    0, 1026,    1,    0,    0,    0,    0,    2,    0],\n","        [   0,    0,    0, 1006,    0,    0,    0,    1,    3,    0],\n","        [   0,    0,    0,    0,  979,    0,    1,    0,    1,    1],\n","        [   0,    0,    0,    2,    0,  888,    1,    0,    1,    0],\n","        [   2,    2,    0,    0,    2,    2,  945,    0,    5,    0],\n","        [   1,    0,    6,    1,    1,    0,    0, 1013,    1,    5],\n","        [   2,    0,    3,    0,    1,    1,    0,    2,  960,    5],\n","        [   1,    3,    0,    4,    4,    0,    0,    3,    0,  994]],\n","       dtype=torch.int32)\n","===================================================================\n","output to ./results_2j_b_rank/unbias8unbias-bias-107_2025-01-07_10-01-18_SEED107_EPOCHS50_BGN0.1_exp1g_VGG19_unbiased_colour_mnist-test.csv\n","Evaluate ranks and output to ./results_2j_b_rank/unbias22unbias-bias-107_2025-01-07_10-01-18_SEED107_EPOCHS50_BGN0.1_exp1g_VGG19_unbiased_colour_mnist-test.csv\n","stitch from model unbias\n","get_layer_output_shape for type='VGG19'\n","The shape of the output from layer 22 of send_model is: torch.Size([1, 512, 4, 4])\n","Train the stitch after layer 22\n","Epoch 0, loss 173.62\n","Epoch 1, loss 69.73\n","Epoch 2, loss 55.73\n","Epoch 3, loss 49.11\n","Epoch 4, loss 45.16\n","Epoch 5, loss 43.10\n","Epoch 6, loss 41.69\n","Epoch 7, loss 41.51\n","Epoch 8, loss 41.18\n","Epoch 9, loss 41.59\n","Epoch 10, loss 41.68\n","Epoch 11, loss 42.72\n","Epoch 12, loss 43.75\n","Epoch 13, loss 44.77\n","Epoch 14, loss 46.29\n","Epoch 15, loss 47.38\n","Epoch 16, loss 48.97\n","Epoch 17, loss 50.65\n","Epoch 18, loss 51.86\n","Epoch 19, loss 53.32\n","Epoch 20, loss 54.80\n","Epoch 21, loss 56.39\n","Epoch 22, loss 57.87\n","Epoch 23, loss 59.08\n","Epoch 24, loss 59.98\n","Epoch 25, loss 61.07\n","Epoch 26, loss 61.88\n","Epoch 27, loss 62.36\n","Epoch 28, loss 62.61\n","Epoch 29, loss 62.77\n","Epoch 30, loss 62.96\n","Epoch 31, loss 63.10\n","Epoch 32, loss 62.57\n","Epoch 33, loss 62.86\n","Epoch 34, loss 62.64\n","Epoch 35, loss 62.95\n","Epoch 36, loss 63.05\n","Epoch 37, loss 62.40\n","Epoch 38, loss 62.34\n","Epoch 39, loss 62.28\n","Epoch 40, loss 62.29\n","Epoch 41, loss 62.46\n","Epoch 42, loss 62.48\n","Epoch 43, loss 62.52\n","Epoch 44, loss 62.25\n","Epoch 45, loss 62.22\n","Epoch 46, loss 62.04\n","Epoch 47, loss 61.85\n","Epoch 48, loss 62.40\n","Epoch 49, loss 61.63\n","**** Finished Training ****\n","Number of weight / bias in stitch layer is 512\n","Change in stitch weights: 13.154555320739746\n","Largest abs weight change: 0.05829961597919464\n","Number of weights changing > 0.1 of that: 227423\n","Change in stitch bias: 0.5639434456825256\n","Largest abs bias change: 0.04371483623981476\n","Number of bias changing > 0.1 of that: 461\n","Test the trained stitch\n","Test Accuracy: 98.91 %\n","Confusion Matrix\n","tensor([[ 975,    0,    1,    0,    0,    0,    2,    1,    1,    0],\n","        [   0, 1127,    3,    0,    0,    0,    1,    0,    4,    0],\n","        [   4,    1, 1024,    0,    0,    0,    0,    1,    2,    0],\n","        [   0,    0,    0, 1001,    0,    5,    0,    2,    2,    0],\n","        [   0,    0,    0,    0,  978,    0,    1,    0,    0,    3],\n","        [   0,    0,    0,    7,    0,  880,    2,    0,    1,    2],\n","        [   3,    2,    0,    0,    2,    1,  948,    0,    2,    0],\n","        [   2,    1,    5,    0,    0,    0,    0, 1013,    1,    6],\n","        [   4,    0,    3,    1,    2,    0,    0,    2,  955,    7],\n","        [   2,    4,    0,    3,    4,    1,    0,    3,    2,  990]],\n","       dtype=torch.int32)\n","===================================================================\n","output to ./results_2j_b_rank/unbias22unbias-bias-107_2025-01-07_10-01-18_SEED107_EPOCHS50_BGN0.1_exp1g_VGG19_unbiased_colour_mnist-test.csv\n","Evaluate ranks and output to ./results_2j_b_rank/unbias29unbias-bias-107_2025-01-07_10-01-18_SEED107_EPOCHS50_BGN0.1_exp1g_VGG19_unbiased_colour_mnist-test.csv\n","stitch from model unbias\n","get_layer_output_shape for type='VGG19'\n","The shape of the output from layer 29 of send_model is: torch.Size([1, 512, 2, 2])\n","Train the stitch after layer 29\n","Epoch 0, loss 154.85\n","Epoch 1, loss 45.13\n","Epoch 2, loss 35.73\n","Epoch 3, loss 32.07\n","Epoch 4, loss 29.88\n","Epoch 5, loss 29.21\n","Epoch 6, loss 30.08\n","Epoch 7, loss 30.36\n","Epoch 8, loss 31.58\n","Epoch 9, loss 33.52\n","Epoch 10, loss 35.79\n","Epoch 11, loss 38.67\n","Epoch 12, loss 41.86\n","Epoch 13, loss 45.11\n","Epoch 14, loss 48.45\n","Epoch 15, loss 52.36\n","Epoch 16, loss 55.27\n","Epoch 17, loss 58.11\n","Epoch 18, loss 60.39\n","Epoch 19, loss 62.12\n","Epoch 20, loss 63.87\n","Epoch 21, loss 65.52\n","Epoch 22, loss 66.91\n","Epoch 23, loss 67.67\n","Epoch 24, loss 69.07\n","Epoch 25, loss 69.42\n","Epoch 26, loss 70.16\n","Epoch 27, loss 70.57\n","Epoch 28, loss 71.15\n","Epoch 29, loss 71.44\n","Epoch 30, loss 71.50\n","Epoch 31, loss 71.42\n","Epoch 32, loss 71.56\n","Epoch 33, loss 71.68\n","Epoch 34, loss 71.47\n","Epoch 35, loss 71.48\n","Epoch 36, loss 71.56\n","Epoch 37, loss 71.68\n","Epoch 38, loss 71.62\n","Epoch 39, loss 71.81\n","Epoch 40, loss 71.67\n","Epoch 41, loss 71.70\n","Epoch 42, loss 71.95\n","Epoch 43, loss 71.52\n","Epoch 44, loss 71.73\n","Epoch 45, loss 71.53\n","Epoch 46, loss 71.79\n","Epoch 47, loss 71.54\n","Epoch 48, loss 71.91\n","Epoch 49, loss 71.83\n","**** Finished Training ****\n","Number of weight / bias in stitch layer is 512\n","Change in stitch weights: 13.137073516845703\n","Largest abs weight change: 0.060048915445804596\n","Number of weights changing > 0.1 of that: 226371\n","Change in stitch bias: 0.5616792440414429\n","Largest abs bias change: 0.043746285140514374\n","Number of bias changing > 0.1 of that: 442\n","Test the trained stitch\n","Test Accuracy: 98.77 %\n","Confusion Matrix\n","tensor([[ 975,    0,    0,    0,    0,    0,    2,    1,    2,    0],\n","        [   0, 1128,    2,    0,    0,    0,    1,    0,    4,    0],\n","        [   4,    0, 1024,    0,    0,    0,    1,    0,    3,    0],\n","        [   0,    0,    0, 1000,    0,    4,    0,    2,    3,    1],\n","        [   0,    0,    0,    0,  969,    0,    3,    1,    0,    9],\n","        [   0,    0,    0,    5,    0,  885,    1,    0,    1,    0],\n","        [   3,    3,    1,    0,    3,    2,  944,    0,    2,    0],\n","        [   2,    3,    6,    2,    0,    0,    0, 1009,    1,    5],\n","        [   2,    0,    2,    1,    0,    4,    0,    2,  958,    5],\n","        [   2,    3,    0,    7,    3,    2,    1,    4,    2,  985]],\n","       dtype=torch.int32)\n","===================================================================\n","output to ./results_2j_b_rank/unbias29unbias-bias-107_2025-01-07_10-01-18_SEED107_EPOCHS50_BGN0.1_exp1g_VGG19_unbiased_colour_mnist-test.csv\n","Evaluate ranks and output to ./results_2j_b_rank/unbias35unbias-bias-107_2025-01-07_10-01-18_SEED107_EPOCHS50_BGN0.1_exp1g_VGG19_unbiased_colour_mnist-test.csv\n","stitch from model unbias\n","get_layer_output_shape for type='VGG19'\n","The shape of the output from layer 35 of send_model is: torch.Size([1, 512, 2, 2])\n","Train the stitch after layer 35\n","Epoch 0, loss 44.39\n","Epoch 1, loss 17.25\n","Epoch 2, loss 14.54\n","Epoch 3, loss 13.65\n","Epoch 4, loss 13.32\n","Epoch 5, loss 13.15\n","Epoch 6, loss 12.87\n","Epoch 7, loss 13.15\n","Epoch 8, loss 13.40\n","Epoch 9, loss 13.98\n","Epoch 10, loss 15.04\n","Epoch 11, loss 15.60\n","Epoch 12, loss 17.02\n","Epoch 13, loss 18.72\n","Epoch 14, loss 20.87\n","Epoch 15, loss 22.72\n","Epoch 16, loss 25.53\n","Epoch 17, loss 27.93\n","Epoch 18, loss 30.73\n","Epoch 19, loss 33.01\n","Epoch 20, loss 34.88\n","Epoch 21, loss 36.98\n","Epoch 22, loss 38.07\n","Epoch 23, loss 39.24\n","Epoch 24, loss 40.14\n","Epoch 25, loss 41.02\n","Epoch 26, loss 41.52\n","Epoch 27, loss 41.52\n","Epoch 28, loss 42.58\n","Epoch 29, loss 42.54\n","Epoch 30, loss 42.68\n","Epoch 31, loss 42.77\n","Epoch 32, loss 43.10\n","Epoch 33, loss 42.83\n","Epoch 34, loss 43.09\n","Epoch 35, loss 43.37\n","Epoch 36, loss 43.38\n","Epoch 37, loss 42.86\n","Epoch 38, loss 42.81\n","Epoch 39, loss 43.43\n","Epoch 40, loss 43.12\n","Epoch 41, loss 43.51\n","Epoch 42, loss 43.64\n","Epoch 43, loss 43.30\n","Epoch 44, loss 43.78\n","Epoch 45, loss 43.51\n","Epoch 46, loss 43.36\n","Epoch 47, loss 43.35\n","Epoch 48, loss 43.38\n","Epoch 49, loss 43.48\n","**** Finished Training ****\n","Number of weight / bias in stitch layer is 512\n","Change in stitch weights: 13.040953636169434\n","Largest abs weight change: 0.06406603753566742\n","Number of weights changing > 0.1 of that: 223981\n","Change in stitch bias: 0.5669589638710022\n","Largest abs bias change: 0.04351980984210968\n","Number of bias changing > 0.1 of that: 460\n","Test the trained stitch\n","Test Accuracy: 98.66 %\n","Confusion Matrix\n","tensor([[ 973,    0,    0,    0,    0,    0,    3,    1,    3,    0],\n","        [   0, 1129,    2,    0,    0,    0,    1,    0,    3,    0],\n","        [   2,    2, 1022,    0,    0,    0,    1,    1,    4,    0],\n","        [   0,    0,    3,  998,    0,    3,    0,    2,    3,    1],\n","        [   0,    1,    0,    0,  967,    0,    4,    1,    1,    8],\n","        [   2,    0,    0,    6,    0,  880,    1,    1,    2,    0],\n","        [   4,    3,    1,    1,    3,    3,  941,    0,    2,    0],\n","        [   2,    3,    3,    1,    0,    0,    0, 1015,    1,    3],\n","        [   2,    0,    2,    3,    0,    3,    0,    2,  958,    4],\n","        [   2,    4,    0,    8,    4,    1,    0,    5,    2,  983]],\n","       dtype=torch.int32)\n","===================================================================\n","output to ./results_2j_b_rank/unbias35unbias-bias-107_2025-01-07_10-01-18_SEED107_EPOCHS50_BGN0.1_exp1g_VGG19_unbiased_colour_mnist-test.csv\n"]}],"source":["stitching_accuracies = dict()\n","stitching_penalties = dict()\n","\n","for key, val in process_structure.items():\n","    stitching_accuracies[key] = dict()\n","    stitching_penalties[key] = dict()\n","    for layer_to_cut_after in [1,8,22,29,35]:  #  [1,3,6,8,11,13,15,17,20,22,24,26,29,31,33,35]\n","        ###################### Don't bother to stitch and train if we've already analysed it\n","\n","        if val[\"train\"]:\n","            filename = val[\"saveas\"]\n","        else:\n","            filename = val[\"loadfrom\"]\n","        rank_filename = filename.split('/')[-1].replace('.weights', '-test.csv')\n","        # denote output name as <model_training_type>-dataset-<name>\n","        # where <model_training_type> is [sender_model or X][layer_to_cut_after][Receiver_model]\n","        model_training_type = f\"{key}{layer_to_cut_after}unbias\"\n","        dataset_type = \"bias\"\n","\n","        outpath = f\"./{results_root}_rank/{model_training_type}-{dataset_type}-{seed}_{rank_filename}\"\n","        #PENGUIN\n","\n","        if os.path.exists(f\"{outpath}\"):\n","            logtofile(f\"Already evaluated for {outpath}\")\n","            continue\n","        ####################################################################################\n","        logtofile(f\"Evaluate ranks and output to {outpath}\")\n","        logtofile(f\"stitch from model {key}\")\n","\n","#        model = torchvision.models.vgg19(weights=None, num_classes=10).to(device)  # Untrained model\n","#        model.load_state_dict(torch.load(filename, map_location=torch.device(device)))  # uses either the load/save name depending whether it'\n","#        cut_layer_output_size = get_layer_output_shape(model, layer_to_cut_after, vgg19_input_shape, device, type=\"VGG19\")\n","#        model_cut = RcvVGG19(model, layer_to_cut_after, vgg19_input_shape, device).to(device)\n","\n","        # Always use the standard model as receiver\n","        rcv_model = torchvision.models.vgg19(weights=None, num_classes=10).to(device)  # Untrained model\n","        rcv_model.load_state_dict(torch.load(unbiased_colour_mnist_model_to_load, map_location=torch.device(device)))\n","        rcv_model.eval()\n","\n","        # train a stitch on the unbiased_colour dataset to compare receiver network performance with stitched\n","        model_stitched = StitchedVGG19(send_model=val[\"model\"],\n","                                          after_layer_index=layer_to_cut_after,\n","                                          rcv_model=rcv_model,\n","                                          input_image_shape=vgg19_input_shape, device=device  ).to(device)\n","\n","        #############################################################\n","        # store the initial stitch state\n","        initial_stitch_weight = model_stitched.stitch.s_conv1.weight.clone()\n","        initial_stitch_bias   = model_stitched.stitch.s_conv1.bias.clone()\n","        stitch_initial_weight_outpath    = f\"./{results_root}/STITCH_initial_weight_{model_training_type}-{dataset_type}-{seed}_{filename.split('/')[-1]}\"\n","        stitch_initial_bias_outpath      = f\"./{results_root}/STITCH_initial_bias_{model_training_type}-{dataset_type}-{seed}_{filename.split('/')[-1]}\"\n","        if save_stitch_delta:\n","            torch.save(initial_stitch_weight, stitch_initial_weight_outpath)\n","            torch.save(initial_stitch_bias, stitch_initial_bias_outpath)\n","        ############################################################\n","\n","        # define the loss function and the optimiser\n","        loss_function = nn.CrossEntropyLoss()\n","        optimiser = optim.SGD(model_stitched.parameters(), lr=1e-3, momentum=0.9, weight_decay=0.01)\n","\n","        # Put top model into train mode so that bn and dropout perform in training mode\n","        model_stitched.train()\n","        # Freeze the top model\n","        model_stitched.requires_grad_(False)\n","        # Un-Freeze the stitch layer\n","        for name, param in model_stitched.stitch.named_parameters():\n","            param.requires_grad_(True)\n","        logtofile(f\"Train the stitch after layer {layer_to_cut_after}\")\n","        # the epoch loop: note that we're training the whole network\n","\n","        for epoch in range(stitch_train_epochs):\n","            running_loss = 0.0\n","            for data in process_structure[key][\"train_loader\"]:\n","                # data is (representations, labels) tuple\n","                # get the inputs and put them on the GPU\n","                inputs, labels = data\n","\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","\n","                # zero the parameter gradients\n","                optimiser.zero_grad()\n","\n","                # forward + loss + backward + optimise (update weights)\n","                outputs = model_stitched(inputs)\n","                loss = loss_function(outputs, labels)\n","                loss.backward()\n","                optimiser.step()\n","\n","                # keep track of the loss this epoch\n","                running_loss += loss.item()\n","            logtofile(\"Epoch %d, loss %4.2f\" % (epoch, running_loss))\n","        logtofile('**** Finished Training ****')\n","\n","        model_stitched.eval() # ALWAYS DO THIS BEFORE YOU EVALUATE MODELS\n","\n","        ############################################################\n","        # store the trained stitch\n","        trained_stitch_weight = model_stitched.stitch.s_conv1.weight.clone()\n","        trained_stitch_bias   = model_stitched.stitch.s_conv1.bias.clone()\n","        stitch_trained_weight_outpath    = f\"./{results_root}/STITCH_trained_weight_{model_training_type}-{dataset_type}-{seed}_{filename.split('/')[-1]}\"\n","        stitch_trained_bias_outpath      = f\"./{results_root}/STITCH_trained_bias_{model_training_type}-{dataset_type}-{seed}_{filename.split('/')[-1]}\"\n","\n","        if save_stitch_delta:\n","            torch.save(trained_stitch_weight, stitch_trained_weight_outpath)\n","            torch.save(trained_stitch_bias, stitch_trained_bias_outpath)\n","\n","        print(f\"Number of weight / bias in stitch layer is {len(initial_stitch_weight)}\")\n","        stitch_weight_diff = trained_stitch_weight - initial_stitch_weight\n","        stitch_weight_delta = torch.linalg.norm(stitch_weight_diff).item()\n","        logtofile(f\"Change in stitch weights: {stitch_weight_delta}\")\n","        maxabsweight =  torch.max(stitch_weight_diff.abs()).item()\n","        logtofile(f\"Largest abs weight change: {maxabsweight}\")\n","        stitch_weight_number = torch.sum(torch.where(stitch_weight_diff.abs() > 0.1*maxabsweight, True, False)).item()\n","        logtofile(f\"Number of weights changing > 0.1 of that: {stitch_weight_number}\")\n","\n","        stitch_bias_diff = trained_stitch_bias - initial_stitch_bias\n","        stitch_bias_delta = torch.linalg.norm(stitch_bias_diff).item()\n","        logtofile(f\"Change in stitch bias: {stitch_bias_delta}\")\n","        maxabsbias =  torch.max(stitch_bias_diff.abs()).item()\n","        logtofile(f\"Largest abs bias change: {maxabsbias}\")\n","        stitch_bias_number = torch.sum(torch.where(stitch_bias_diff.abs() > 0.1*maxabsbias, True, False)).item()\n","        logtofile(f\"Number of bias changing > 0.1 of that: {stitch_bias_number}\")\n","\n","        #new_tensor = torch.load(\"foo_1_state.pt\")\n","        ##############################################################\n","\n","        logtofile(\"Test the trained stitch\")\n","        # Compute the model accuracy on the test set\n","        correct = 0\n","        total = 0\n","\n","        # assuming 10 classes\n","        # rows represent actual class, columns are predicted\n","        confusion_matrix = torch.zeros(10,10, dtype=torch.int)\n","        TDL = process_structure[key][\"test_loader\"]\n","        for data in TDL:\n","            inputs, labels = data\n","            #print(inputs)\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","\n","            predictions = torch.argmax(model_stitched(inputs),1)\n","            #print(model_cut(inputs))\n","            matches = predictions == labels.to(device)\n","            correct += matches.sum().item()\n","            total += len(labels)\n","\n","            for idx, l in enumerate(labels):\n","                confusion_matrix[l, predictions[idx]] = 1 + confusion_matrix[l, predictions[idx]]\n","        acc = ((100.0 * correct) / total)\n","        logtofile('Test Accuracy: %2.2f %%' % acc)\n","        logtofile('Confusion Matrix')\n","        logtofile(confusion_matrix)\n","        logtofile(\"===================================================================\")\n","        stitching_accuracies[key][layer_to_cut_after] = acc\n","        stitching_penalties[key][layer_to_cut_after] = original_accuracy[key] - acc\n","\n","        if measure_rank:\n","            dl = TDL\n","            params = {}\n","            params[\"model\"] = model_training_type\n","            params[\"dataset\"] = dataset_type\n","            params[\"seed\"] = seed\n","            if val[\"train\"]: # as only one network used, record its filename as both send and receive files\n","                params[\"send_file\"] = val[\"saveas\"]\n","                params[\"rcv_file\"] = val[\"saveas\"]\n","            else:\n","                params[\"send_file\"] = val[\"loadfrom\"]\n","                params[\"rcv_file\"] = val[\"loadfrom\"]\n","            params[\"stitch_weight_delta\"] = stitch_weight_delta\n","            params[\"stitch_bias_delta\"] = stitch_bias_delta\n","            params[\"stitch_weight_number\"] = stitch_weight_number\n","            params[\"stitch_bias_number\"] = stitch_bias_number\n","            params[\"my_acc\"] = acc\n","\n","            with torch.no_grad():\n","                layers, features, handles = install_hooks(model_stitched)\n","\n","                metrics = evaluate_model(model_stitched, dl, 'acc', verbose=2, device=device)\n","                params.update(metrics)\n","                classes = None\n","                df = perform_analysis(features, classes, layers, params, n=0)\n","                df.to_csv(f\"{outpath}\")\n","\n","            for h in handles:\n","                h.remove()\n","            del model_stitched, layers, features, metrics, params, df, handles\n","            gc.collect()\n","        else:\n","            dl = TDL\n","\n","            print(f\"output to {outpath}\")\n","\n","            params = {}\n","            params[\"model\"] = model_training_type\n","            params[\"dataset\"] = dataset_type\n","            params[\"seed\"] = seed\n","            if val[\"train\"]: # as only one network used, record its filename as both send and receive files\n","                params[\"send_file\"] = val[\"saveas\"]\n","                params[\"rcv_file\"] = val[\"saveas\"]\n","            else:\n","                params[\"send_file\"] = val[\"loadfrom\"]\n","                params[\"rcv_file\"] = val[\"loadfrom\"]\n","            params[\"val_acc\"] = acc / 100\n","            params[\"name\"] = \"only\"\n","\n","            results = []\n","            results.append(params)\n","            df = pd.DataFrame.from_records(results)\n","            df.to_csv(f\"{outpath}\")\n","\n","            del  params, df\n","            gc.collect()\n"]},{"cell_type":"code","execution_count":11,"id":"2b388249-6105-4382-be3f-e8b5c9678479","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2b388249-6105-4382-be3f-e8b5c9678479","executionInfo":{"status":"ok","timestamp":1747733816767,"user_tz":-60,"elapsed":28,"user":{"displayName":"Damian Smith","userId":"13736799635312870126"}},"outputId":"c7d178c3-1495-4031-b3c9-3454f286ac3d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Change in stitch weights: 13.040953636169434\n","Largest abs weight change: 0.06406603753566742\n","Number of weights changing > 0.1 of that: 223981\n","13.040953636169434\n","223981\n"]}],"source":["\n","stitch_weight_diff = trained_stitch_weight - initial_stitch_weight\n","stitch_weight_delta = torch.linalg.norm(stitch_weight_diff).item()\n","print(f\"Change in stitch weights: {stitch_weight_delta}\")\n","maxabsweight =  torch.max(stitch_weight_diff.abs()).item()\n","print(f\"Largest abs weight change: {maxabsweight}\")\n","stitch_weight_number = torch.sum(torch.where(stitch_weight_diff.abs() > 0.1*maxabsweight, True, False)).item()\n","print(f\"Number of weights changing > 0.1 of that: {stitch_weight_number}\")\n","\n","print(stitch_weight_delta)\n","print(stitch_weight_number)"]},{"cell_type":"code","execution_count":12,"id":"3db5808a-6351-4133-9e7c-85e8e9248cfb","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3db5808a-6351-4133-9e7c-85e8e9248cfb","executionInfo":{"status":"ok","timestamp":1747733816788,"user_tz":-60,"elapsed":19,"user":{"displayName":"Damian Smith","userId":"13736799635312870126"}},"outputId":"394fa3ea-c706-49f5-aacb-df58dfd8abb2"},"outputs":[{"output_type":"stream","name":"stdout","text":["stitching_accuracies={'randinit': {}, 'unbias': {8: 99.18, 22: 98.91, 29: 98.77, 35: 98.66}}\n","stitching_penalties={'randinit': {}, 'unbias': {8: -0.7700000000000102, 22: -0.5, 29: -0.35999999999999943, 35: -0.25}}\n"]}],"source":["logtofile(f\"{stitching_accuracies=}\")\n","logtofile(f\"{stitching_penalties=}\")"]},{"cell_type":"code","execution_count":13,"id":"cd669a19-7c42-4265-b8e8-57165995c097","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cd669a19-7c42-4265-b8e8-57165995c097","executionInfo":{"status":"ok","timestamp":1747733816809,"user_tz":-60,"elapsed":19,"user":{"displayName":"Damian Smith","userId":"13736799635312870126"}},"outputId":"0591c694-b6e7-4fb2-d581-8162d7047e93"},"outputs":[{"output_type":"stream","name":"stdout","text":["randinit-unbias\n","original_accuracy[s_key]=10.32\n","Stitch Accuracy\n","--------------------------\n","unbias-unbias\n","original_accuracy[s_key]=98.41\n","Stitch Accuracy\n","L8: 99.18\n","L22: 98.91\n","L29: 98.77\n","L35: 98.66\n","--------------------------\n"]}],"source":["for s_key in stitching_accuracies:\n","    logtofile(f\"{s_key}-unbias\")\n","    logtofile(f\"{original_accuracy[s_key]=}\")\n","    logtofile(\"Stitch Accuracy\")\n","    for layer in stitching_accuracies[s_key]:\n","        logtofile(f\"L{layer}: {stitching_accuracies[s_key][layer]}\")\n","    logtofile(\"--------------------------\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"colab":{"provenance":[],"gpuType":"A100"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}