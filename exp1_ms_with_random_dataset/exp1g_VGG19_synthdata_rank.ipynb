{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46e81ec0-e3c4-4060-97c1-bfccf47bc350",
   "metadata": {},
   "source": [
    "# To investigate Hernandez 2023 model-stitching analysis on VGG19\n",
    "Use VGG19 models trained on different datasets and test stitching at various layers. \n",
    "Based on exp1c and exp1f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51fc60f8-f6f6-469c-8705-c9015bd43951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "%matplotlib inline\n",
    "\n",
    "import argparse\n",
    "import gc\n",
    "import os.path\n",
    "\n",
    "import pandas as pd\n",
    "from torch.linalg import LinAlgError\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "import datetime\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "import os\n",
    "# add the path to find colour_mnist\n",
    "sys.path.append(os.path.abspath('../ReferenceCode'))\n",
    "import colour_mnist\n",
    "from stitch_utils import train_model, RcvVGG19, get_layer_output_shape\n",
    "from stitch_utils import generate_activations, SyntheticDataset\n",
    "import stitch_utils\n",
    "\n",
    "# add the path to find the rank analysis code\n",
    "# https://github.com/DHLSmith/jons-tunnel-effect/tree/NeurIPSPaper\n",
    "\n",
    "sys.path.append(os.path.abspath('../../jons-tunnel-effect/'))\n",
    "from utils.modelfitting import evaluate_model, set_seed\n",
    "from extract_weight_rank import install_hooks, perform_analysis\n",
    "\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "def logtofile(log_text, verbose=True):\n",
    "    if verbose:\n",
    "        print(log_text)\n",
    "    with open(save_log_as, \"a\") as f:    \n",
    "        print(log_text, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6761870b-2996-4763-8d90-76529ec5822e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Parameters\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 104\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "random.seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "train_all = True\n",
    "\n",
    "# MIX is 1/3 bgonly, 1/3 mnist only, 1/3 biased data\n",
    "train_mix_mnist_model = train_all\n",
    "mix_mnist_model_to_load = './results_1g/2024-08-11_11-32-44_SEED104_EPOCHS50_BGN0.1_exp1g_VGG19_mix_mnist.weights'\n",
    "\n",
    "# BW is greyscale mnist with no colour added (i.e. original mnist)\n",
    "train_bw_mnist_model = train_all  # when False, automatically loads a trained model\n",
    "bw_mnist_model_to_load = './results_1g/2024-08-10_22-57-13_SEED104_EPOCHS50_BGN0.1_exp1g_VGG19_bw_mnist.weights'\n",
    "\n",
    "# BG_ONLY contains no mnist data, just a coloured background\n",
    "train_bg_only_colour_mnist_model = train_all # when False, automatically loads a trained model\n",
    "bg_only_colour_mnist_model_to_load =  './results_1g/2024-08-10_22-57-13_SEED104_EPOCHS50_BGN0.1_exp1g_VGG19_bg_only_colour_mnist.weights'\n",
    "\n",
    "# BG_UNBIASED is digits with randomly selected colour background. Targets represent the colour\n",
    "train_bg_unbiased_colour_mnist_model = train_all  # when False, automatically loads a trained model\n",
    "bg_unbiased_colour_mnist_model_to_load = './results_1g/2024-08-11_11-32-44_SEED104_EPOCHS50_BGN0.1_exp1g_VGG19_bg_unbiased_colour_mnist.weights'\n",
    "\n",
    "# BIASED is digits with consistent per-class colour background. \n",
    "train_biased_colour_mnist_model = train_all  # when False, automatically loads a trained model\n",
    "biased_colour_mnist_model_to_load = './results_1g/2024-08-11_11-32-44_SEED104_EPOCHS50_BGN0.1_exp1g_VGG19_biased_colour_mnist.weights'\n",
    "\n",
    "# UNBIASED is digits with randoly selected colour background. Targets are digit values\n",
    "train_unbiased_colour_mnist_model = train_all  # when False, automatically loads a trained model\n",
    "unbiased_colour_mnist_model_to_load = './results_1g/2024-08-10_22-57-13_SEED104_EPOCHS50_BGN0.1_exp1g_VGG19_unbiased_colour_mnist.weights'\n",
    "\n",
    "original_train_epochs = 50\n",
    "bg_noise = 0.1\n",
    "synthetic_dataset_noise = 0.1\n",
    "\n",
    "stitch_train_epochs = 50\n",
    "#stitch_train_loss_tol = 40\n",
    "\n",
    "device = 'cuda:0'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20ee6e98-a6f2-4647-b378-5f7b1af48581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executed at 2025-01-13_16-05-59\n",
      "logging to ./results_1g/2025-01-13_16-05-59_SEED104_EPOCHS50_BGN0.1_exp1g_VGG19_log.txt\n",
      "seed=104\n",
      "bg_noise=0.1\n",
      "synthetic_dataset_noise=0.1\n",
      "train_mix_mnist_model=True\n",
      "save_mix_mnist_model_as='./results_1g/2025-01-13_16-05-59_SEED104_EPOCHS50_BGN0.1_exp1g_VGG19_mix_mnist.weights'\n",
      "original_train_epochs=50\n",
      "train_bw_mnist_model=True\n",
      "save_bw_mnist_model_as='./results_1g/2025-01-13_16-05-59_SEED104_EPOCHS50_BGN0.1_exp1g_VGG19_bw_mnist.weights'\n",
      "original_train_epochs=50\n",
      "train_bg_only_colour_mnist_model=True\n",
      "save_bg_only_colour_mnist_model_as='./results_1g/2025-01-13_16-05-59_SEED104_EPOCHS50_BGN0.1_exp1g_VGG19_bg_only_colour_mnist.weights'\n",
      "original_train_epochs=50\n",
      "train_bg_unbiased_colour_mnist_model=True\n",
      "save_bg_unbiased_colour_mnist_model_as='./results_1g/2025-01-13_16-05-59_SEED104_EPOCHS50_BGN0.1_exp1g_VGG19_bg_unbiased_colour_mnist.weights'\n",
      "original_train_epochs=50\n",
      "train_biased_colour_mnist_model=True\n",
      "save_biased_colour_mnist_model_as='./results_1g/2025-01-13_16-05-59_SEED104_EPOCHS50_BGN0.1_exp1g_VGG19_biased_colour_mnist.weights'\n",
      "original_train_epochs=50\n",
      "train_unbiased_colour_mnist_model=True\n",
      "save_unbiased_colour_mnist_model_as='./results_1g/2025-01-13_16-05-59_SEED104_EPOCHS50_BGN0.1_exp1g_VGG19_unbiased_colour_mnist.weights'\n",
      "original_train_epochs=50\n",
      "stitch_train_epochs=50\n",
      "================================================\n"
     ]
    }
   ],
   "source": [
    "# Generate filenames and log the setup details\n",
    "formatted_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "filename_prefix = f\"./results_1g/{formatted_time}_SEED{seed}_EPOCHS{original_train_epochs}_BGN{bg_noise}_exp1g_VGG19\"\n",
    "save_mix_mnist_model_as = f\"{filename_prefix}_mix_mnist.weights\"\n",
    "save_bw_mnist_model_as = f\"{filename_prefix}_bw_mnist.weights\"\n",
    "save_bg_only_colour_mnist_model_as = f\"{filename_prefix}_bg_only_colour_mnist.weights\"\n",
    "save_bg_unbiased_colour_mnist_model_as = f\"{filename_prefix}_bg_unbiased_colour_mnist.weights\"\n",
    "save_biased_colour_mnist_model_as = f\"{filename_prefix}_biased_colour_mnist.weights\"\n",
    "save_unbiased_colour_mnist_model_as = f\"{filename_prefix}_unbiased_colour_mnist.weights\"\n",
    "save_log_as = f\"{filename_prefix}_log.txt\"\n",
    "\n",
    "colour_mnist_shape = (3,28,28)\n",
    "vgg19_input_shape = (3,32,32) # monochrome\n",
    "\n",
    "logtofile(f\"Executed at {formatted_time}\")\n",
    "logtofile(f\"logging to {save_log_as}\")\n",
    "logtofile(f\"{seed=}\")\n",
    "logtofile(f\"{bg_noise=}\")\n",
    "logtofile(f\"{synthetic_dataset_noise=}\")\n",
    "\n",
    "logtofile(f\"{train_mix_mnist_model=}\")\n",
    "if train_mix_mnist_model:\n",
    "    logtofile(f\"{save_mix_mnist_model_as=}\")\n",
    "    logtofile(f\"{original_train_epochs=}\")\n",
    "else:\n",
    "    logtofile(f\"{mix_mnist_model_to_load=}\")\n",
    "\n",
    "logtofile(f\"{train_bw_mnist_model=}\")\n",
    "if train_bw_mnist_model:\n",
    "    logtofile(f\"{save_bw_mnist_model_as=}\")\n",
    "    logtofile(f\"{original_train_epochs=}\")\n",
    "else:\n",
    "    logtofile(f\"{bw_mnist_model_to_load=}\")\n",
    "    \n",
    "logtofile(f\"{train_bg_only_colour_mnist_model=}\")\n",
    "if train_bg_only_colour_mnist_model:\n",
    "    logtofile(f\"{save_bg_only_colour_mnist_model_as=}\")\n",
    "    logtofile(f\"{original_train_epochs=}\")\n",
    "else:\n",
    "    logtofile(f\"{bg_only_colour_mnist_model_to_load=}\")\n",
    "    \n",
    "logtofile(f\"{train_bg_unbiased_colour_mnist_model=}\")\n",
    "if train_bg_unbiased_colour_mnist_model:\n",
    "    logtofile(f\"{save_bg_unbiased_colour_mnist_model_as=}\")\n",
    "    logtofile(f\"{original_train_epochs=}\")\n",
    "else:\n",
    "    logtofile(f\"{bg_unbiased_colour_mnist_model_to_load=}\")\n",
    "\n",
    "logtofile(f\"{train_biased_colour_mnist_model=}\")\n",
    "if train_biased_colour_mnist_model:\n",
    "    logtofile(f\"{save_biased_colour_mnist_model_as=}\")\n",
    "    logtofile(f\"{original_train_epochs=}\")\n",
    "else:\n",
    "    logtofile(f\"{biased_colour_mnist_model_to_load=}\")\n",
    "\n",
    "logtofile(f\"{train_unbiased_colour_mnist_model=}\")\n",
    "if train_unbiased_colour_mnist_model:\n",
    "    logtofile(f\"{save_unbiased_colour_mnist_model_as=}\")\n",
    "    logtofile(f\"{original_train_epochs=}\")\n",
    "else:\n",
    "    logtofile(f\"{unbiased_colour_mnist_model_to_load=}\")\n",
    "\n",
    "logtofile(f\"{stitch_train_epochs=}\")\n",
    "logtofile(f\"================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc621f6-55e8-4191-8288-dc2493cd6bff",
   "metadata": {},
   "source": [
    "mnist and cifar-10 both use 10-classes, with 60_000 train samples and 10_000 test samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d34a54d2-c8fa-4f51-8809-7a40b4fefc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up dataloaders\n",
    "transform_resize=transforms.Resize(vgg19_input_shape[2])\n",
    "transform_bw = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),  # Convert to 3 channels \n",
    "    transform_resize,\n",
    "    transforms.ToTensor(),  # convert to tensor. We always do this one    \n",
    "    transforms.Normalize((0.1307,) * 3, (0.3081,) * 3)     \n",
    "])\n",
    "\n",
    "\n",
    "mnist_train = MNIST(\"./MNIST\", train=True, download=True, transform=transform_bw)\n",
    "mnist_test = MNIST(\"./MNIST\", train=False, download=True, transform=transform_bw)\n",
    "\n",
    "bw_train_dataloader = DataLoader(mnist_train, batch_size=64, shuffle=True, drop_last=True)\n",
    "bw_test_dataloader  = DataLoader(mnist_test,  batch_size=64, shuffle=True, drop_last=False)\n",
    "\n",
    "# mix dataloader\n",
    "mix_train_dataloader = colour_mnist.get_mixed_mnist_dataloader(root=\"./MNIST\", batch_size=64, train=True, bg_noise_level=bg_noise, standard_getitem=True, dl_transform=transform_resize)\n",
    "mix_test_dataloader = colour_mnist.get_mixed_mnist_dataloader(root=\"./MNIST\", batch_size=64,  train=False, bg_noise_level=bg_noise, standard_getitem=True, dl_transform=transform_resize)\n",
    "\n",
    "# bg_only means no digits - we will use colour as label\n",
    "bg_only_train_dataloader = colour_mnist.get_biased_mnist_dataloader(root=\"./MNIST\", batch_size=64, data_label_correlation=1.0, train=True, bg_noise_level=bg_noise, bg_only=True, standard_getitem=True, dl_transform=transform_resize)\n",
    "bg_only_test_dataloader = colour_mnist.get_biased_mnist_dataloader(root=\"./MNIST\", batch_size=64, data_label_correlation=1.0, train=False, bg_noise_level=bg_noise, bg_only=True, standard_getitem=True, dl_transform=transform_resize)\n",
    "\n",
    "# unbiased means each digit has correct label and random colour - but bg means we will use colour as label (i.e. the bias_target will be the target)\n",
    "bg_unbiased_train_dataloader = colour_mnist.get_biased_mnist_dataloader(root=\"./MNIST\", batch_size=64, data_label_correlation=0.1, train=True, bg_noise_level=bg_noise, bias_targets_as_targets=True, dl_transform=transform_resize)\n",
    "bg_unbiased_test_dataloader = colour_mnist.get_biased_mnist_dataloader(root=\"./MNIST\", batch_size=64, data_label_correlation=0.1, train=False, bg_noise_level=bg_noise, bias_targets_as_targets=True, dl_transform=transform_resize)\n",
    "\n",
    "# biased means each digit has correct label and consistent colour - Expect network to learn the colours only\n",
    "biased_train_dataloader = colour_mnist.get_biased_mnist_dataloader(root=\"./MNIST\", batch_size=64, data_label_correlation=1.0, train=True, bg_noise_level=bg_noise, standard_getitem=True, dl_transform=transform_resize)\n",
    "biased_test_dataloader = colour_mnist.get_biased_mnist_dataloader(root=\"./MNIST\", batch_size=64, data_label_correlation=1.0, train=False, bg_noise_level=bg_noise, standard_getitem=True, dl_transform=transform_resize)\n",
    "\n",
    "# unbiased means each digit has correct label and random colour - Expect network to disregard colours?\n",
    "unbiased_train_dataloader = colour_mnist.get_biased_mnist_dataloader(root=\"./MNIST\", batch_size=64, data_label_correlation=0.1, train=True, bg_noise_level=bg_noise, standard_getitem=True, dl_transform=transform_resize)\n",
    "unbiased_test_dataloader = colour_mnist.get_biased_mnist_dataloader(root=\"./MNIST\", batch_size=64, data_label_correlation=0.1, train=False, bg_noise_level=bg_noise, standard_getitem=True, dl_transform=transform_resize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24842d82-5f62-4d1b-acc5-d1997a08b0b9",
   "metadata": {},
   "source": [
    "## Set up VGG19 models and train it on versions of MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf635cfd-a9ad-4e37-98a0-80d0db2a3b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing for key='bw'\n",
      "Train Model on bw\n",
      "Epoch 1, loss 2156.62\n",
      "Epoch 2, loss 2156.53\n",
      "Epoch 3, loss 2156.48\n",
      "Epoch 4, loss 2156.42\n",
      "Epoch 5, loss 2156.38\n",
      "Epoch 6, loss 2156.34\n",
      "Epoch 7, loss 2156.32\n",
      "Epoch 8, loss 2156.30\n",
      "Epoch 9, loss 2156.29\n",
      "Epoch 10, loss 2156.26\n",
      "Epoch 11, loss 2156.25\n",
      "Epoch 12, loss 2156.24\n",
      "Epoch 13, loss 2156.23\n",
      "Epoch 14, loss 2156.23\n",
      "Epoch 15, loss 2156.23\n",
      "Epoch 16, loss 2156.22\n",
      "Epoch 17, loss 2156.21\n",
      "Epoch 18, loss 2156.21\n",
      "Epoch 19, loss 2156.21\n",
      "Epoch 20, loss 2156.20\n",
      "Epoch 21, loss 2156.20\n",
      "Epoch 22, loss 2156.19\n",
      "Epoch 23, loss 2156.19\n",
      "Epoch 24, loss 2156.19\n",
      "Epoch 25, loss 2156.20\n",
      "Epoch 26, loss 2156.19\n",
      "Epoch 27, loss 2156.19\n",
      "Epoch 28, loss 2156.19\n",
      "Epoch 29, loss 2156.18\n",
      "Epoch 30, loss 2156.18\n",
      "Epoch 31, loss 2156.18\n",
      "Epoch 32, loss 2156.20\n",
      "Epoch 33, loss 2156.19\n",
      "Epoch 34, loss 2156.19\n",
      "Epoch 35, loss 2156.19\n",
      "Epoch 36, loss 2156.19\n",
      "Epoch 37, loss 2156.19\n",
      "Epoch 38, loss 2156.19\n",
      "Epoch 39, loss 2156.19\n",
      "Epoch 40, loss 2156.19\n",
      "Epoch 41, loss 2156.18\n",
      "Epoch 42, loss 2156.19\n",
      "Epoch 43, loss 2156.19\n",
      "Epoch 44, loss 2156.18\n",
      "Epoch 45, loss 2156.18\n",
      "Epoch 46, loss 2156.19\n",
      "Epoch 47, loss 2156.18\n",
      "Epoch 48, loss 2156.18\n",
      "Epoch 49, loss 2156.18\n",
      "**** Finished Training ****\n",
      "saveas='./results_1g/2025-01-13_16-05-59_SEED104_EPOCHS50_BGN0.1_exp1g_VGG19_bw_mnist.weights'\n",
      "Processing for key='mix'\n",
      "Train Model on mix\n",
      "Epoch 0, loss 1305.11\n",
      "Epoch 1, loss 1024.07\n",
      "Epoch 2, loss 901.74\n",
      "Epoch 3, loss 707.08\n",
      "Epoch 4, loss 503.31\n",
      "Epoch 5, loss 270.00\n",
      "Epoch 6, loss 144.35\n",
      "Epoch 7, loss 98.23\n",
      "Epoch 8, loss 73.81\n",
      "Epoch 9, loss 58.45\n",
      "Epoch 10, loss 51.42\n",
      "Epoch 11, loss 44.29\n",
      "Epoch 12, loss 37.77\n",
      "Epoch 13, loss 34.90\n",
      "Epoch 14, loss 32.28\n",
      "Epoch 15, loss 28.72\n",
      "Epoch 16, loss 25.67\n",
      "Epoch 17, loss 26.09\n",
      "Epoch 18, loss 23.36\n",
      "Epoch 19, loss 22.00\n",
      "Epoch 20, loss 19.84\n",
      "Epoch 21, loss 18.18\n",
      "Epoch 22, loss 16.29\n",
      "Epoch 23, loss 15.15\n",
      "Epoch 24, loss 14.88\n",
      "Epoch 25, loss 14.69\n",
      "Epoch 26, loss 13.00\n",
      "Epoch 27, loss 13.30\n",
      "Epoch 28, loss 11.76\n",
      "Epoch 29, loss 13.08\n",
      "Epoch 30, loss 10.55\n",
      "Epoch 31, loss 10.92\n",
      "Epoch 32, loss 9.12\n",
      "Epoch 33, loss 9.21\n",
      "Epoch 34, loss 8.31\n",
      "Epoch 35, loss 7.73\n",
      "Epoch 36, loss 7.69\n",
      "Epoch 37, loss 7.58\n",
      "Epoch 38, loss 7.48\n",
      "Epoch 39, loss 7.13\n",
      "Epoch 40, loss 6.07\n",
      "Epoch 41, loss 5.89\n",
      "Epoch 42, loss 5.08\n",
      "Epoch 43, loss 5.18\n",
      "Epoch 44, loss 5.27\n",
      "Epoch 45, loss 4.19\n",
      "Epoch 46, loss 4.55\n",
      "Epoch 47, loss 4.41\n",
      "Epoch 48, loss 4.14\n",
      "Epoch 49, loss 4.42\n",
      "**** Finished Training ****\n",
      "saveas='./results_1g/2025-01-13_16-05-59_SEED104_EPOCHS50_BGN0.1_exp1g_VGG19_mix_mnist.weights'\n",
      "Processing for key='bgonly'\n",
      "Train Model on bgonly\n",
      "Epoch 0, loss 1716.99\n",
      "Epoch 1, loss 1629.11\n",
      "Epoch 2, loss 1283.62\n",
      "Epoch 3, loss 1032.93\n",
      "Epoch 4, loss 966.32\n",
      "Epoch 5, loss 940.62\n",
      "Epoch 6, loss 907.52\n",
      "Epoch 7, loss 740.86\n",
      "Epoch 8, loss 652.58\n",
      "Epoch 9, loss 531.30\n",
      "Epoch 10, loss 493.25\n",
      "Epoch 11, loss 474.89\n",
      "Epoch 12, loss 370.83\n",
      "Epoch 13, loss 272.18\n",
      "Epoch 14, loss 190.28\n",
      "Epoch 15, loss 82.71\n",
      "Epoch 16, loss 14.69\n",
      "Epoch 17, loss 2.07\n",
      "Epoch 18, loss 36.16\n",
      "Epoch 19, loss 1.87\n",
      "Epoch 20, loss 0.70\n",
      "Epoch 21, loss 0.65\n",
      "Epoch 22, loss 0.37\n",
      "Epoch 23, loss 0.18\n",
      "Epoch 24, loss 0.28\n",
      "Epoch 25, loss 0.42\n",
      "Epoch 26, loss 0.20\n",
      "Epoch 27, loss 0.12\n",
      "Epoch 28, loss 0.16\n",
      "Epoch 29, loss 0.08\n",
      "Epoch 30, loss 0.07\n",
      "Epoch 31, loss 0.11\n",
      "Epoch 32, loss 0.10\n",
      "Epoch 33, loss 0.12\n",
      "Epoch 34, loss 0.11\n",
      "Epoch 35, loss 0.11\n",
      "Epoch 36, loss 0.19\n",
      "Epoch 37, loss 0.05\n",
      "Epoch 38, loss 0.05\n",
      "Epoch 39, loss 0.48\n",
      "Epoch 40, loss 1.89\n",
      "Epoch 41, loss 0.08\n",
      "Epoch 42, loss 0.17\n",
      "Epoch 43, loss 0.07\n",
      "Epoch 44, loss 0.08\n",
      "Epoch 45, loss 0.06\n",
      "Epoch 46, loss 0.05\n",
      "Epoch 47, loss 0.05\n",
      "Epoch 48, loss 0.05\n",
      "Epoch 49, loss 0.04\n",
      "**** Finished Training ****\n",
      "saveas='./results_1g/2025-01-13_16-05-59_SEED104_EPOCHS50_BGN0.1_exp1g_VGG19_bg_only_colour_mnist.weights'\n",
      "Processing for key='bg'\n",
      "Train Model on bg\n",
      "Epoch 0, loss 1579.84\n",
      "Epoch 1, loss 1488.77\n",
      "Epoch 2, loss 1300.20\n",
      "Epoch 3, loss 1223.89\n",
      "Epoch 4, loss 1183.06\n",
      "Epoch 5, loss 1160.85\n",
      "Epoch 6, loss 1147.73\n",
      "Epoch 7, loss 1136.77\n",
      "Epoch 8, loss 1126.82\n",
      "Epoch 9, loss 1117.60\n",
      "Epoch 10, loss 1108.65\n",
      "Epoch 11, loss 1100.16\n",
      "Epoch 12, loss 1091.36\n",
      "Epoch 13, loss 1080.61\n",
      "Epoch 14, loss 1031.12\n",
      "Epoch 15, loss 877.45\n",
      "Epoch 16, loss 762.45\n",
      "Epoch 17, loss 665.45\n",
      "Epoch 18, loss 546.48\n",
      "Epoch 19, loss 407.48\n",
      "Epoch 20, loss 212.88\n",
      "Epoch 21, loss 78.72\n",
      "Epoch 22, loss 11.87\n",
      "Epoch 23, loss 10.76\n",
      "Epoch 24, loss 7.80\n",
      "Epoch 25, loss 1.31\n",
      "Epoch 26, loss 0.85\n",
      "Epoch 27, loss 0.58\n",
      "Epoch 28, loss 3.18\n",
      "Epoch 29, loss 1.00\n",
      "Epoch 30, loss 0.61\n",
      "Epoch 31, loss 0.82\n",
      "Epoch 32, loss 0.30\n",
      "Epoch 33, loss 443.32\n",
      "Epoch 34, loss 17.13\n",
      "Epoch 35, loss 3.12\n",
      "Epoch 36, loss 1.06\n",
      "Epoch 37, loss 0.55\n",
      "Epoch 38, loss 0.36\n",
      "Epoch 39, loss 0.32\n",
      "Epoch 40, loss 0.37\n",
      "Epoch 41, loss 0.61\n",
      "Epoch 42, loss 3.89\n",
      "Epoch 43, loss 0.33\n",
      "Epoch 44, loss 0.21\n",
      "Epoch 45, loss 0.16\n",
      "Epoch 46, loss 0.14\n",
      "Epoch 47, loss 0.13\n",
      "Epoch 48, loss 0.10\n",
      "Epoch 49, loss 0.10\n",
      "**** Finished Training ****\n",
      "saveas='./results_1g/2025-01-13_16-05-59_SEED104_EPOCHS50_BGN0.1_exp1g_VGG19_bg_unbiased_colour_mnist.weights'\n",
      "Processing for key='bias'\n",
      "Train Model on bias\n",
      "Epoch 0, loss 1641.07\n",
      "Epoch 1, loss 1348.31\n",
      "Epoch 2, loss 988.62\n",
      "Epoch 3, loss 577.16\n",
      "Epoch 4, loss 207.24\n",
      "Epoch 5, loss 76.15\n",
      "Epoch 6, loss 16.72\n",
      "Epoch 7, loss 37.10\n",
      "Epoch 8, loss 5.62\n",
      "Epoch 9, loss 5.25\n",
      "Epoch 10, loss 3.01\n",
      "Epoch 11, loss 1.75\n",
      "Epoch 12, loss 8.13\n",
      "Epoch 13, loss 0.73\n",
      "Epoch 14, loss 1.32\n",
      "Epoch 15, loss 0.44\n",
      "Epoch 16, loss 0.95\n",
      "Epoch 17, loss 0.58\n",
      "Epoch 18, loss 0.70\n",
      "Epoch 19, loss 0.75\n"
     ]
    }
   ],
   "source": [
    "\n",
    "process_structure = dict()\n",
    "\n",
    "process_structure[\"bw\"] = dict()\n",
    "process_structure[\"mix\"] = dict()\n",
    "process_structure[\"bgonly\"] = dict()\n",
    "process_structure[\"bg\"] = dict()\n",
    "process_structure[\"bias\"]      = dict()\n",
    "process_structure[\"unbias\"]    = dict()\n",
    "\n",
    "# \"bw\"\n",
    "process_structure[\"bw\"][\"model\"] = torchvision.models.vgg19(weights=None, num_classes=10).to(device) # Untrained model\n",
    "process_structure[\"bw\"][\"train\"] = train_bw_mnist_model \n",
    "process_structure[\"bw\"][\"train_loader\"] = bw_train_dataloader\n",
    "process_structure[\"bw\"][\"test_loader\"] = bw_test_dataloader\n",
    "process_structure[\"bw\"][\"saveas\"] = save_bw_mnist_model_as\n",
    "process_structure[\"bw\"][\"loadfrom\"] = bw_mnist_model_to_load\n",
    "\n",
    "# \"mix\"\n",
    "process_structure[\"mix\"][\"model\"] = torchvision.models.vgg19(weights=None, num_classes=10).to(device) # Untrained model\n",
    "process_structure[\"mix\"][\"train\"] = train_mix_mnist_model \n",
    "process_structure[\"mix\"][\"train_loader\"] = mix_train_dataloader\n",
    "process_structure[\"mix\"][\"test_loader\"] = mix_test_dataloader\n",
    "process_structure[\"mix\"][\"saveas\"] = save_mix_mnist_model_as\n",
    "process_structure[\"mix\"][\"loadfrom\"] = mix_mnist_model_to_load\n",
    "\n",
    "# \"bg_only_colour\"\n",
    "process_structure[\"bgonly\"][\"model\"] = torchvision.models.vgg19(weights=None, num_classes=10).to(device) # Untrained model\n",
    "process_structure[\"bgonly\"][\"train\"] = train_bg_only_colour_mnist_model \n",
    "process_structure[\"bgonly\"][\"train_loader\"] = bg_only_train_dataloader\n",
    "process_structure[\"bgonly\"][\"test_loader\"] = bg_only_test_dataloader\n",
    "process_structure[\"bgonly\"][\"saveas\"] = save_bg_only_colour_mnist_model_as\n",
    "process_structure[\"bgonly\"][\"loadfrom\"] = bg_only_colour_mnist_model_to_load\n",
    "\n",
    "# \"bg_unbiased_colour\"\n",
    "process_structure[\"bg\"][\"model\"] = torchvision.models.vgg19(weights=None, num_classes=10).to(device) # Untrained model\n",
    "process_structure[\"bg\"][\"train\"] = train_bg_unbiased_colour_mnist_model \n",
    "process_structure[\"bg\"][\"train_loader\"] = bg_unbiased_train_dataloader\n",
    "process_structure[\"bg\"][\"test_loader\"] = bg_unbiased_test_dataloader\n",
    "process_structure[\"bg\"][\"saveas\"] = save_bg_unbiased_colour_mnist_model_as\n",
    "process_structure[\"bg\"][\"loadfrom\"] = bg_unbiased_colour_mnist_model_to_load\n",
    "\n",
    "# \"biased_colour_mnist\"\n",
    "process_structure[\"bias\"][\"model\"] = torchvision.models.vgg19(weights=None, num_classes=10).to(device) # Untrained model\n",
    "process_structure[\"bias\"][\"train\"] = train_biased_colour_mnist_model\n",
    "process_structure[\"bias\"][\"train_loader\"] = biased_train_dataloader\n",
    "process_structure[\"bias\"][\"test_loader\"] = biased_test_dataloader\n",
    "process_structure[\"bias\"][\"saveas\"] = save_biased_colour_mnist_model_as\n",
    "process_structure[\"bias\"][\"loadfrom\"] =  biased_colour_mnist_model_to_load\n",
    "\n",
    "# \"unbiased_colour_mnist\"\n",
    "process_structure[\"unbias\"][\"model\"] = torchvision.models.vgg19(weights=None, num_classes=10).to(device) # Untrained model\n",
    "process_structure[\"unbias\"][\"train\"] = train_unbiased_colour_mnist_model\n",
    "process_structure[\"unbias\"][\"train_loader\"] = unbiased_train_dataloader\n",
    "process_structure[\"unbias\"][\"test_loader\"] = unbiased_test_dataloader\n",
    "process_structure[\"unbias\"][\"saveas\"] = save_unbiased_colour_mnist_model_as\n",
    "process_structure[\"unbias\"][\"loadfrom\"] =  unbiased_colour_mnist_model_to_load\n",
    "\n",
    "for key, val in process_structure.items():\n",
    "    print(f\"Processing for {key=}\")\n",
    "    if val[\"train\"]:\n",
    "        train_model(model=val[\"model\"], train_loader=val[\"train_loader\"], \n",
    "                    epochs=original_train_epochs, saveas=val[\"saveas\"], \n",
    "                    description=key, device=device, logtofile=logtofile,                     \n",
    "                    milestones=[100,150],\n",
    "                    lr=0.01, #0.1 didn't work for some datasets                    \n",
    "                   )\n",
    "    else:\n",
    "        logtofile(f\"{val['loadfrom']=}\")\n",
    "        val[\"model\"].load_state_dict(torch.load(val[\"loadfrom\"], map_location=torch.device(device)))\n",
    "    val[\"model\"].eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b169c5c7-7929-48e1-82eb-6f047aa4e5f2",
   "metadata": {},
   "source": [
    "## Measure the Accuracy, Record the Confusion Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbb925a-269d-4027-9500-6cdce4de9d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "original_accuracy = dict()\n",
    "for key, val in process_structure.items():\n",
    "    logtofile(f\"Accuracy Calculation for VGG19 with {key=}\")\n",
    "    model = val[\"model\"]\n",
    "    model.eval() # ALWAYS DO THIS BEFORE YOU EVALUATE MODELS\n",
    "    \n",
    "    # Compute the model accuracy on the test set\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # assuming 10 classes\n",
    "    # rows represent actual class, columns are predicted\n",
    "    confusion_matrix = torch.zeros(10,10, dtype=torch.int)\n",
    "    \n",
    "    for data in val[\"test_loader\"]:\n",
    "        inputs, labels = data\n",
    "        \n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        predictions = torch.argmax(model(inputs),1)\n",
    "        \n",
    "        matches = predictions == labels\n",
    "        correct += matches.sum().item()\n",
    "        total += len(labels)\n",
    "        for idx, l in enumerate(labels):\n",
    "            confusion_matrix[l, predictions[idx]] = 1 + confusion_matrix[l, predictions[idx]] \n",
    "    \n",
    "    logtofile(\"Test the Trained VGG19\")\n",
    "    acc = ((100.0 * correct) / total)\n",
    "    logtofile('Test Accuracy: %2.2f %%' % acc)\n",
    "    original_accuracy[key] = acc\n",
    "    logtofile('Confusion Matrix')\n",
    "    logtofile(confusion_matrix)\n",
    "    logtofile(confusion_matrix.sum())\n",
    "\n",
    "logtofile(f\"{original_accuracy=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da373b34-fe35-4256-a9e0-1040f699d45d",
   "metadata": {},
   "source": [
    "## Measure Rank with original dataloader (test) before cutting and stitching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f74591-2a3f-4521-8aec-32c324125a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the Whole Model - but we will pass it through the RcvResNet18 function to get matching feature names\n",
    "for key, val in process_structure.items():\n",
    "    \n",
    "    dl = val[\"test_loader\"]\n",
    "    \n",
    "    if val[\"train\"]:\n",
    "        filename = val[\"saveas\"] \n",
    "    else:    \n",
    "        filename = val[\"loadfrom\"] \n",
    "    assert os.path.exists(filename)\n",
    "\n",
    "    model = torchvision.models.vgg19(weights=None, num_classes=10)  # Untrained model\n",
    "\n",
    "    state = torch.load(filename, map_location=torch.device(\"cpu\"))\n",
    "    model.load_state_dict(state, assign=True)\n",
    "    model = model.to(device)\n",
    "                  \n",
    "    model = RcvVGG19(model, -1, vgg19_input_shape, device).to(device)\n",
    "    out_filename = filename.split('/')[-1].replace('.weights', '-test.csv')\n",
    "    \n",
    "    outpath = f\"./results_1g_rank/{key}-{key}-{seed}_{out_filename}\"  # denote output name as <model_training_type>-dataset-<name>\n",
    "    \n",
    "    if os.path.exists(f\"{outpath}\"):\n",
    "        logtofile(f\"Already evaluated for {outpath}\")\n",
    "        continue\n",
    "    logtofile(f\"Measure Rank for {key=}\")\n",
    "    print(f\"output to {outpath}\")\n",
    "            \n",
    "    params = {}\n",
    "    params[\"model\"] = key\n",
    "    params[\"dataset\"] = key\n",
    "    params[\"seed\"] = seed\n",
    "    if val[\"train\"]: # as only one network used, record its filename as both send and receive files\n",
    "        params[\"send_file\"] = val[\"saveas\"] \n",
    "        params[\"rcv_file\"] = val[\"saveas\"] \n",
    "    else:    \n",
    "        params[\"send_file\"] = val[\"loadfrom\"] \n",
    "        params[\"rcv_file\"] = val[\"loadfrom\"]\n",
    "    with torch.no_grad():\n",
    "        layers, features, handles = install_hooks(model)\n",
    "        \n",
    "        metrics = evaluate_model(model, dl, 'acc', verbose=2)\n",
    "        params.update(metrics)\n",
    "        classes = None\n",
    "        df = perform_analysis(features, classes, layers, params, n=8000)\n",
    "        df.to_csv(f\"{outpath}\")\n",
    "                \n",
    "    for h in handles:\n",
    "        h.remove()\n",
    "    del model, layers, features, metrics, params, df, handles\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2928d3-9c5f-411f-a590-14fd04026ab6",
   "metadata": {},
   "source": [
    "# Stitch at a given layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbaf773-ed43-4d91-b0a0-a35fd468ac02",
   "metadata": {},
   "source": [
    "## Train the stitch layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1c72bd-a3d0-471d-809c-06e6d532d313",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_to_cut_after = 1\n",
    "device = \"cpu\"\n",
    "model = torchvision.models.vgg19(weights=None, num_classes=10).to(device)  # Untrained model\n",
    "#model.load_state_dict(torch.load(filename, map_location=torch.device(device)))  # uses either the load/save name depending whether it'\n",
    "cut_layer_output_size = get_layer_output_shape(model, layer_to_cut_after, vgg19_input_shape, device, type=\"VGG19\")\n",
    "model_cut = RcvVGG19(model, layer_to_cut_after, vgg19_input_shape, device).to(device)\n",
    "print(model_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c29d808-b86e-4a0c-a3c5-127c952f3ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stitching_accuracies = dict()\n",
    "stitching_penalties = dict()\n",
    "\n",
    "for key, val in process_structure.items():        \n",
    "    stitching_accuracies[key] = dict()\n",
    "    stitching_penalties[key] = dict()\n",
    "    for layer_to_cut_after in [1,8,22,29,35]:  #  [1,3,6,8,11,13,15,17,20,22,24,26,29,31,33,35]\n",
    "        ###################### Don't bother to stitch and train if we've already analysed it\n",
    "        \n",
    "        if val[\"train\"]:\n",
    "            filename = val[\"saveas\"] \n",
    "        else:    \n",
    "            filename = val[\"loadfrom\"] \n",
    "        rank_filename = filename.split('/')[-1].replace('.weights', '-test.csv')        \n",
    "        # denote output name as <model_training_type>-dataset-<name>\n",
    "        # where <model_training_type> is [sender_model or X][layer_to_cut_after][Receiver_model]\n",
    "        model_training_type = f\"X{layer_to_cut_after}{key}\"\n",
    "        dataset_type = \"synth\"\n",
    "        outpath = f\"./results_1g_rank/{model_training_type}-{dataset_type}-{seed}_{rank_filename}\"  \n",
    "                        \n",
    "        if os.path.exists(f\"{outpath}\"):\n",
    "            logtofile(f\"Already evaluated for {outpath}\")\n",
    "            continue\n",
    "        ####################################################################################\n",
    "        logtofile(f\"Evaluate ranks and output to {outpath}\")\n",
    "        logtofile(f\"stitch into model {key}\")\n",
    "                \n",
    "        model = torchvision.models.vgg19(weights=None, num_classes=10).to(device)  # Untrained model\n",
    "        model.load_state_dict(torch.load(filename, map_location=torch.device(device)))  # uses either the load/save name depending whether it'\n",
    "        cut_layer_output_size = get_layer_output_shape(model, layer_to_cut_after, vgg19_input_shape, device, type=\"VGG19\")\n",
    "        model_cut = RcvVGG19(model, layer_to_cut_after, vgg19_input_shape, device).to(device)\n",
    "\n",
    "        #############################################################\n",
    "        # store the initial stitch state\n",
    "        initial_stitch_weight = model_cut.stitch.s_conv1.weight.clone()\n",
    "        initial_stitch_bias   = model_cut.stitch.s_conv1.bias.clone()\n",
    "        stitch_initial_weight_outpath    = f\"./results_1g/STITCH_initial_weight_{model_training_type}-{dataset_type}-{seed}_{filename.split('/')[-1]}\"  \n",
    "        stitch_initial_bias_outpath      = f\"./results_1g/STITCH_initial_bias_{model_training_type}-{dataset_type}-{seed}_{filename.split('/')[-1]}\"  \n",
    "        #torch.save(initial_stitch_weight, stitch_initial_weight_outpath)\n",
    "        #torch.save(initial_stitch_bias, stitch_initial_bias_outpath)\n",
    "        ############################################################\n",
    "        \n",
    "        representation_shape=(cut_layer_output_size[1:])\n",
    "        \n",
    "        syn_activations = generate_activations(num_classes=10, representation_shape=representation_shape)\n",
    "        logtofile(f\"after layer {layer_to_cut_after}, activations shape is {syn_activations.shape}\")\n",
    "        \n",
    "        syn_train_set  = SyntheticDataset(train=True, activations=syn_activations, noise=synthetic_dataset_noise)\n",
    "        syn_trainloader  = DataLoader(syn_train_set, batch_size=64, shuffle=True, drop_last=True)\n",
    "        syn_test_set  = SyntheticDataset(train=False, activations=syn_activations, noise=synthetic_dataset_noise)\n",
    "        syn_testloader  = DataLoader(syn_test_set, batch_size=64, shuffle=False, drop_last=False)        \n",
    "                \n",
    "        # define the loss function and the optimiser\n",
    "        loss_function = nn.CrossEntropyLoss()\n",
    "        optimiser = optim.SGD(model_cut.parameters(), lr=1e-3, momentum=0.9, weight_decay=0.01)\n",
    "        \n",
    "        # Put top model into train mode so that bn and dropout perform in training mode\n",
    "        model_cut.train()\n",
    "        # Freeze the top model\n",
    "        model_cut.requires_grad_(False)\n",
    "        # Un-Freeze the stitch layer\n",
    "        for name, param in model_cut.stitch.named_parameters():\n",
    "            param.requires_grad_(True)\n",
    "        logtofile(f\"Train the stitch to a top model cut after layer {layer_to_cut_after}\")    \n",
    "        # the epoch loop: note that we're training the whole network\n",
    "        for epoch in range(stitch_train_epochs):\n",
    "            running_loss = 0.0\n",
    "            for data in syn_trainloader:  # Use synthetic training data\n",
    "                # data is (representations, labels) tuple\n",
    "                # get the inputs and put them on the GPU\n",
    "                inputs, labels = data\n",
    "                                \n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "        \n",
    "                # zero the parameter gradients\n",
    "                optimiser.zero_grad()\n",
    "        \n",
    "                # forward + loss + backward + optimise (update weights)\n",
    "                outputs = model_cut(inputs)\n",
    "                loss = loss_function(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimiser.step()\n",
    "        \n",
    "                # keep track of the loss this epoch\n",
    "                running_loss += loss.item()\n",
    "            logtofile(\"Epoch %d, loss %4.2f\" % (epoch, running_loss))\n",
    "        logtofile('**** Finished Training ****')\n",
    "        \n",
    "        model_cut.eval() # ALWAYS DO THIS BEFORE YOU EVALUATE MODELS\n",
    "\n",
    "        ############################################################\n",
    "        # store the trained stitch\n",
    "        trained_stitch_weight = model_cut.stitch.s_conv1.weight.clone()\n",
    "        trained_stitch_bias   = model_cut.stitch.s_conv1.bias.clone()\n",
    "        stitch_trained_weight_outpath    = f\"./results_1g/STITCH_trained_weight_{model_training_type}-{dataset_type}-{seed}_{filename.split('/')[-1]}\"  \n",
    "        stitch_trained_bias_outpath      = f\"./results_1g/STITCH_trained_bias_{model_training_type}-{dataset_type}-{seed}_{filename.split('/')[-1]}\"  \n",
    "        #torch.save(trained_stitch_weight, stitch_trained_weight_outpath)\n",
    "        #torch.save(trained_stitch_bias, stitch_trained_bias_outpath)\n",
    "        \n",
    "        print(f\"Number of weight / bias in stitch layer is {len(initial_stitch_weight)}\")\n",
    "        stitch_weight_diff = trained_stitch_weight - initial_stitch_weight\n",
    "        stitch_weight_delta = torch.linalg.norm(stitch_weight_diff).item()\n",
    "        logtofile(f\"Change in stitch weights: {stitch_weight_delta}\")\n",
    "        maxabsweight =  torch.max(stitch_weight_diff.abs()).item()\n",
    "        logtofile(f\"Largest abs weight change: {maxabsweight}\")\n",
    "        stitch_weight_number = torch.sum(torch.where(stitch_weight_diff.abs() > 0.1*maxabsweight, True, False)).item()\n",
    "        logtofile(f\"Number of weights changing > 0.1 of that: {stitch_weight_number}\")\n",
    "        \n",
    "        stitch_bias_diff = trained_stitch_bias - initial_stitch_bias\n",
    "        stitch_bias_delta = torch.linalg.norm(stitch_bias_diff).item()\n",
    "        logtofile(f\"Change in stitch bias: {stitch_bias_delta}\")\n",
    "        maxabsbias =  torch.max(stitch_bias_diff.abs()).item()\n",
    "        logtofile(f\"Largest abs bias change: {maxabsbias}\")\n",
    "        stitch_bias_number = torch.sum(torch.where(stitch_bias_diff.abs() > 0.1*maxabsbias, True, False)).item()\n",
    "        logtofile(f\"Number of bias changing > 0.1 of that: {stitch_bias_number}\")\n",
    "\n",
    "        #new_tensor = torch.load(\"foo_1_state.pt\")\n",
    "        ##############################################################\n",
    "        \n",
    "        logtofile(\"Test the trained stitch\")        \n",
    "        # Compute the model accuracy on the test set\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        # assuming 10 classes\n",
    "        # rows represent actual class, columns are predicted\n",
    "        confusion_matrix = torch.zeros(10,10, dtype=torch.int)\n",
    "        \n",
    "        for data in syn_testloader:\n",
    "            inputs, labels = data\n",
    "            #print(inputs)   \n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            predictions = torch.argmax(model_cut(inputs),1)\n",
    "            #print(model_cut(inputs))\n",
    "            matches = predictions == labels.to(device)\n",
    "            correct += matches.sum().item()\n",
    "            total += len(labels)\n",
    "        \n",
    "            for idx, l in enumerate(labels):\n",
    "                confusion_matrix[l, predictions[idx]] = 1 + confusion_matrix[l, predictions[idx]] \n",
    "        acc = ((100.0 * correct) / total)\n",
    "        logtofile('Test Accuracy: %2.2f %%' % acc)\n",
    "        logtofile('Confusion Matrix')\n",
    "        logtofile(confusion_matrix)\n",
    "        logtofile(\"===================================================================\")\n",
    "        stitching_accuracies[key][layer_to_cut_after] = acc\n",
    "        stitching_penalties[key][layer_to_cut_after] = original_accuracy[key] - acc        \n",
    "        \n",
    "        dl = syn_testloader        \n",
    "        params = {}\n",
    "        params[\"model\"] = model_training_type\n",
    "        params[\"dataset\"] = dataset_type\n",
    "        params[\"seed\"] = seed\n",
    "        if val[\"train\"]: # as only one network used, record its filename as both send and receive files\n",
    "            params[\"rcv_file\"] = val[\"saveas\"] \n",
    "        else:    \n",
    "            params[\"rcv_file\"] = val[\"loadfrom\"]\n",
    "        params[\"stitch_weight_delta\"] = stitch_weight_delta\n",
    "        params[\"stitch_bias_delta\"] = stitch_bias_delta        \n",
    "        params[\"stitch_weight_number\"] = stitch_weight_number\n",
    "        params[\"stitch_bias_number\"] = stitch_bias_number\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            layers, features, handles = install_hooks(model_cut)\n",
    "            \n",
    "            metrics = evaluate_model(model_cut, dl, 'acc', verbose=2)\n",
    "            params.update(metrics)\n",
    "            classes = None\n",
    "            df = perform_analysis(features, classes, layers, params, n=8000)\n",
    "            df.to_csv(f\"{outpath}\")\n",
    "            \n",
    "        for h in handles:\n",
    "            h.remove()\n",
    "        del model_cut, layers, features, metrics, params, df, handles\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b388249-6105-4382-be3f-e8b5c9678479",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stitch_weight_diff = trained_stitch_weight - initial_stitch_weight\n",
    "stitch_weight_delta = torch.linalg.norm(stitch_weight_diff).item()\n",
    "print(f\"Change in stitch weights: {stitch_weight_delta}\")\n",
    "maxabsweight =  torch.max(stitch_weight_diff.abs()).item()\n",
    "print(f\"Largest abs weight change: {maxabsweight}\")\n",
    "stitch_weight_number = torch.sum(torch.where(stitch_weight_diff.abs() > 0.1*maxabsweight, True, False)).item()\n",
    "print(f\"Number of weights changing > 0.1 of that: {stitch_weight_number}\")\n",
    "\n",
    "print(stitch_weight_delta)\n",
    "print(stitch_weight_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db5808a-6351-4133-9e7c-85e8e9248cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "logtofile(f\"{stitching_accuracies=}\")\n",
    "logtofile(f\"{stitching_penalties=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd669a19-7c42-4265-b8e8-57165995c097",
   "metadata": {},
   "outputs": [],
   "source": [
    "for r_key in stitching_accuracies:\n",
    "    logtofile(f\"synth-{r_key}\")\n",
    "    logtofile(original_accuracy[r_key])\n",
    "    logtofile(\"Stitch Accuracy\")\n",
    "    for layer in stitching_accuracies[r_key]:\n",
    "        logtofile(stitching_accuracies[r_key][layer])\n",
    "    logtofile(\"--------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
