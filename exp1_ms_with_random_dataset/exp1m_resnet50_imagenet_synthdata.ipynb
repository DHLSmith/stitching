{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46e81ec0-e3c4-4060-97c1-bfccf47bc350",
   "metadata": {},
   "source": [
    "# To investigate Hernandez 2023 model-stitching analysis\n",
    "Hernandez found that stitching from later in a resnet to earlier was still stitching-connected.\n",
    "I would like to make a synthetic dataset in which each image of a class is replaced by the same, randomly generated activations.\n",
    "Use the Imagenet pretrained ResNet50 model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51fc60f8-f6f6-469c-8705-c9015bd43951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "%matplotlib inline\n",
    "\n",
    "import argparse\n",
    "import gc\n",
    "import os.path\n",
    "\n",
    "import pandas as pd\n",
    "from torch.linalg import LinAlgError\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "import datetime\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "import os\n",
    "# add the path to find colour_mnist\n",
    "sys.path.append(os.path.abspath('../ReferenceCode'))\n",
    "import colour_mnist\n",
    "from stitch_utils import train_model, RcvResNet50, get_layer_output_shape\n",
    "from stitch_utils import generate_activations, DynamicSyntheticDataset\n",
    "import stitch_utils\n",
    "\n",
    "# add the path to find the rank analysis code\n",
    "# https://github.com/DHLSmith/jons-tunnel-effect/tree/NeurIPSPaper\n",
    "\n",
    "sys.path.append(os.path.abspath('../../jons-tunnel-effect/'))\n",
    "from utils.modelfitting import evaluate_model, set_seed\n",
    "from extract_weight_rank import install_hooks, perform_analysis\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "def logtofile(log_text, verbose=True):\n",
    "    if verbose:\n",
    "        print(log_text)\n",
    "    with open(save_log_as, \"a\") as f:    \n",
    "        print(log_text, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6761870b-2996-4763-8d90-76529ec5822e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Parameters\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 102\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "random.seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "results_root = \"results_1m\"\n",
    "synthetic_dataset_noise = 0.1\n",
    "num_classes = 1000\n",
    "num_train_samples = 1_200_000 \n",
    "num_test_samples  = 120_000 \n",
    "stitch_train_epochs = 3\n",
    "device = 'cuda:3'\n",
    "\n",
    "measure_rank = False\n",
    "save_stitch_delta = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20ee6e98-a6f2-4647-b378-5f7b1af48581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executed at 2025-01-21_09-20-17\n",
      "logging to ./results_1m/2025-01-21_09-20-17_SEED102_exp1m_ResNet50_log.txt\n",
      "seed=102\n",
      "synthetic_dataset_noise=0.1\n",
      "num_classes=1000 num_train_samples=1200000 num_test_samples=120000\n",
      "model will be model = torchvision.models.resnet50(pretrained=True)\n",
      "stitch_train_epochs=3\n",
      "================================================\n"
     ]
    }
   ],
   "source": [
    "# Generate filenames and log the setup details\n",
    "formatted_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "filename_prefix = f\"./{results_root}/{formatted_time}_SEED{seed}_exp1m_ResNet50\"\n",
    "\n",
    "save_log_as = f\"{filename_prefix}_log.txt\"\n",
    "\n",
    "#colour_mnist_shape = (3,28,28)\n",
    "imagenet_shape = (3,224,224)\n",
    "\n",
    "logtofile(f\"Executed at {formatted_time}\")\n",
    "logtofile(f\"logging to {save_log_as}\")\n",
    "logtofile(f\"{seed=}\")\n",
    "logtofile(f\"{synthetic_dataset_noise=}\")\n",
    "logtofile(f\"{num_classes=} {num_train_samples=} {num_test_samples=}\")\n",
    "\n",
    "logtofile(f\"model will be model = torchvision.models.resnet50(pretrained=True)\")\n",
    "\n",
    "logtofile(f\"{stitch_train_epochs=}\")\n",
    "logtofile(f\"================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc621f6-55e8-4191-8288-dc2493cd6bff",
   "metadata": {},
   "source": [
    "mnist and cifar-10 both use 10-classes, with 60_000 train samples and 10_000 test samples. \n",
    "ImageNet has 1000 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d34a54d2-c8fa-4f51-8809-7a40b4fefc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # Set up dataloaders\n",
    "    transform_bw = transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=3),  # Convert to 3 channels    \n",
    "        transforms.ToTensor(),  # convert to tensor. We always do this one    \n",
    "        transforms.Normalize((0.1307,) * 3, (0.3081,) * 3)     \n",
    "    ])\n",
    "    \n",
    "    transform_list = [\n",
    "        transforms.Resize(size=(512, 512)),\n",
    "        transforms.RandomCrop(256),\n",
    "        transforms.ToTensor()\n",
    "    ]\n",
    "    \n",
    "    mnist_train = MNIST(\"./MNIST\", train=True, download=True, transform=transform_bw)\n",
    "    mnist_test = MNIST(\"./MNIST\", train=False, download=True, transform=transform_bw)\n",
    "    \n",
    "    bw_train_dataloader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    bw_test_dataloader  = DataLoader(mnist_test,  batch_size=batch_size, shuffle=True, drop_last=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24842d82-5f62-4d1b-acc5-d1997a08b0b9",
   "metadata": {},
   "source": [
    "## Set up resnet50 models and train it on versions of MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf635cfd-a9ad-4e37-98a0-80d0db2a3b9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "process_structure = dict()\n",
    "\n",
    "process_structure[\"imagenet\"] = dict()\n",
    "\n",
    "# \"mix\"\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "process_structure[\"imagenet\"][\"model\"] = torchvision.models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "# print(process_structure[\"imagenet\"][\"model\"])\n",
    "\n",
    "#process_structure[\"imagenet\"][\"train\"] = False \n",
    "#process_structure[\"imagenet\"][\"train_loader\"] = mix_train_dataloader\n",
    "#process_structure[\"imagenet\"][\"test_loader\"] = mix_test_dataloader\n",
    "#process_structure[\"imagenet\"][\"saveas\"] = save_mix_mnist_model_as\n",
    "#process_structure[\"imagenet\"][\"loadfrom\"] = mix_mnist_model_to_load\n",
    "\n",
    "process_structure[\"imagenet\"][\"model\"].eval()\n",
    "process_structure[\"imagenet\"][\"model\"].to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b169c5c7-7929-48e1-82eb-6f047aa4e5f2",
   "metadata": {},
   "source": [
    "## Measure the Accuracy, Record the Confusion Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fbb925a-269d-4027-9500-6cdce4de9d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not measuring initial accuracy as I assume this is available\n"
     ]
    }
   ],
   "source": [
    "logtofile(\"not measuring initial accuracy as I assume this is available\")\n",
    "if False:\n",
    "    original_accuracy = dict()\n",
    "    for key, val in process_structure.items():\n",
    "        logtofile(f\"Accuracy Calculation for ResNet50 with {key=}\")\n",
    "        model = val[\"model\"]\n",
    "        model.eval() # ALWAYS DO THIS BEFORE YOU EVALUATE MODELS\n",
    "        \n",
    "        # Compute the model accuracy on the test set\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        # assuming 10 classes\n",
    "        # rows represent actual class, columns are predicted\n",
    "        confusion_matrix = torch.zeros(num_classes,num_classes, dtype=torch.int)\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        for data in val[\"test_loader\"]:\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            predictions = torch.argmax(model(inputs),1)\n",
    "            \n",
    "            matches = predictions == labels\n",
    "            correct += matches.sum().item()\n",
    "            total += len(labels)\n",
    "            for idx, l in enumerate(labels):\n",
    "                confusion_matrix[l, predictions[idx]] = 1 + confusion_matrix[l, predictions[idx]] \n",
    "        \n",
    "        logtofile(\"Test the Trained Resnet50\")\n",
    "        acc = ((100.0 * correct) / total)\n",
    "        logtofile('Test Accuracy: %2.2f %%' % acc)\n",
    "        original_accuracy[key] = acc\n",
    "        logtofile('Confusion Matrix')\n",
    "        logtofile(confusion_matrix)\n",
    "        logtofile(confusion_matrix.sum())\n",
    "        if not measure_rank:\n",
    "            raise ValueError(\"This needs to be implemented\")\n",
    "    \n",
    "    logtofile(f\"{original_accuracy=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da373b34-fe35-4256-a9e0-1040f699d45d",
   "metadata": {},
   "source": [
    "## Measure Rank with original dataloader (test) before cutting and stitching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db170b3d-ee28-48de-98ae-2f12e0b4cc54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:3\n",
      "get_layer_output_shape for type='ResNet50'\n",
      "The shape of the output from layer 4 is: torch.Size([1, 256, 56, 56])\n",
      "Output Size is : 802816\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "num_layers_in_model = len(list(process_structure[\"imagenet\"][\"model\"].children()))\n",
    "\n",
    "# Specify the layer name you're interested in\n",
    "print(device)\n",
    "#print(model.training)\n",
    "layer_index = 4\n",
    "output_shape = get_layer_output_shape(process_structure[\"imagenet\"][\"model\"], layer_index, imagenet_shape, device=device, type=\"ResNet50\")\n",
    "print(f\"The shape of the output from layer {layer_index} is: {output_shape}\")\n",
    "print(f\"Output Size is : {output_shape.numel()}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbaf773-ed43-4d91-b0a0-a35fd468ac02",
   "metadata": {},
   "source": [
    "## Train the stitch layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4f74591-2a3f-4521-8aec-32c324125a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if measure_rank:\n",
    "    # For the Whole Model - but we will pass it through the RcvResNet150 function to get matching feature names\n",
    "    for key, val in process_structure.items():\n",
    "        \n",
    "        dl = val[\"test_loader\"]\n",
    "        \n",
    "        if val[\"train\"]:\n",
    "            filename = val[\"saveas\"] \n",
    "        else:    \n",
    "            filename = val[\"loadfrom\"] \n",
    "        assert os.path.exists(filename)\n",
    "        mdl = torchvision.models.resnet18(num_classes=10) # Untrained model\n",
    "        state = torch.load(filename, map_location=torch.device(\"cpu\"))\n",
    "        mdl.load_state_dict(state, assign=True)\n",
    "        mdl=mdl.to(device)\n",
    "        mdl = RcvResNet18(mdl, -1, colour_mnist_shape, device).to(device)\n",
    "    \n",
    "        out_filename = filename.split('/')[-1].replace('.weights', '-test.csv')\n",
    "        \n",
    "        outpath = f\"./results_4_epochs_rank/{key}-{key}-{seed}_{out_filename}\"  # denote output name as <model_training_type>-dataset-<name>\n",
    "        \n",
    "        if os.path.exists(f\"{outpath}\"):\n",
    "            logtofile(f\"Already evaluated for {outpath}\")\n",
    "            continue\n",
    "        logtofile(f\"Measure Rank for {key=}\")\n",
    "        print(f\"output to {outpath}\")\n",
    "                \n",
    "        params = {}\n",
    "        params[\"model\"] = key\n",
    "        params[\"dataset\"] = key\n",
    "        params[\"seed\"] = seed\n",
    "        if val[\"train\"]: # as only one network used, record its filename as both send and receive files\n",
    "            params[\"send_file\"] = val[\"saveas\"] \n",
    "            params[\"rcv_file\"] = val[\"saveas\"] \n",
    "        else:    \n",
    "            params[\"send_file\"] = val[\"loadfrom\"] \n",
    "            params[\"rcv_file\"] = val[\"loadfrom\"]\n",
    "        with torch.no_grad():\n",
    "            layers, features, handles = install_hooks(mdl)\n",
    "            \n",
    "            metrics = evaluate_model(mdl, dl, 'acc', verbose=2)\n",
    "            params.update(metrics)\n",
    "            classes = None\n",
    "            df = perform_analysis(features, classes, layers, params, n=-1)\n",
    "            df.to_csv(f\"{outpath}\")\n",
    "                    \n",
    "        for h in handles:\n",
    "            h.remove()\n",
    "        del mdl, layers, features, metrics, params, df, handles\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c29d808-b86e-4a0c-a3c5-127c952f3ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate ranks and output to ./results_1m_rank/X4imagenet-synth-102_resnet50_imagenet1k-test.csv\n",
      "stitch into model imagenet\n",
      "get_layer_output_shape for type='ResNet50'\n",
      "get_layer_output_shape for type='ResNet50'\n",
      "The shape of the output from layer 4 is: torch.Size([1, 256, 56, 56]), with 802816 elements\n",
      "About to generate activations\n",
      "after layer 4, activations shape is torch.Size([1000, 256, 56, 56])\n",
      "About to create datasets\n",
      "Train the stitch to a top model cut after layer 4\n"
     ]
    }
   ],
   "source": [
    "stitching_accuracies = dict()\n",
    "#stitching_penalties = dict()\n",
    "\n",
    "for key, val in process_structure.items():        \n",
    "    stitching_accuracies[key] = dict()\n",
    "    #stitching_penalties[key] = dict()\n",
    "    for layer_to_cut_after in [num_layers_in_model - 6]:   #range(3,num_layers_in_model - 1):\n",
    "        ###################### Don't bother to stitch and train if we've already analysed it\n",
    "        \n",
    "        filename = 'resnet50_imagenet1k-test.weights'\n",
    "        rank_filename = 'resnet50_imagenet1k-test.csv'     \n",
    "        # denote output name as <model_training_type>-dataset-<name>\n",
    "        # where <model_training_type> is [sender_model or X][layer_to_cut_after][Receiver_model]\n",
    "        model_training_type = f\"X{layer_to_cut_after}{key}\"\n",
    "        dataset_type = \"synth\"\n",
    "        outpath = f\"./{results_root}_rank/{model_training_type}-{dataset_type}-{seed}_{rank_filename}\"  \n",
    "                        \n",
    "        if os.path.exists(f\"{outpath}\"):\n",
    "            logtofile(f\"Already evaluated for {outpath}\")\n",
    "            continue\n",
    "        ####################################################################################\n",
    "        logtofile(f\"Evaluate ranks and output to {outpath}\")\n",
    "        logtofile(f\"stitch into model {key}\")\n",
    "        \n",
    "        model = process_structure[\"imagenet\"][\"model\"]\n",
    "        #model.load_state_dict(torch.load(filename, map_location=torch.device(device)))  # uses either the load/save name depending whether it'\n",
    "        cut_layer_output_size = get_layer_output_shape(model, layer_to_cut_after, imagenet_shape, device, type=\"ResNet50\")\n",
    "        model_cut = RcvResNet50(model, layer_to_cut_after, imagenet_shape, device).to(device)\n",
    "\n",
    "        if save_stitch_delta:\n",
    "            #############################################################\n",
    "            # store the initial stitch state\n",
    "            initial_stitch_weight = model_cut.stitch.s_conv1.weight.clone()\n",
    "            initial_stitch_bias   = model_cut.stitch.s_conv1.bias.clone()\n",
    "            stitch_initial_weight_outpath    = f\"./{results_root}/STITCH_initial_weight_{model_training_type}-{dataset_type}-{seed}_{filename.split('/')[-1]}\"  \n",
    "            stitch_initial_bias_outpath      = f\"./{results_root}/STITCH_initial_bias_{model_training_type}-{dataset_type}-{seed}_{filename.split('/')[-1]}\"  \n",
    "            torch.save(initial_stitch_weight, stitch_initial_weight_outpath)\n",
    "            torch.save(initial_stitch_bias, stitch_initial_bias_outpath)\n",
    "            ############################################################\n",
    "        representation_shape=(cut_layer_output_size[1:])\n",
    "        \n",
    "        logtofile(\"About to generate activations\")\n",
    "        syn_activations = generate_activations(num_classes=num_classes, representation_shape=representation_shape)\n",
    "        logtofile(f\"after layer {layer_to_cut_after}, activations shape is {syn_activations.shape}\")\n",
    "\n",
    "        logtofile(\"About to create datasets\")\n",
    "        # syn_train_set  = SyntheticDataset(train=True, activations=syn_activations, noise=synthetic_dataset_noise, num_classes=num_classes)\n",
    "        syn_train_set  = DynamicSyntheticDataset(num_samples=num_train_samples, activations=syn_activations, noise=synthetic_dataset_noise, local_seed=seed)\n",
    "        syn_trainloader  = DataLoader(syn_train_set, batch_size=128, shuffle=True, drop_last=True)\n",
    "        #syn_test_set  = SyntheticDataset(train=False, activations=syn_activations, noise=synthetic_dataset_noise, num_classes=num_classes)\n",
    "        syn_test_set = DynamicSyntheticDataset(num_samples=num_test_samples, activations=syn_activations, noise=synthetic_dataset_noise, local_seed=(seed + 1))\n",
    "        syn_testloader  = DataLoader(syn_test_set, batch_size=128, shuffle=False, drop_last=False)        \n",
    "                \n",
    "        # define the loss function and the optimiser\n",
    "        loss_function = nn.CrossEntropyLoss()\n",
    "        optimiser = optim.SGD(model_cut.parameters(), lr=1e-4, momentum=0.9, weight_decay=0.01)\n",
    "        \n",
    "        # Put top model into train mode so that bn and dropout perform in training mode\n",
    "        model_cut.train()\n",
    "        # Freeze the top model\n",
    "        model_cut.requires_grad_(False)\n",
    "        # Un-Freeze the stitch layer\n",
    "        for name, param in model_cut.stitch.named_parameters():\n",
    "            param.requires_grad_(True)\n",
    "        logtofile(f\"Train the stitch to a top model cut after layer {layer_to_cut_after}\")    \n",
    "        # the epoch loop: note that we're training the whole network\n",
    "        for epoch in range(stitch_train_epochs):\n",
    "            running_loss = 0.0\n",
    "            for data in syn_trainloader:  # Use synthetic training data\n",
    "                # data is (representations, labels) tuple\n",
    "                # get the inputs and put them on the GPU\n",
    "                inputs, labels = data\n",
    "                # logtofile(f\"{epoch=}: {labels=}\")\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                # raise ValueError(\"STOP HERE\")\n",
    "                # zero the parameter gradients\n",
    "                optimiser.zero_grad()\n",
    "        \n",
    "                # forward + loss + backward + optimise (update weights)\n",
    "                outputs = model_cut(inputs)\n",
    "                loss = loss_function(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimiser.step()\n",
    "        \n",
    "                # keep track of the loss this epoch\n",
    "                running_loss += loss.item()\n",
    "            logtofile(\"Epoch %d, loss %4.2f\" % (epoch, running_loss))\n",
    "        logtofile('**** Finished Training ****')\n",
    "        \n",
    "        model_cut.eval() # ALWAYS DO THIS BEFORE YOU EVALUATE MODELS\n",
    "\n",
    "        if save_stitch_delta:\n",
    "            ############################################################\n",
    "            # store the trained stitch\n",
    "            trained_stitch_weight = model_cut.stitch.s_conv1.weight.clone()\n",
    "            trained_stitch_bias   = model_cut.stitch.s_conv1.bias.clone()\n",
    "            stitch_trained_weight_outpath    = f\"./{results_root}/STITCH_trained_weight_{model_training_type}-{dataset_type}-{seed}_{filename.split('/')[-1]}\"  \n",
    "            stitch_trained_bias_outpath      = f\"./{results_root}/STITCH_trained_bias_{model_training_type}-{dataset_type}-{seed}_{filename.split('/')[-1]}\"  \n",
    "            torch.save(trained_stitch_weight, stitch_trained_weight_outpath)\n",
    "            torch.save(trained_stitch_bias, stitch_trained_bias_outpath)\n",
    "            \n",
    "            print(f\"Number of weight / bias in stitch layer is {len(initial_stitch_weight)}\")\n",
    "            stitch_weight_diff = trained_stitch_weight - initial_stitch_weight\n",
    "            stitch_weight_delta = torch.linalg.norm(stitch_weight_diff).item()\n",
    "            logtofile(f\"Change in stitch weights: {stitch_weight_delta}\")\n",
    "            maxabsweight =  torch.max(stitch_weight_diff.abs()).item()\n",
    "            logtofile(f\"Largest abs weight change: {maxabsweight}\")\n",
    "            stitch_weight_number = torch.sum(torch.where(stitch_weight_diff.abs() > 0.1*maxabsweight, True, False)).item()\n",
    "            logtofile(f\"Number of weights changing > 0.1 of that: {stitch_weight_number}\")\n",
    "            \n",
    "            stitch_bias_diff = trained_stitch_bias - initial_stitch_bias\n",
    "            stitch_bias_delta = torch.linalg.norm(stitch_bias_diff).item()\n",
    "            logtofile(f\"Change in stitch bias: {stitch_bias_delta}\")\n",
    "            maxabsbias =  torch.max(stitch_bias_diff.abs()).item()\n",
    "            logtofile(f\"Largest abs bias change: {maxabsbias}\")\n",
    "            stitch_bias_number = torch.sum(torch.where(stitch_bias_diff.abs() > 0.1*maxabsbias, True, False)).item()\n",
    "            logtofile(f\"Number of bias changing > 0.1 of that: {stitch_bias_number}\")\n",
    "\n",
    "        #new_tensor = torch.load(\"foo_1_state.pt\")\n",
    "        ##############################################################\n",
    "        \n",
    "        logtofile(\"Test the trained stitch\")        \n",
    "        # Compute the model accuracy on the test set\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        # assuming 10 classes\n",
    "        # rows represent actual class, columns are predicted\n",
    "        confusion_matrix = torch.zeros(num_classes,num_classes, dtype=torch.int)\n",
    "        \n",
    "        for data in syn_testloader:\n",
    "            inputs, labels = data\n",
    "            #print(inputs)   \n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            predictions = torch.argmax(model_cut(inputs),1)\n",
    "            #print(model_cut(inputs))\n",
    "            matches = predictions == labels.to(device)\n",
    "            correct += matches.sum().item()\n",
    "            total += len(labels)\n",
    "        \n",
    "            for idx, l in enumerate(labels):\n",
    "                confusion_matrix[l, predictions[idx]] = 1 + confusion_matrix[l, predictions[idx]] \n",
    "        acc = ((100.0 * correct) / total)\n",
    "        logtofile('Test Accuracy: %2.2f %%' % acc)\n",
    "        logtofile('Confusion Matrix')\n",
    "        logtofile(confusion_matrix)\n",
    "        logtofile(\"===================================================================\")\n",
    "        stitching_accuracies[key][layer_to_cut_after] = acc\n",
    "        #stitching_penalties[key][layer_to_cut_after] = original_accuracy[key] - acc        \n",
    "\n",
    "        dl = syn_testloader        \n",
    "        if measure_rank:\n",
    "            params = {}\n",
    "            params[\"model\"] = model_training_type\n",
    "            params[\"dataset\"] = dataset_type\n",
    "            params[\"seed\"] = seed\n",
    "\n",
    "            if save_stitch_delta:\n",
    "                params[\"stitch_weight_delta\"] = stitch_weight_delta\n",
    "                params[\"stitch_bias_delta\"] = stitch_bias_delta        \n",
    "                params[\"stitch_weight_number\"] = stitch_weight_number\n",
    "                params[\"stitch_bias_number\"] = stitch_bias_number\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                layers, features, handles = install_hooks(model_cut)\n",
    "                \n",
    "                metrics = evaluate_model(model_cut, dl, 'acc', verbose=2)\n",
    "                params.update(metrics)\n",
    "                classes = None\n",
    "                df = perform_analysis(features, classes, layers, params, n=-1)\n",
    "                df.to_csv(f\"{outpath}\")\n",
    "                \n",
    "            for h in handles:\n",
    "                h.remove()\n",
    "            del model_cut, layers, features, metrics, params, df, handles\n",
    "            gc.collect()\n",
    "        else:  # not measuring rank           \n",
    "            logtofile(f\"output to {outpath}\")\n",
    "                    \n",
    "            params = {}\n",
    "            # params[\"offset\"] = target_offset\n",
    "            params[\"model\"] = model_training_type\n",
    "            params[\"dataset\"] = dataset_type\n",
    "            params[\"seed\"] = seed\n",
    "            params[\"val_acc\"] = acc / 100\n",
    "            params[\"name\"] = \"only\"\n",
    "    \n",
    "            results = []\n",
    "            results.append(params)\n",
    "            df = pd.DataFrame.from_records(results)\n",
    "            df.to_csv(f\"{outpath}\")\n",
    "                        \n",
    "            del  params, df\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d78bc9-298a-4525-9f9f-759248f3c871",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    model_cut.eval() # ALWAYS DO THIS BEFORE YOU EVALUATE MODELS\n",
    "    \n",
    "    logtofile(\"Test the trained stitch\")        \n",
    "    # Compute the model accuracy on the test set\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # assuming 10 classes\n",
    "    # rows represent actual class, columns are predicted\n",
    "    confusion_matrix = torch.zeros(num_classes,num_classes, dtype=torch.int)\n",
    "    \n",
    "    for data in syn_testloader:\n",
    "        inputs, labels = data\n",
    "        #print(inputs)   \n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        predictions = torch.argmax(model_cut(inputs),1)\n",
    "        #print(model_cut(inputs))\n",
    "        matches = predictions == labels.to(device)\n",
    "        correct += matches.sum().item()\n",
    "        total += len(labels)\n",
    "    \n",
    "        for idx, l in enumerate(labels):\n",
    "            confusion_matrix[l, predictions[idx]] = 1 + confusion_matrix[l, predictions[idx]] \n",
    "    acc = ((100.0 * correct) / total)\n",
    "    logtofile('Test Accuracy: %2.2f %%' % acc)\n",
    "    logtofile('Confusion Matrix')\n",
    "    logtofile(confusion_matrix)\n",
    "    logtofile(\"===================================================================\")\n",
    "    stitching_accuracies[key][layer_to_cut_after] = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b388249-6105-4382-be3f-e8b5c9678479",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_stitch_delta:\n",
    "    stitch_weight_diff = trained_stitch_weight - initial_stitch_weight\n",
    "    stitch_weight_delta = torch.linalg.norm(stitch_weight_diff).item()\n",
    "    print(f\"Change in stitch weights: {stitch_weight_delta}\")\n",
    "    maxabsweight =  torch.max(stitch_weight_diff.abs()).item()\n",
    "    print(f\"Largest abs weight change: {maxabsweight}\")\n",
    "    stitch_weight_number = torch.sum(torch.where(stitch_weight_diff.abs() > 0.1*maxabsweight, True, False)).item()\n",
    "    print(f\"Number of weights changing > 0.1 of that: {stitch_weight_number}\")\n",
    "    \n",
    "    print(stitch_weight_delta)\n",
    "    print(stitch_weight_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db5808a-6351-4133-9e7c-85e8e9248cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "logtofile(f\"{stitching_accuracies=}\")\n",
    "#logtofile(f\"{stitching_penalties=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd669a19-7c42-4265-b8e8-57165995c097",
   "metadata": {},
   "outputs": [],
   "source": [
    "for r_key in stitching_accuracies:\n",
    "    logtofile(f\"synth-{r_key}\")\n",
    "    #logtofile(original_accuracy[r_key])\n",
    "    logtofile(\"Stitch Accuracy\")\n",
    "    for layer in stitching_accuracies[r_key]:\n",
    "        logtofile(stitching_accuracies[r_key][layer])\n",
    "    logtofile(\"--------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
