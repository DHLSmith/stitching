{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46e81ec0-e3c4-4060-97c1-bfccf47bc350",
   "metadata": {},
   "source": [
    "# To investigate increasing the noise radius in stitching synthetic data\n",
    "\n",
    "Load in ResNet18 models that I've trained on different types of dataset. Cut them at one layer and stitch in the random dataloader with different noise values.\n",
    "No rank measurements to save time. save the csv files of details\n",
    "## Only train for 4 epochs\n",
    "original models only train 4 epochs so they learn less accuracy. This is to allow our experiments better to determine change and improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51fc60f8-f6f6-469c-8705-c9015bd43951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "%matplotlib inline\n",
    "\n",
    "import argparse\n",
    "import gc\n",
    "import os.path\n",
    "\n",
    "import pandas as pd\n",
    "from torch.linalg import LinAlgError\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "import datetime\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "import os\n",
    "# add the path to find colour_mnist\n",
    "sys.path.append(os.path.abspath('../ReferenceCode'))\n",
    "import colour_mnist\n",
    "from stitch_utils import train_model, RcvResNet18, get_layer_output_shape\n",
    "from stitch_utils import generate_activations, SyntheticDataset\n",
    "import stitch_utils\n",
    "\n",
    "# add the path to find the rank analysis code\n",
    "# https://github.com/DHLSmith/jons-tunnel-effect/tree/NeurIPSPaper\n",
    "\n",
    "sys.path.append(os.path.abspath('../../jons-tunnel-effect/'))\n",
    "from utils.modelfitting import evaluate_model, set_seed\n",
    "from extract_weight_rank import install_hooks, perform_analysis\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "def logtofile(log_text, verbose=True):\n",
    "    if verbose:\n",
    "        print(log_text)\n",
    "    with open(save_log_as, \"a\") as f:    \n",
    "        print(log_text, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6761870b-2996-4763-8d90-76529ec5822e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Parameters\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 102\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "random.seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "train_all = False\n",
    "\n",
    "# MIX is 1/3 bgonly, 1/3 mnist only, 1/3 biased data\n",
    "train_mix_mnist_model = train_all\n",
    "mix_mnist_model_to_load = './results_4_epochs/2024-08-01_07-29-36_SEED101_EPOCHS4_BGN0.1_exp1e_ResNet18_mix_mnist.weights'\n",
    "\n",
    "# BW is greyscale mnist with no colour added\n",
    "train_bw_mnist_model = train_all  # when False, automatically loads a trained model\n",
    "bw_mnist_model_to_load ='./results_4_epochs/2024-08-01_11-00-22_SEED101_EPOCHS4_BGN0.1_exp1e_ResNet18_bw_mnist.weights'\n",
    "\n",
    "# BG_ONLY contains no mnist data, just a coloured background\n",
    "train_bg_only_colour_mnist_model = train_all # when False, automatically loads a trained model\n",
    "bg_only_colour_mnist_model_to_load =  './results_4_epochs/2024-08-01_11-00-22_SEED101_EPOCHS4_BGN0.1_exp1e_ResNet18_bg_only_colour_mnist.weights'\n",
    "\n",
    "# BG_UNBIASED is digits with randomly selected colour background. Targets represent the colour\n",
    "train_bg_unbiased_colour_mnist_model = train_all  # when False, automatically loads a trained model\n",
    "bg_unbiased_colour_mnist_model_to_load = './results_4_epochs/2024-08-01_11-00-22_SEED101_EPOCHS4_BGN0.1_exp1e_ResNet18_bg_unbiased_colour_mnist.weights'\n",
    "\n",
    "# BIASED is digits with consistent per-class colour background. \n",
    "train_biased_colour_mnist_model = train_all  # when False, automatically loads a trained model\n",
    "biased_colour_mnist_model_to_load = './results_4_epochs/2024-08-01_11-00-22_SEED101_EPOCHS4_BGN0.1_exp1e_ResNet18_biased_colour_mnist.weights'\n",
    "\n",
    "# UNBIASED is digits with randoly selected colour background. Targets are digit values\n",
    "train_unbiased_colour_mnist_model = train_all  # when False, automatically loads a trained model\n",
    "unbiased_colour_mnist_model_to_load = './results_4_epochs/2024-08-01_11-00-22_SEED101_EPOCHS4_BGN0.1_exp1e_ResNet18_unbiased_colour_mnist.weights'\n",
    "\n",
    "original_train_epochs = 4\n",
    "bg_noise = 0.1\n",
    "synthetic_dataset_noises = [0.5, 0.9, 0.99, 1.0]  # the set of noise radiuses we'll use to test\n",
    "\n",
    "stitch_train_epochs = 4\n",
    "device = 'cuda:2'\n",
    "\n",
    "\n",
    "measure_rank = False\n",
    "\n",
    "results_root = \"results_1n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20ee6e98-a6f2-4647-b378-5f7b1af48581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executed at 2025-01-21_10-12-57\n",
      "logging to ./results_1n/2025-01-21_10-12-57_SEED102_EPOCHS4_BGN0.1_exp1f_ResNet18_log.txt\n",
      "seed=102\n",
      "bg_noise=0.1\n",
      "synthetic_dataset_noises=[0.5, 0.9, 0.99, 1.0]\n",
      "measure_rank=False\n",
      "train_mix_mnist_model=False\n",
      "mix_mnist_model_to_load='./results_4_epochs/2024-08-01_07-29-36_SEED101_EPOCHS4_BGN0.1_exp1e_ResNet18_mix_mnist.weights'\n",
      "train_bw_mnist_model=False\n",
      "bw_mnist_model_to_load='./results_4_epochs/2024-08-01_11-00-22_SEED101_EPOCHS4_BGN0.1_exp1e_ResNet18_bw_mnist.weights'\n",
      "train_bg_only_colour_mnist_model=False\n",
      "bg_only_colour_mnist_model_to_load='./results_4_epochs/2024-08-01_11-00-22_SEED101_EPOCHS4_BGN0.1_exp1e_ResNet18_bg_only_colour_mnist.weights'\n",
      "train_bg_unbiased_colour_mnist_model=False\n",
      "bg_unbiased_colour_mnist_model_to_load='./results_4_epochs/2024-08-01_11-00-22_SEED101_EPOCHS4_BGN0.1_exp1e_ResNet18_bg_unbiased_colour_mnist.weights'\n",
      "train_biased_colour_mnist_model=False\n",
      "biased_colour_mnist_model_to_load='./results_4_epochs/2024-08-01_11-00-22_SEED101_EPOCHS4_BGN0.1_exp1e_ResNet18_biased_colour_mnist.weights'\n",
      "train_unbiased_colour_mnist_model=False\n",
      "unbiased_colour_mnist_model_to_load='./results_4_epochs/2024-08-01_11-00-22_SEED101_EPOCHS4_BGN0.1_exp1e_ResNet18_unbiased_colour_mnist.weights'\n",
      "stitch_train_epochs=4\n",
      "================================================\n"
     ]
    }
   ],
   "source": [
    "# Generate filenames and log the setup details\n",
    "formatted_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "filename_prefix = f\"./{results_root}/{formatted_time}_SEED{seed}_EPOCHS{original_train_epochs}_BGN{bg_noise}_exp1f_ResNet18\"\n",
    "save_mix_mnist_model_as = f\"{filename_prefix}_mix_mnist.weights\"\n",
    "save_bw_mnist_model_as = f\"{filename_prefix}_bw_mnist.weights\"\n",
    "save_bg_only_colour_mnist_model_as = f\"{filename_prefix}_bg_only_colour_mnist.weights\"\n",
    "save_bg_unbiased_colour_mnist_model_as = f\"{filename_prefix}_bg_unbiased_colour_mnist.weights\"\n",
    "save_biased_colour_mnist_model_as = f\"{filename_prefix}_biased_colour_mnist.weights\"\n",
    "save_unbiased_colour_mnist_model_as = f\"{filename_prefix}_unbiased_colour_mnist.weights\"\n",
    "save_log_as = f\"{filename_prefix}_log.txt\"\n",
    "\n",
    "colour_mnist_shape = (3,28,28)\n",
    "\n",
    "logtofile(f\"Executed at {formatted_time}\")\n",
    "logtofile(f\"logging to {save_log_as}\")\n",
    "logtofile(f\"{seed=}\")\n",
    "logtofile(f\"{bg_noise=}\")\n",
    "logtofile(f\"{synthetic_dataset_noises=}\")\n",
    "logtofile(f\"{measure_rank=}\")\n",
    "\n",
    "logtofile(f\"{train_mix_mnist_model=}\")\n",
    "if train_mix_mnist_model:\n",
    "    logtofile(f\"{save_mix_mnist_model_as=}\")\n",
    "    logtofile(f\"{original_train_epochs=}\")\n",
    "else:\n",
    "    logtofile(f\"{mix_mnist_model_to_load=}\")\n",
    "\n",
    "logtofile(f\"{train_bw_mnist_model=}\")\n",
    "if train_bw_mnist_model:\n",
    "    logtofile(f\"{save_bw_mnist_model_as=}\")\n",
    "    logtofile(f\"{original_train_epochs=}\")\n",
    "else:\n",
    "    logtofile(f\"{bw_mnist_model_to_load=}\")\n",
    "    \n",
    "logtofile(f\"{train_bg_only_colour_mnist_model=}\")\n",
    "if train_bg_only_colour_mnist_model:\n",
    "    logtofile(f\"{save_bg_only_colour_mnist_model_as=}\")\n",
    "    logtofile(f\"{original_train_epochs=}\")\n",
    "else:\n",
    "    logtofile(f\"{bg_only_colour_mnist_model_to_load=}\")\n",
    "    \n",
    "logtofile(f\"{train_bg_unbiased_colour_mnist_model=}\")\n",
    "if train_bg_unbiased_colour_mnist_model:\n",
    "    logtofile(f\"{save_bg_unbiased_colour_mnist_model_as=}\")\n",
    "    logtofile(f\"{original_train_epochs=}\")\n",
    "else:\n",
    "    logtofile(f\"{bg_unbiased_colour_mnist_model_to_load=}\")\n",
    "\n",
    "logtofile(f\"{train_biased_colour_mnist_model=}\")\n",
    "if train_biased_colour_mnist_model:\n",
    "    logtofile(f\"{save_biased_colour_mnist_model_as=}\")\n",
    "    logtofile(f\"{original_train_epochs=}\")\n",
    "else:\n",
    "    logtofile(f\"{biased_colour_mnist_model_to_load=}\")\n",
    "\n",
    "logtofile(f\"{train_unbiased_colour_mnist_model=}\")\n",
    "if train_unbiased_colour_mnist_model:\n",
    "    logtofile(f\"{save_unbiased_colour_mnist_model_as=}\")\n",
    "    logtofile(f\"{original_train_epochs=}\")\n",
    "else:\n",
    "    logtofile(f\"{unbiased_colour_mnist_model_to_load=}\")\n",
    "\n",
    "logtofile(f\"{stitch_train_epochs=}\")\n",
    "logtofile(f\"================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc621f6-55e8-4191-8288-dc2493cd6bff",
   "metadata": {},
   "source": [
    "mnist and cifar-10 both use 10-classes, with 60_000 train samples and 10_000 test samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d34a54d2-c8fa-4f51-8809-7a40b4fefc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up dataloaders\n",
    "transform_bw = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),  # Convert to 3 channels    \n",
    "    transforms.ToTensor(),  # convert to tensor. We always do this one    \n",
    "    transforms.Normalize((0.1307,) * 3, (0.3081,) * 3)     \n",
    "])\n",
    "\n",
    "mnist_train = MNIST(\"./MNIST\", train=True, download=True, transform=transform_bw)\n",
    "mnist_test = MNIST(\"./MNIST\", train=False, download=True, transform=transform_bw)\n",
    "\n",
    "bw_train_dataloader = DataLoader(mnist_train, batch_size=128, shuffle=True, drop_last=True)\n",
    "bw_test_dataloader  = DataLoader(mnist_test,  batch_size=128, shuffle=True, drop_last=False)\n",
    "\n",
    "# mix dataloader\n",
    "mix_train_dataloader = colour_mnist.get_mixed_mnist_dataloader(root=\"./MNIST\", batch_size=128, train=True, bg_noise_level=bg_noise, standard_getitem=True)\n",
    "mix_test_dataloader = colour_mnist.get_mixed_mnist_dataloader(root=\"./MNIST\", batch_size=128,  train=False, bg_noise_level=bg_noise, standard_getitem=True)\n",
    "\n",
    "# bg_only means no digits - we will use colour as label\n",
    "bg_only_train_dataloader = colour_mnist.get_biased_mnist_dataloader(root=\"./MNIST\", batch_size=128, data_label_correlation=1.0, train=True, bg_noise_level=bg_noise, bg_only=True, standard_getitem=True)\n",
    "bg_only_test_dataloader = colour_mnist.get_biased_mnist_dataloader(root=\"./MNIST\", batch_size=128, data_label_correlation=1.0, train=False, bg_noise_level=bg_noise, bg_only=True, standard_getitem=True)\n",
    "\n",
    "# unbiased means each digit has correct label and random colour - but bg means we will use colour as label (i.e. the bias_target will be the target)\n",
    "bg_unbiased_train_dataloader = colour_mnist.get_biased_mnist_dataloader(root=\"./MNIST\", batch_size=128, data_label_correlation=0.1, train=True, bg_noise_level=bg_noise, bias_targets_as_targets=True)\n",
    "bg_unbiased_test_dataloader = colour_mnist.get_biased_mnist_dataloader(root=\"./MNIST\", batch_size=128, data_label_correlation=0.1, train=False, bg_noise_level=bg_noise, bias_targets_as_targets=True)\n",
    "\n",
    "# biased means each digit has correct label and consistent colour - Expect network to learn the colours only\n",
    "biased_train_dataloader = colour_mnist.get_biased_mnist_dataloader(root=\"./MNIST\", batch_size=128, data_label_correlation=1.0, train=True, bg_noise_level=bg_noise, standard_getitem=True)\n",
    "biased_test_dataloader = colour_mnist.get_biased_mnist_dataloader(root=\"./MNIST\", batch_size=128, data_label_correlation=1.0, train=False, bg_noise_level=bg_noise, standard_getitem=True)\n",
    "\n",
    "# unbiased means each digit has correct label and random colour - Expect network to disregard colours?\n",
    "unbiased_train_dataloader = colour_mnist.get_biased_mnist_dataloader(root=\"./MNIST\", batch_size=128, data_label_correlation=0.1, train=True, bg_noise_level=bg_noise, standard_getitem=True)\n",
    "unbiased_test_dataloader = colour_mnist.get_biased_mnist_dataloader(root=\"./MNIST\", batch_size=128, data_label_correlation=0.1, train=False, bg_noise_level=bg_noise, standard_getitem=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24842d82-5f62-4d1b-acc5-d1997a08b0b9",
   "metadata": {},
   "source": [
    "## Set up resnet18 models and train it on versions of MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf635cfd-a9ad-4e37-98a0-80d0db2a3b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing for key='bw'\n",
      "val['loadfrom']='./results_4_epochs/2024-08-01_11-00-22_SEED101_EPOCHS4_BGN0.1_exp1e_ResNet18_bw_mnist.weights'\n",
      "Processing for key='bgonly'\n",
      "val['loadfrom']='./results_4_epochs/2024-08-01_11-00-22_SEED101_EPOCHS4_BGN0.1_exp1e_ResNet18_bg_only_colour_mnist.weights'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "process_structure = dict()\n",
    "\n",
    "# process_structure[\"mix\"] = dict()\n",
    "process_structure[\"bw\"] = dict()\n",
    "process_structure[\"bgonly\"] = dict()\n",
    "#process_structure[\"bg\"] = dict()\n",
    "#process_structure[\"bias\"]      = dict()\n",
    "#process_structure[\"unbias\"]    = dict()\n",
    "\n",
    "## \"mix\"\n",
    "#process_structure[\"mix\"][\"model\"] = torchvision.models.resnet18(num_classes=10).to(device) # Untrained model\n",
    "#process_structure[\"mix\"][\"train\"] = train_mix_mnist_model \n",
    "#process_structure[\"mix\"][\"train_loader\"] = mix_train_dataloader\n",
    "#process_structure[\"mix\"][\"test_loader\"] = mix_test_dataloader\n",
    "#process_structure[\"mix\"][\"saveas\"] = save_mix_mnist_model_as\n",
    "#process_structure[\"mix\"][\"loadfrom\"] = mix_mnist_model_to_load\n",
    "\n",
    "# \"bw\"\n",
    "process_structure[\"bw\"][\"model\"] = torchvision.models.resnet18(num_classes=10).to(device) # Untrained model\n",
    "process_structure[\"bw\"][\"train\"] = train_bw_mnist_model \n",
    "process_structure[\"bw\"][\"train_loader\"] = bw_train_dataloader\n",
    "process_structure[\"bw\"][\"test_loader\"] = bw_test_dataloader\n",
    "process_structure[\"bw\"][\"saveas\"] = save_bw_mnist_model_as\n",
    "process_structure[\"bw\"][\"loadfrom\"] = bw_mnist_model_to_load\n",
    "\n",
    "# \"bg_only_colour\"\n",
    "process_structure[\"bgonly\"][\"model\"] = torchvision.models.resnet18(num_classes=10).to(device) # Untrained model\n",
    "process_structure[\"bgonly\"][\"train\"] = train_bg_only_colour_mnist_model \n",
    "process_structure[\"bgonly\"][\"train_loader\"] = bg_only_train_dataloader\n",
    "process_structure[\"bgonly\"][\"test_loader\"] = bg_only_test_dataloader\n",
    "process_structure[\"bgonly\"][\"saveas\"] = save_bg_only_colour_mnist_model_as\n",
    "process_structure[\"bgonly\"][\"loadfrom\"] = bg_only_colour_mnist_model_to_load\n",
    "\n",
    "## \"bg_unbiased_colour\"\n",
    "#process_structure[\"bg\"][\"model\"] = torchvision.models.resnet18(num_classes=10).to(device) # Untrained model\n",
    "#process_structure[\"bg\"][\"train\"] = train_bg_unbiased_colour_mnist_model \n",
    "#process_structure[\"bg\"][\"train_loader\"] = bg_unbiased_train_dataloader\n",
    "#process_structure[\"bg\"][\"test_loader\"] = bg_unbiased_test_dataloader\n",
    "#process_structure[\"bg\"][\"saveas\"] = save_bg_unbiased_colour_mnist_model_as\n",
    "#process_structure[\"bg\"][\"loadfrom\"] = bg_unbiased_colour_mnist_model_to_load\n",
    "#\n",
    "## \"biased_colour_mnist\"\n",
    "#process_structure[\"bias\"][\"model\"] = torchvision.models.resnet18(num_classes=10).to(device) # Untrained model\n",
    "#process_structure[\"bias\"][\"train\"] = train_biased_colour_mnist_model\n",
    "#process_structure[\"bias\"][\"train_loader\"] = biased_train_dataloader\n",
    "#process_structure[\"bias\"][\"test_loader\"] = biased_test_dataloader\n",
    "#process_structure[\"bias\"][\"saveas\"] = save_biased_colour_mnist_model_as\n",
    "#process_structure[\"bias\"][\"loadfrom\"] =  biased_colour_mnist_model_to_load\n",
    "#\n",
    "## \"unbiased_colour_mnist\"\n",
    "#process_structure[\"unbias\"][\"model\"] = torchvision.models.resnet18(num_classes=10).to(device) # Untrained model\n",
    "#process_structure[\"unbias\"][\"train\"] = train_unbiased_colour_mnist_model\n",
    "#process_structure[\"unbias\"][\"train_loader\"] = unbiased_train_dataloader\n",
    "#process_structure[\"unbias\"][\"test_loader\"] = unbiased_test_dataloader\n",
    "#process_structure[\"unbias\"][\"saveas\"] = save_unbiased_colour_mnist_model_as\n",
    "#process_structure[\"unbias\"][\"loadfrom\"] =  unbiased_colour_mnist_model_to_load\n",
    "\n",
    "\n",
    "\n",
    "for key, val in process_structure.items():\n",
    "    print(f\"Processing for {key=}\")\n",
    "    if val[\"train\"]:\n",
    "        train_model(model=val[\"model\"], train_loader=val[\"train_loader\"], \n",
    "                    epochs=original_train_epochs, saveas=val[\"saveas\"], \n",
    "                    description=key, device=device, logtofile=logtofile)\n",
    "    else:\n",
    "        logtofile(f\"{val['loadfrom']=}\")\n",
    "        val[\"model\"].load_state_dict(torch.load(val[\"loadfrom\"], map_location=torch.device(device)))\n",
    "    val[\"model\"].eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b169c5c7-7929-48e1-82eb-6f047aa4e5f2",
   "metadata": {},
   "source": [
    "## Measure the Accuracy, Record the Confusion Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fbb925a-269d-4027-9500-6cdce4de9d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Calculation for ResNet18 with key='bw'\n",
      "Test the Trained Resnet18\n",
      "Test Accuracy: 98.26 %\n",
      "Confusion Matrix\n",
      "tensor([[ 976,    0,    2,    0,    0,    1,    0,    1,    0,    0],\n",
      "        [   1, 1132,    0,    1,    0,    0,    0,    0,    1,    0],\n",
      "        [   6,    4, 1006,   10,    1,    0,    0,    4,    1,    0],\n",
      "        [   0,    1,    1,  994,    0,    9,    0,    1,    4,    0],\n",
      "        [   0,    0,    2,    0,  959,    0,    1,    0,    2,   18],\n",
      "        [   1,    0,    0,    5,    0,  882,    1,    1,    1,    1],\n",
      "        [   7,    4,    0,    1,    1,   11,  933,    0,    1,    0],\n",
      "        [   0,    5,    5,    1,    1,    0,    0, 1001,    2,   13],\n",
      "        [   5,    0,    0,    3,    2,    2,    2,    2,  954,    4],\n",
      "        [   4,    1,    0,    2,    3,    4,    0,    2,    4,  989]],\n",
      "       dtype=torch.int32)\n",
      "tensor(10000)\n",
      "output to ./results_1n_rank/bw-bw-102_2024-08-01_11-00-22_SEED101_EPOCHS4_BGN0.1_exp1e_ResNet18_bw_mnist-test.csv\n",
      "Accuracy Calculation for ResNet18 with key='bgonly'\n",
      "Test the Trained Resnet18\n",
      "Test Accuracy: 100.00 %\n",
      "Confusion Matrix\n",
      "tensor([[ 980,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   0, 1135,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   0,    0, 1032,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   0,    0,    0, 1010,    0,    0,    0,    0,    0,    0],\n",
      "        [   0,    0,    0,    0,  982,    0,    0,    0,    0,    0],\n",
      "        [   0,    0,    0,    0,    0,  892,    0,    0,    0,    0],\n",
      "        [   0,    0,    0,    0,    0,    0,  958,    0,    0,    0],\n",
      "        [   0,    0,    0,    0,    0,    0,    0, 1028,    0,    0],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,  974,    0],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0, 1009]],\n",
      "       dtype=torch.int32)\n",
      "tensor(10000)\n",
      "output to ./results_1n_rank/bgonly-bgonly-102_2024-08-01_11-00-22_SEED101_EPOCHS4_BGN0.1_exp1e_ResNet18_bg_only_colour_mnist-test.csv\n",
      "original_accuracy={'bw': 98.26, 'bgonly': 100.0}\n"
     ]
    }
   ],
   "source": [
    "original_accuracy = dict()\n",
    "for key, val in process_structure.items():\n",
    "    logtofile(f\"Accuracy Calculation for ResNet18 with {key=}\")\n",
    "    model = val[\"model\"]\n",
    "    model.eval() # ALWAYS DO THIS BEFORE YOU EVALUATE MODELS\n",
    "    \n",
    "    # Compute the model accuracy on the test set\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # assuming 10 classes\n",
    "    # rows represent actual class, columns are predicted\n",
    "    confusion_matrix = torch.zeros(10,10, dtype=torch.int)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    for data in val[\"test_loader\"]:\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        predictions = torch.argmax(model(inputs),1)\n",
    "        \n",
    "        matches = predictions == labels\n",
    "        correct += matches.sum().item()\n",
    "        total += len(labels)\n",
    "        for idx, l in enumerate(labels):\n",
    "            confusion_matrix[l, predictions[idx]] = 1 + confusion_matrix[l, predictions[idx]] \n",
    "    \n",
    "    logtofile(\"Test the Trained Resnet18\")\n",
    "    acc = ((100.0 * correct) / total)\n",
    "    logtofile('Test Accuracy: %2.2f %%' % acc)\n",
    "    original_accuracy[key] = acc\n",
    "    logtofile('Confusion Matrix')\n",
    "    logtofile(confusion_matrix)\n",
    "    logtofile(confusion_matrix.sum())\n",
    "\n",
    "    if not measure_rank:\n",
    "        \n",
    "        if val[\"train\"]:\n",
    "            filename = val[\"saveas\"] \n",
    "        else:    \n",
    "            filename = val[\"loadfrom\"] \n",
    "        assert os.path.exists(filename)\n",
    "    \n",
    "        out_filename = filename.split('/')[-1].replace('.weights', '-test.csv')\n",
    "        \n",
    "        outpath = f\"./{results_root}_rank/{key}-{key}-{seed}_{out_filename}\"  # denote output name as <model_training_type>-dataset-<name>\n",
    "        \n",
    "        if os.path.exists(f\"{outpath}\"):\n",
    "            logtofile(f\"Already evaluated for {outpath}\")\n",
    "            continue\n",
    "        #logtofile(f\"Measure Rank for {key=}\")\n",
    "        logtofile(f\"output to {outpath}\")\n",
    "                \n",
    "        params = {}\n",
    "        params[\"model\"] = key\n",
    "        params[\"dataset\"] = key\n",
    "        params[\"seed\"] = seed\n",
    "        if val[\"train\"]: # as only one network used, record its filename as both send and receive files\n",
    "            params[\"send_file\"] = val[\"saveas\"] \n",
    "            params[\"rcv_file\"] = val[\"saveas\"] \n",
    "        else:    \n",
    "            params[\"send_file\"] = val[\"loadfrom\"] \n",
    "            params[\"rcv_file\"] = val[\"loadfrom\"]\n",
    "        params[\"val_acc\"] = acc / 100\n",
    "        params[\"name\"] = \"only\"\n",
    "\n",
    "        results = []\n",
    "        results.append(params)\n",
    "        df = pd.DataFrame.from_records(results)\n",
    "        df.to_csv(f\"{outpath}\")\n",
    "                    \n",
    "        del  params, df\n",
    "        gc.collect()\n",
    "\n",
    "logtofile(f\"{original_accuracy=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da373b34-fe35-4256-a9e0-1040f699d45d",
   "metadata": {},
   "source": [
    "## Measure Rank with original dataloader (test) before cutting and stitching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db170b3d-ee28-48de-98ae-2f12e0b4cc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "num_layers_in_model = len(list(model.children()))\n",
    "\n",
    "# Specify the layer name you're interested in\n",
    "\n",
    "#print(model.training)\n",
    "#layer_index = 3\n",
    "#output_shape = get_layer_output_shape(model, layer_index, input_image_shape)\n",
    "#print(f\"The shape of the output from layer {layer_index} is: {output_shape}\")\n",
    "#print(f\"Output Size is : {output_shape.numel()}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4f74591-2a3f-4521-8aec-32c324125a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if measure_rank:\n",
    "    # For the Whole Model - but we will pass it through the RcvResNet18 function to get matching feature names\n",
    "    for key, val in process_structure.items():\n",
    "        \n",
    "        dl = val[\"test_loader\"]\n",
    "        \n",
    "        if val[\"train\"]:\n",
    "            filename = val[\"saveas\"] \n",
    "        else:    \n",
    "            filename = val[\"loadfrom\"] \n",
    "        assert os.path.exists(filename)\n",
    "        mdl = torchvision.models.resnet18(num_classes=10) # Untrained model\n",
    "        state = torch.load(filename, map_location=torch.device(\"cpu\"))\n",
    "        mdl.load_state_dict(state, assign=True)\n",
    "        mdl=mdl.to(device)\n",
    "        mdl = RcvResNet18(mdl, -1, colour_mnist_shape, device).to(device)\n",
    "    \n",
    "        out_filename = filename.split('/')[-1].replace('.weights', '-test.csv')\n",
    "        \n",
    "        outpath = f\"./{results_root}_rank/{key}-{key}-{seed}_{out_filename}\"  # denote output name as <model_training_type>-dataset-<name>\n",
    "        \n",
    "        if os.path.exists(f\"{outpath}\"):\n",
    "            logtofile(f\"Already evaluated for {outpath}\")\n",
    "            continue\n",
    "        logtofile(f\"Measure Rank for {key=}\")\n",
    "        print(f\"output to {outpath}\")\n",
    "                \n",
    "        params = {}\n",
    "        params[\"model\"] = key\n",
    "        params[\"dataset\"] = key\n",
    "        params[\"seed\"] = seed\n",
    "        if val[\"train\"]: # as only one network used, record its filename as both send and receive files\n",
    "            params[\"send_file\"] = val[\"saveas\"] \n",
    "            params[\"rcv_file\"] = val[\"saveas\"] \n",
    "        else:    \n",
    "            params[\"send_file\"] = val[\"loadfrom\"] \n",
    "            params[\"rcv_file\"] = val[\"loadfrom\"]\n",
    "        with torch.no_grad():\n",
    "            layers, features, handles = install_hooks(mdl)\n",
    "            \n",
    "            metrics = evaluate_model(mdl, dl, 'acc', verbose=2)\n",
    "            params.update(metrics)\n",
    "            classes = None\n",
    "            df = perform_analysis(features, classes, layers, params, n=-1)\n",
    "            df.to_csv(f\"{outpath}\")\n",
    "                    \n",
    "        for h in handles:\n",
    "            h.remove()\n",
    "        del mdl, layers, features, metrics, params, df, handles\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbaf773-ed43-4d91-b0a0-a35fd468ac02",
   "metadata": {},
   "source": [
    "## Train the stitch layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c29d808-b86e-4a0c-a3c5-127c952f3ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_layer_output_shape for type='ResNet18'\n",
      "key='bw' val={'model': ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
      "), 'train': False, 'train_loader': <torch.utils.data.dataloader.DataLoader object at 0x7f305cdecdc0>, 'test_loader': <torch.utils.data.dataloader.DataLoader object at 0x7f305cdeccd0>, 'saveas': './results_1n/2025-01-21_10-12-57_SEED102_EPOCHS4_BGN0.1_exp1f_ResNet18_bw_mnist.weights', 'loadfrom': './results_4_epochs/2024-08-01_11-00-22_SEED101_EPOCHS4_BGN0.1_exp1e_ResNet18_bw_mnist.weights'}\n",
      "after layer 5, activations shape is torch.Size([10, 128, 4, 4])\n",
      "Testing stitch with synthetic noise radius synthetic_dataset_noise=0.5\n",
      "get_layer_output_shape for type='ResNet18'\n",
      "The shape of the output from layer 5 is: torch.Size([1, 128, 4, 4]), with 2048 elements\n",
      "Evaluate ranks and output to ./results_1n_rank/X5bw-SYN_NOISE0.5-synth-102_2024-08-01_11-00-22_SEED101_EPOCHS4_BGN0.1_exp1e_ResNet18_bw_mnist-test.csv\n",
      "stitch into model bw\n",
      "Train the stitch to a top model cut after layer 5\n",
      "Epoch 0, loss 644.15\n",
      "Epoch 1, loss 53.74\n",
      "Epoch 2, loss 29.81\n",
      "Epoch 3, loss 22.05\n",
      "**** Finished Training ****\n",
      "Test the trained stitch\n",
      "Test Accuracy: 99.74 %\n",
      "Confusion Matrix\n",
      "tensor([[1000,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   1,  999,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   0,    0, 1000,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   0,    0,    0,  998,    0,    1,    0,    0,    0,    1],\n",
      "        [   1,    1,    0,    1,  993,    0,    0,    1,    1,    2],\n",
      "        [   1,    0,    0,    0,    0,  999,    0,    0,    0,    0],\n",
      "        [   1,    0,    0,    0,    0,    2,  996,    0,    1,    0],\n",
      "        [   0,    3,    0,    0,    0,    1,    0,  994,    0,    2],\n",
      "        [   0,    0,    0,    0,    0,    3,    0,    0,  997,    0],\n",
      "        [   0,    0,    0,    0,    0,    2,    0,    0,    0,  998]],\n",
      "       dtype=torch.int32)\n",
      "===================================================================\n",
      "output to ./results_1n_rank/X5bw-SYN_NOISE0.5-synth-102_2024-08-01_11-00-22_SEED101_EPOCHS4_BGN0.1_exp1e_ResNet18_bw_mnist-test.csv\n",
      "Testing stitch with synthetic noise radius synthetic_dataset_noise=0.9\n",
      "get_layer_output_shape for type='ResNet18'\n",
      "The shape of the output from layer 5 is: torch.Size([1, 128, 4, 4]), with 2048 elements\n",
      "Evaluate ranks and output to ./results_1n_rank/X5bw-SYN_NOISE0.9-synth-102_2024-08-01_11-00-22_SEED101_EPOCHS4_BGN0.1_exp1e_ResNet18_bw_mnist-test.csv\n",
      "stitch into model bw\n",
      "Train the stitch to a top model cut after layer 5\n",
      "Epoch 0, loss 1690.49\n",
      "Epoch 1, loss 273.66\n",
      "Epoch 2, loss 164.24\n",
      "Epoch 3, loss 109.09\n",
      "**** Finished Training ****\n",
      "Test the trained stitch\n",
      "Test Accuracy: 97.29 %\n",
      "Confusion Matrix\n",
      "tensor([[990,   0,   4,   1,   0,   4,   0,   0,   0,   1],\n",
      "        [  2, 983,   1,   0,   1,   7,   0,   1,   4,   1],\n",
      "        [ 13,   8, 960,   7,   3,   0,   4,   1,   4,   0],\n",
      "        [  1,   1,   4, 969,   0,  11,   0,   3,   8,   3],\n",
      "        [  0,   0,   3,   4, 962,   3,   0,   4,   2,  22],\n",
      "        [  1,   6,   0,   9,   0, 978,   0,   1,   2,   3],\n",
      "        [ 17,   1,   0,   0,   0,  20, 955,   0,   7,   0],\n",
      "        [  3,   6,   2,   1,  10,  11,   0, 963,   0,   4],\n",
      "        [  1,   2,   1,   4,   0,   5,   0,   1, 981,   5],\n",
      "        [  0,   0,   0,   2,   3,   4,   0,   1,   2, 988]], dtype=torch.int32)\n",
      "===================================================================\n",
      "output to ./results_1n_rank/X5bw-SYN_NOISE0.9-synth-102_2024-08-01_11-00-22_SEED101_EPOCHS4_BGN0.1_exp1e_ResNet18_bw_mnist-test.csv\n",
      "Testing stitch with synthetic noise radius synthetic_dataset_noise=0.99\n",
      "get_layer_output_shape for type='ResNet18'\n",
      "The shape of the output from layer 5 is: torch.Size([1, 128, 4, 4]), with 2048 elements\n",
      "Evaluate ranks and output to ./results_1n_rank/X5bw-SYN_NOISE0.99-synth-102_2024-08-01_11-00-22_SEED101_EPOCHS4_BGN0.1_exp1e_ResNet18_bw_mnist-test.csv\n",
      "stitch into model bw\n",
      "Train the stitch to a top model cut after layer 5\n",
      "Epoch 0, loss 2871.19\n",
      "Epoch 1, loss 356.33\n",
      "Epoch 2, loss 202.82\n",
      "Epoch 3, loss 143.17\n",
      "**** Finished Training ****\n",
      "Test the trained stitch\n",
      "Test Accuracy: 96.41 %\n",
      "Confusion Matrix\n",
      "tensor([[984,   0,   6,   2,   0,   3,   2,   0,   3,   0],\n",
      "        [ 11, 967,   3,   1,   3,  11,   0,   2,   2,   0],\n",
      "        [ 16,   6, 960,   9,   1,   1,   4,   0,   3,   0],\n",
      "        [  0,   1,   3, 956,   0,  26,   0,   4,   7,   3],\n",
      "        [  0,   9,   0,   2, 963,   3,   0,   3,   5,  15],\n",
      "        [  5,   1,   0,   3,   0, 981,   3,   1,   1,   5],\n",
      "        [ 19,   1,   2,   0,   1,  20, 957,   0,   0,   0],\n",
      "        [ 12,  11,   4,   3,   2,  12,   0, 949,   1,   6],\n",
      "        [  3,   1,   1,   4,   0,  13,   8,   3, 961,   6],\n",
      "        [  2,   0,   0,   6,   5,  17,   0,   6,   1, 963]], dtype=torch.int32)\n",
      "===================================================================\n",
      "output to ./results_1n_rank/X5bw-SYN_NOISE0.99-synth-102_2024-08-01_11-00-22_SEED101_EPOCHS4_BGN0.1_exp1e_ResNet18_bw_mnist-test.csv\n",
      "Testing stitch with synthetic noise radius synthetic_dataset_noise=1.0\n",
      "get_layer_output_shape for type='ResNet18'\n",
      "The shape of the output from layer 5 is: torch.Size([1, 128, 4, 4]), with 2048 elements\n",
      "Evaluate ranks and output to ./results_1n_rank/X5bw-SYN_NOISE1.0-synth-102_2024-08-01_11-00-22_SEED101_EPOCHS4_BGN0.1_exp1e_ResNet18_bw_mnist-test.csv\n",
      "stitch into model bw\n",
      "Train the stitch to a top model cut after layer 5\n",
      "Epoch 0, loss 3590.87\n",
      "Epoch 1, loss 379.38\n",
      "Epoch 2, loss 210.42\n",
      "Epoch 3, loss 155.31\n",
      "**** Finished Training ****\n",
      "Test the trained stitch\n",
      "Test Accuracy: 95.96 %\n",
      "Confusion Matrix\n",
      "tensor([[993,   0,   1,   0,   0,   3,   0,   0,   2,   1],\n",
      "        [  7, 959,   5,   1,   1,  15,   1,   5,   4,   2],\n",
      "        [ 20,   2, 952,   9,   5,   0,   5,   1,   6,   0],\n",
      "        [  1,   4,   5, 958,   1,  20,   0,   2,   6,   3],\n",
      "        [  2,   6,   3,   5, 954,   3,   1,   8,   6,  12],\n",
      "        [  1,   2,   1,  13,   0, 977,   4,   0,   1,   1],\n",
      "        [ 22,   3,   0,   0,   0,  29, 937,   0,   9,   0],\n",
      "        [  8,   9,   7,   4,   5,  11,   0, 946,   2,   8],\n",
      "        [  4,   1,   0,   3,   0,  10,   7,   1, 961,  13],\n",
      "        [  6,   1,   2,   1,   5,  15,   0,   3,   8, 959]], dtype=torch.int32)\n",
      "===================================================================\n",
      "output to ./results_1n_rank/X5bw-SYN_NOISE1.0-synth-102_2024-08-01_11-00-22_SEED101_EPOCHS4_BGN0.1_exp1e_ResNet18_bw_mnist-test.csv\n",
      "get_layer_output_shape for type='ResNet18'\n",
      "key='bgonly' val={'model': ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
      "), 'train': False, 'train_loader': <torch.utils.data.dataloader.DataLoader object at 0x7f3146828970>, 'test_loader': <torch.utils.data.dataloader.DataLoader object at 0x7f31468287c0>, 'saveas': './results_1n/2025-01-21_10-12-57_SEED102_EPOCHS4_BGN0.1_exp1f_ResNet18_bg_only_colour_mnist.weights', 'loadfrom': './results_4_epochs/2024-08-01_11-00-22_SEED101_EPOCHS4_BGN0.1_exp1e_ResNet18_bg_only_colour_mnist.weights'}\n",
      "after layer 5, activations shape is torch.Size([10, 128, 4, 4])\n",
      "Testing stitch with synthetic noise radius synthetic_dataset_noise=0.5\n",
      "get_layer_output_shape for type='ResNet18'\n",
      "The shape of the output from layer 5 is: torch.Size([1, 128, 4, 4]), with 2048 elements\n",
      "Evaluate ranks and output to ./results_1n_rank/X5bgonly-SYN_NOISE0.5-synth-102_2024-08-01_11-00-22_SEED101_EPOCHS4_BGN0.1_exp1e_ResNet18_bg_only_colour_mnist-test.csv\n",
      "stitch into model bgonly\n",
      "Train the stitch to a top model cut after layer 5\n",
      "Epoch 0, loss 874.85\n",
      "Epoch 1, loss 119.95\n",
      "Epoch 2, loss 31.76\n",
      "Epoch 3, loss 37.47\n",
      "**** Finished Training ****\n",
      "Test the trained stitch\n",
      "Test Accuracy: 99.83 %\n",
      "Confusion Matrix\n",
      "tensor([[ 996,    0,    0,    0,    0,    0,    1,    0,    3,    0],\n",
      "        [   0, 1000,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   0,    0, 1000,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   0,    0,    0,  999,    0,    0,    1,    0,    0,    0],\n",
      "        [   0,    0,    0,    0,  997,    0,    0,    1,    2,    0],\n",
      "        [   0,    0,    0,    0,    2,  998,    0,    0,    0,    0],\n",
      "        [   0,    0,    0,    0,    0,    0, 1000,    0,    0,    0],\n",
      "        [   0,    0,    0,    0,    1,    0,    0,  999,    0,    0],\n",
      "        [   0,    0,    0,    1,    3,    0,    0,    0,  996,    0],\n",
      "        [   0,    0,    0,    1,    0,    0,    0,    0,    1,  998]],\n",
      "       dtype=torch.int32)\n",
      "===================================================================\n",
      "output to ./results_1n_rank/X5bgonly-SYN_NOISE0.5-synth-102_2024-08-01_11-00-22_SEED101_EPOCHS4_BGN0.1_exp1e_ResNet18_bg_only_colour_mnist-test.csv\n",
      "Testing stitch with synthetic noise radius synthetic_dataset_noise=0.9\n",
      "get_layer_output_shape for type='ResNet18'\n",
      "The shape of the output from layer 5 is: torch.Size([1, 128, 4, 4]), with 2048 elements\n",
      "Evaluate ranks and output to ./results_1n_rank/X5bgonly-SYN_NOISE0.9-synth-102_2024-08-01_11-00-22_SEED101_EPOCHS4_BGN0.1_exp1e_ResNet18_bg_only_colour_mnist-test.csv\n",
      "stitch into model bgonly\n",
      "Train the stitch to a top model cut after layer 5\n",
      "Epoch 0, loss 2248.05\n",
      "Epoch 1, loss 312.18\n",
      "Epoch 2, loss 209.47\n",
      "Epoch 3, loss 153.93\n",
      "**** Finished Training ****\n",
      "Test the trained stitch\n",
      "Test Accuracy: 97.75 %\n",
      "Confusion Matrix\n",
      "tensor([[967,   0,   2,   7,   3,   0,   4,  13,   4,   0],\n",
      "        [  1, 999,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  1,   0, 987,   1,   2,   6,   0,   0,   3,   0],\n",
      "        [  1,   1,   0, 988,   5,   1,   1,   0,   3,   0],\n",
      "        [  0,   0,   3,   5, 952,   2,   3,  11,  23,   1],\n",
      "        [  0,   1,   1,   0,   2, 995,   1,   0,   0,   0],\n",
      "        [  2,   1,   0,   5,   6,   2, 972,   3,   3,   6],\n",
      "        [  0,   0,   0,   1,   3,   1,   2, 993,   0,   0],\n",
      "        [  2,   0,   7,  12,  25,   0,   8,   0, 943,   3],\n",
      "        [  0,   0,   3,   3,   2,   0,   7,   1,   5, 979]], dtype=torch.int32)\n",
      "===================================================================\n",
      "output to ./results_1n_rank/X5bgonly-SYN_NOISE0.9-synth-102_2024-08-01_11-00-22_SEED101_EPOCHS4_BGN0.1_exp1e_ResNet18_bg_only_colour_mnist-test.csv\n",
      "Testing stitch with synthetic noise radius synthetic_dataset_noise=0.99\n",
      "get_layer_output_shape for type='ResNet18'\n",
      "The shape of the output from layer 5 is: torch.Size([1, 128, 4, 4]), with 2048 elements\n",
      "Evaluate ranks and output to ./results_1n_rank/X5bgonly-SYN_NOISE0.99-synth-102_2024-08-01_11-00-22_SEED101_EPOCHS4_BGN0.1_exp1e_ResNet18_bg_only_colour_mnist-test.csv\n",
      "stitch into model bgonly\n",
      "Train the stitch to a top model cut after layer 5\n",
      "Epoch 0, loss 2793.20\n",
      "Epoch 1, loss 459.48\n",
      "Epoch 2, loss 319.69\n",
      "Epoch 3, loss 249.11\n",
      "**** Finished Training ****\n",
      "Test the trained stitch\n",
      "Test Accuracy: 96.31 %\n",
      "Confusion Matrix\n",
      "tensor([[ 941,    0,    7,   11,    3,    7,    5,    9,   17,    0],\n",
      "        [   0, 1000,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   0,    0,  983,    1,    0,    6,    0,    0,    6,    4],\n",
      "        [   3,    0,    1,  963,    7,    0,    5,    0,   20,    1],\n",
      "        [   0,    0,   10,   20,  913,    5,    7,   10,   33,    2],\n",
      "        [   0,    0,    1,    0,    0,  999,    0,    0,    0,    0],\n",
      "        [   6,    1,    0,    5,   10,    3,  966,    1,    3,    5],\n",
      "        [   5,    0,    1,    0,    5,    2,    3,  984,    0,    0],\n",
      "        [   1,    4,   16,   17,   30,    0,    9,    0,  920,    3],\n",
      "        [   1,    0,    1,    7,    2,    3,   16,    1,    7,  962]],\n",
      "       dtype=torch.int32)\n",
      "===================================================================\n",
      "output to ./results_1n_rank/X5bgonly-SYN_NOISE0.99-synth-102_2024-08-01_11-00-22_SEED101_EPOCHS4_BGN0.1_exp1e_ResNet18_bg_only_colour_mnist-test.csv\n",
      "Testing stitch with synthetic noise radius synthetic_dataset_noise=1.0\n",
      "get_layer_output_shape for type='ResNet18'\n",
      "The shape of the output from layer 5 is: torch.Size([1, 128, 4, 4]), with 2048 elements\n",
      "Evaluate ranks and output to ./results_1n_rank/X5bgonly-SYN_NOISE1.0-synth-102_2024-08-01_11-00-22_SEED101_EPOCHS4_BGN0.1_exp1e_ResNet18_bg_only_colour_mnist-test.csv\n",
      "stitch into model bgonly\n",
      "Train the stitch to a top model cut after layer 5\n",
      "Epoch 0, loss 3346.32\n",
      "Epoch 1, loss 442.85\n",
      "Epoch 2, loss 281.83\n",
      "Epoch 3, loss 194.84\n",
      "**** Finished Training ****\n",
      "Test the trained stitch\n",
      "Test Accuracy: 96.63 %\n",
      "Confusion Matrix\n",
      "tensor([[ 950,    1,    8,    9,    2,    2,    8,    6,   14,    0],\n",
      "        [   0, 1000,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   0,    0,  983,    3,    0,    5,    0,    1,    7,    1],\n",
      "        [   2,    0,    0,  974,    7,    1,    2,    1,   12,    1],\n",
      "        [   0,    0,    0,   18,  929,    4,   10,   12,   24,    3],\n",
      "        [   0,    0,    3,    0,    1,  994,    1,    0,    0,    1],\n",
      "        [   3,    1,    2,    8,    8,    2,  948,    2,   16,   10],\n",
      "        [   4,    0,    0,    1,    3,    1,    2,  985,    3,    1],\n",
      "        [   2,    1,    9,   19,   17,    0,    4,    1,  943,    4],\n",
      "        [   0,    1,    3,    7,    1,    9,   15,    0,    7,  957]],\n",
      "       dtype=torch.int32)\n",
      "===================================================================\n",
      "output to ./results_1n_rank/X5bgonly-SYN_NOISE1.0-synth-102_2024-08-01_11-00-22_SEED101_EPOCHS4_BGN0.1_exp1e_ResNet18_bg_only_colour_mnist-test.csv\n"
     ]
    }
   ],
   "source": [
    "stitching_accuracies = dict()\n",
    "stitching_penalties = dict()\n",
    "\n",
    "for key, val in process_structure.items():        \n",
    "    stitching_accuracies[key] = dict()\n",
    "    stitching_penalties[key] = dict()\n",
    "    for layer_to_cut_after in [5]: # range(3,num_layers_in_model - 1):\n",
    "        ###################### Don't bother to stitch and train if we've already analysed it\n",
    "\n",
    "        if val[\"train\"]:\n",
    "            filename = val[\"saveas\"] \n",
    "        else:    \n",
    "            filename = val[\"loadfrom\"] \n",
    "        model = torchvision.models.resnet18(num_classes=10).to(device)\n",
    "        model.load_state_dict(torch.load(filename, map_location=torch.device(device)))  # uses either the load/save name depending whether it'\n",
    "        cut_layer_output_size = get_layer_output_shape(model, layer_to_cut_after, colour_mnist_shape, device)\n",
    "        \n",
    "        representation_shape=(cut_layer_output_size[1:])\n",
    "        # Just do the activation generation once per model so that the only varible iis the noise volume\n",
    "        syn_activations = generate_activations(num_classes=10, representation_shape=representation_shape)\n",
    "        logtofile(f\"{key=} {val=}\")\n",
    "        logtofile(f\"after layer {layer_to_cut_after}, activations shape is {syn_activations.shape}\")\n",
    "        \n",
    "        for synthetic_dataset_noise in synthetic_dataset_noises:\n",
    "            logtofile(f\"Testing stitch with synthetic noise radius {synthetic_dataset_noise=}\")\n",
    "            # must regenerate model_cut each time to ensure the stitch is re-initialised\n",
    "            model_cut = RcvResNet18(model, layer_to_cut_after, colour_mnist_shape, device).to(device)\n",
    "            rank_filename = filename.split('/')[-1].replace('.weights', '-test.csv')        \n",
    "            # denote output name as <model_training_type>-dataset-<name>\n",
    "            # where <model_training_type> is [sender_model or X][layer_to_cut_after][Receiver_model]\n",
    "            model_training_type = f\"X{layer_to_cut_after}{key}\"\n",
    "            dataset_type = \"synth\"\n",
    "                \n",
    "            outpath = f\"./{results_root}_rank/{model_training_type}-SYN_NOISE{synthetic_dataset_noise}-{dataset_type}-{seed}_{rank_filename}\"  \n",
    "                            \n",
    "            if os.path.exists(f\"{outpath}\"):\n",
    "                logtofile(f\"Already evaluated for {outpath}\")\n",
    "                continue\n",
    "            ####################################################################################\n",
    "            logtofile(f\"Evaluate ranks and output to {outpath}\")\n",
    "            logtofile(f\"stitch into model {key}\")\n",
    "                                    \n",
    "            syn_train_set  = SyntheticDataset(train=True, activations=syn_activations, noise=synthetic_dataset_noise)\n",
    "            syn_trainloader  = DataLoader(syn_train_set, batch_size=64, shuffle=True, drop_last=True)\n",
    "            syn_test_set  = SyntheticDataset(train=False, activations=syn_activations, noise=synthetic_dataset_noise)\n",
    "            syn_testloader  = DataLoader(syn_test_set, batch_size=64, shuffle=False, drop_last=False)        \n",
    "                    \n",
    "            # define the loss function and the optimiser\n",
    "            loss_function = nn.CrossEntropyLoss()\n",
    "            optimiser = optim.SGD(model_cut.parameters(), lr=1e-4, momentum=0.9, weight_decay=0.01)\n",
    "            \n",
    "            # Put top model into train mode so that bn and dropout perform in training mode\n",
    "            model_cut.train()\n",
    "            # Freeze the top model\n",
    "            model_cut.requires_grad_(False)\n",
    "            # Un-Freeze the stitch layer\n",
    "            for name, param in model_cut.stitch.named_parameters():\n",
    "                param.requires_grad_(True)\n",
    "            logtofile(f\"Train the stitch to a top model cut after layer {layer_to_cut_after}\")    \n",
    "            # the epoch loop: note that we're training the whole network\n",
    "            for epoch in range(stitch_train_epochs):\n",
    "                running_loss = 0.0\n",
    "                for data in syn_trainloader:  # Use synthetic training data\n",
    "                    # data is (representations, labels) tuple\n",
    "                    # get the inputs and put them on the GPU\n",
    "                    inputs, labels = data                \n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "            \n",
    "                    # zero the parameter gradients\n",
    "                    optimiser.zero_grad()\n",
    "            \n",
    "                    # forward + loss + backward + optimise (update weights)\n",
    "                    outputs = model_cut(inputs)\n",
    "                    loss = loss_function(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimiser.step()\n",
    "            \n",
    "                    # keep track of the loss this epoch\n",
    "                    running_loss += loss.item()\n",
    "                logtofile(\"Epoch %d, loss %4.2f\" % (epoch, running_loss))\n",
    "            logtofile('**** Finished Training ****')\n",
    "            \n",
    "            model_cut.eval() # ALWAYS DO THIS BEFORE YOU EVALUATE MODELS\n",
    "    \n",
    "    \n",
    "            #new_tensor = torch.load(\"foo_1_state.pt\")\n",
    "            ##############################################################\n",
    "            \n",
    "            logtofile(\"Test the trained stitch\")        \n",
    "            # Compute the model accuracy on the test set\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            # assuming 10 classes\n",
    "            # rows represent actual class, columns are predicted\n",
    "            confusion_matrix = torch.zeros(10,10, dtype=torch.int)\n",
    "            \n",
    "            for data in syn_testloader:\n",
    "                inputs, labels = data\n",
    "                #print(inputs)   \n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                predictions = torch.argmax(model_cut(inputs),1)\n",
    "                #print(model_cut(inputs))\n",
    "                matches = predictions == labels.to(device)\n",
    "                correct += matches.sum().item()\n",
    "                total += len(labels)\n",
    "            \n",
    "                for idx, l in enumerate(labels):\n",
    "                    confusion_matrix[l, predictions[idx]] = 1 + confusion_matrix[l, predictions[idx]] \n",
    "            acc = ((100.0 * correct) / total)\n",
    "            logtofile('Test Accuracy: %2.2f %%' % acc)\n",
    "            logtofile('Confusion Matrix')\n",
    "            logtofile(confusion_matrix)\n",
    "            logtofile(\"===================================================================\")\n",
    "            stitching_accuracies[key][layer_to_cut_after] = acc\n",
    "            stitching_penalties[key][layer_to_cut_after] = original_accuracy[key] - acc        \n",
    "    \n",
    "            if measure_rank:\n",
    "                dl = syn_testloader        \n",
    "                params = {}\n",
    "                params[\"model\"] = model_training_type\n",
    "                params[\"dataset\"] = dataset_type\n",
    "                params[\"seed\"] = seed\n",
    "                if val[\"train\"]: # as only one network used, record its filename as both send and receive files\n",
    "                    params[\"rcv_file\"] = val[\"saveas\"] \n",
    "                else:    \n",
    "                    params[\"rcv_file\"] = val[\"loadfrom\"]\n",
    "                \n",
    "                params[\"syn_noise\"] = synthetic_dataset_noise\n",
    "                \n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    layers, features, handles = install_hooks(model_cut)\n",
    "                    \n",
    "                    metrics = evaluate_model(model_cut, dl, 'acc', verbose=2)\n",
    "                    params.update(metrics)\n",
    "                    classes = None\n",
    "                    df = perform_analysis(features, classes, layers, params, n=-1)\n",
    "                    df.to_csv(f\"{outpath}\")\n",
    "                    \n",
    "                for h in handles:\n",
    "                    h.remove()\n",
    "                del model_cut, layers, features, metrics, params, df, handles\n",
    "                gc.collect()\n",
    "            else:  # NOT measuring rank\n",
    "                logtofile(f\"output to {outpath}\")\n",
    "                        \n",
    "                params = {}\n",
    "                # params[\"offset\"] = target_offset\n",
    "                params[\"model\"] = model_training_type\n",
    "                params[\"dataset\"] = dataset_type\n",
    "                params[\"seed\"] = seed\n",
    "                params[\"val_acc\"] = acc / 100\n",
    "                params[\"name\"] = \"only\"\n",
    "                params[\"syn_noise\"] = synthetic_dataset_noise\n",
    "        \n",
    "                results = []\n",
    "                results.append(params)\n",
    "                df = pd.DataFrame.from_records(results)\n",
    "                df.to_csv(f\"{outpath}\")\n",
    "                            \n",
    "                del  params, df\n",
    "                gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3db5808a-6351-4133-9e7c-85e8e9248cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stitching_accuracies={'bw': {5: 95.96}, 'bgonly': {5: 96.63}}\n",
      "stitching_penalties={'bw': {5: 2.3000000000000114}, 'bgonly': {5: 3.3700000000000045}}\n"
     ]
    }
   ],
   "source": [
    "logtofile(f\"{stitching_accuracies=}\")\n",
    "logtofile(f\"{stitching_penalties=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd669a19-7c42-4265-b8e8-57165995c097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synth-bw\n",
      "98.26\n",
      "Stitch Accuracy\n",
      "95.96\n",
      "--------------------------\n",
      "synth-bgonly\n",
      "100.0\n",
      "Stitch Accuracy\n",
      "96.63\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "for r_key in stitching_accuracies:\n",
    "    logtofile(f\"synth-{r_key}\")\n",
    "    logtofile(original_accuracy[r_key])\n",
    "    logtofile(\"Stitch Accuracy\")\n",
    "    for layer in stitching_accuracies[r_key]:\n",
    "        logtofile(stitching_accuracies[r_key][layer])\n",
    "    logtofile(\"--------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
