{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3a7ef23f-9180-4388-a6fb-6f1d044ee863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before adding noise: sample_class=0 self.activations[sample_class][0]=tensor([0.5830, 0.1675, 0.1069, 0.2999])\n",
      "after adding noise: self.activations[sample_class][0]=tensor([0.5830, 0.1675, 0.1069, 0.2999])\n",
      "(tensor([[0.6492, 0.1942, 0.1131, 0.3620]]), 0)\n",
      "before adding noise: sample_class=0 self.activations[sample_class][0]=tensor([0.5830, 0.1675, 0.1069, 0.2999])\n",
      "after adding noise: self.activations[sample_class][0]=tensor([0.5830, 0.1675, 0.1069, 0.2999])\n",
      "(tensor([[0.6223, 0.1451, 0.0750, 0.1794]]), 0)\n",
      "before adding noise: sample_class=0 self.activations[sample_class][0]=tensor([0.5830, 0.1675, 0.1069, 0.2999])\n",
      "after adding noise: self.activations[sample_class][0]=tensor([0.5830, 0.1675, 0.1069, 0.2999])\n",
      "(tensor([[0.6223, 0.1451, 0.0750, 0.1794]]), 0)\n",
      "before adding noise: sample_class=0 self.activations[sample_class][0]=tensor([0.5830, 0.1675, 0.1069, 0.2999])\n",
      "after adding noise: self.activations[sample_class][0]=tensor([0.5830, 0.1675, 0.1069, 0.2999])\n",
      "(tensor([[0.6492, 0.1942, 0.1131, 0.3620]]), 0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "import datetime\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "representation_shape = (1,4)\n",
    "num_classes = 2\n",
    "\n",
    "''' Only generate the activations once, so that the same values are used for train and test datasets.\n",
    "Use torch.rand so that values are in the range [0,1] as this is more likely as a representation'''\n",
    "def generate_activations(num_classes, representation_shape):\n",
    "    activations = torch.empty((num_classes, ) + representation_shape)\n",
    "    for c in range(num_classes):\n",
    "        activations[c] = torch.rand(representation_shape)\n",
    "    return activations\n",
    "\n",
    "activations = generate_activations(num_classes, representation_shape)\n",
    "\n",
    "\n",
    "class SyntheticDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, train, activations, noise=0.0, num_classes = 10):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            train (bool)        : for train=True set, return 60_000 samples, otherwise 10_000\n",
    "            activations         : base set of activations: one per class. values in range [0,1]\n",
    "            noise (float) = 0.0 : factor by which to scale randn values before adding to the base activations. \n",
    "            num_classes = 10    : standard for MNIST and CIFAR-10\n",
    "        \"\"\"\n",
    "        self.train = train\n",
    "        self.size = 60_000 if train else 10_000\n",
    "        \n",
    "        self.data = torch.empty((self.size,) + activations[0].shape)  # start off empty\n",
    "        self.targets = torch.empty(self.size)  #, dtype=torch.int)  # Random targets\n",
    "        \n",
    "        for c in range(num_classes):\n",
    "            #activations = torch.randn(self.shape)\n",
    "            #print(activations.shape)\n",
    "            # FIXME do I need to use clone()? This may not be producing the ball of noise I thought it was.\n",
    "            self.data[c * self.size // num_classes : ((c+1) * self.size // num_classes)]    = activations[c].clone()\n",
    "            self.targets[c * self.size // num_classes : ((c+1) * self.size // num_classes)] = c\n",
    "        print(activations[0])\n",
    "        print(self.data[0])\n",
    "        print(self.data[1])\n",
    "        self.data += noise * torch.randn(self.data.shape)  # add a configurable amount of noise\n",
    "        self.data = torch.clamp(self.data, 0, 1)           # and clamp to [0,1] range again so that it is batchnorm compatible\n",
    "        print(activations[0])\n",
    "        print(self.data[0])\n",
    "        print(self.data[1])\n",
    "        print(f\"Check: {activations[0] is activations[0]}\")\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx], int(self.targets[idx])\n",
    "        return sample\n",
    "\n",
    "\n",
    "\n",
    "class DynamicSyntheticDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, num_samples, activations, noise=0.0, local_seed = 9):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_samples         : would use 60000/10000 for MNIST equivalent. May well require 1000 per class = 1 million for imagenet\n",
    "            activations         : base set of activations: one per class. values in range [0,1]\n",
    "            noise (float) = 0.0 : factor by which to scale randn values before adding to the base activations.             \n",
    "            local_seed          : used to create random noise without upsetting the global generators\n",
    "        \"\"\"\n",
    "        num_classes = len(activations)\n",
    "        self.targets = torch.empty(num_samples)  #, dtype=torch.int)  # Random targets\n",
    "        self.local_seed = local_seed\n",
    "        self.noise = noise\n",
    "        \n",
    "        for c in range(num_classes):\n",
    "            self.targets[c * num_samples // num_classes : ((c+1) * num_samples // num_classes)] = c\n",
    "\n",
    "        self.activations = activations\n",
    "        \n",
    "        # Create a local random number generator\n",
    "        self.local_rng = torch.Generator()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        sample_class = int(self.targets[idx])\n",
    "        # Seed the local generator according to this get request\n",
    "        self.local_rng.manual_seed(self.local_seed + idx)\n",
    "\n",
    "        print(f\"before adding noise: {sample_class=} {self.activations[sample_class][0]=}\") \n",
    "        data = self.activations[sample_class].clone() \n",
    "        data += self.noise * torch.randn(data.shape, generator=self.local_rng)  # add a configurable amount of noise\n",
    "        data = torch.clamp(data, 0, 1)                # and clamp to [0,1] range again so that it is batchnorm compatible \n",
    "        print(f\"after adding noise: {self.activations[sample_class][0]=}\") \n",
    "             \n",
    "        \n",
    "        sample = data, sample_class\n",
    "        return sample\n",
    "\n",
    "# dataset = SyntheticDataset(False, activations, noise=0.1, num_classes=num_classes)\n",
    "dyn_dataset = DynamicSyntheticDataset(10, activations, noise=0.1, local_seed=1)\n",
    "print(dyn_dataset.__getitem__(0))\n",
    "print(dyn_dataset.__getitem__(1))\n",
    "print(dyn_dataset.__getitem__(1))\n",
    "print(dyn_dataset.__getitem__(0))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
